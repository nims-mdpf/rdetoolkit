{"config":{"lang":["ja"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RDEToolKit","text":"<p>RDEToolKit\u306f\u3001RDE\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306aPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002RDEToolKit\u306e\u5404\u7a2e\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001RDE\u3078\u306e\u7814\u7a76\u30fb\u5b9f\u9a13\u30c7\u30fc\u30bf\u306e\u767b\u9332\u51e6\u7406\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002\u4e3b\u306b\u3001RDEToolKit\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u306e\u524d\u51e6\u7406\u30fb\u5f8c\u51e6\u7406\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u7814\u7a76\u3084\u5b9f\u9a13\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4f7f\u7528\u3055\u308c\u3066\u3044\u308bPython\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u767b\u9332\u304b\u3089\u52a0\u5de5\u3001\u30b0\u30e9\u30d5\u5316\u306a\u3069\u3088\u308a\u591a\u69d8\u306a\u51e6\u7406\u3092\u5b9f\u73fe\u53ef\u80fd\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u306e\u30af\u30ec\u30f3\u30b8\u30f3\u30b0\u3001\u5909\u63db\u3001\u96c6\u8a08\u3001\u53ef\u8996\u5316\u306a\u3069\u3001\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5168\u4f53\u3092\u52b9\u7387\u7684\u306b\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002</p> <p></p> <p></p>"},{"location":"#_1","title":"\u4e3b\u306a\u7279\u5fb4","text":"<ul> <li>\u30e2\u30b8\u30e5\u30fc\u30eb\u5316<ul> <li>RDE\u30c7\u30fc\u30bf\u767b\u9332\u306b\u5fc5\u8981\u306a\u524d\u51e6\u7406\u3084\u5f8c\u51e6\u7406\u304c\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u63d0\u4f9b\u3055\u308c\u3066\u304a\u308a\u3001\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3057\u305f\u3044\u51e6\u7406\u3092\u5b9f\u88c5\u3059\u308b\u3060\u3051\u3067\u3001\u67d4\u8edf\u306b\u69cb\u9020\u51e6\u7406\u3092\u5b9f\u88c5\u3067\u304d\u307e\u3059\u3002</li> </ul> </li> <li>\u67d4\u8edf\u306a\u30c7\u30fc\u30bf\u51e6\u7406<ul> <li>\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3001\u5909\u63db\u3001\u53ef\u8996\u5316\u306a\u3069\u3001\u3055\u307e\u3056\u307e\u306a\u30c7\u30fc\u30bf\u51e6\u7406\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002</li> </ul> </li> <li>\u7c21\u5358\u306a\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb<ul> <li><code>pip install rdetoolkit</code> \u30b3\u30de\u30f3\u30c9\u3067\u7c21\u5358\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002</li> </ul> </li> <li>\u62e1\u5f35\u6027<ul> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5b9a\u7fa9\u3057\u305f\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u51e6\u7406\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u7279\u5b9a\u306e\u30c7\u30fc\u30bf\u51e6\u7406\u30cb\u30fc\u30ba\u306b\u5bfe\u5fdc\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u7279\u5b9a\u306e\u7814\u7a76\u3084\u5b9f\u9a13\u306e\u8981\u4ef6\u306b\u5408\u308f\u305b\u305f\u30c7\u30fc\u30bf\u51e6\u7406\u304c\u53ef\u80fd\u3067\u3059\u3002</li> </ul> </li> <li>\u7d71\u5408<ul> <li>\u4ed6\u306ePython\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u767b\u9332\u304b\u3089\u52a0\u5de5\u3001\u30b0\u30e9\u30d5\u5316\u306a\u3069\u3001\u3088\u308a\u591a\u69d8\u306a\u51e6\u7406\u3092\u5b9f\u73fe\u53ef\u80fd\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u69cb\u9020\u5316\u51e6\u7406\u5168\u4f53\u3092\u4e00\u5143\u7ba1\u7406\u3067\u304d\u307e\u3059\u3002</li> </ul> </li> </ul>"},{"location":"#_2","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>RDEToolKit\u306fPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3068\u3057\u3066\u63d0\u4f9b\u3055\u308c\u3066\u304a\u308a\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\u3002</p> <pre><code>pip install rdetoolkit\n</code></pre>"},{"location":"#code-sample","title":"Code Sample","text":"Sample1: \u30e6\u30fc\u30b6\u30fc\u5b9a\u7fa9\u69cb\u9020\u5316\u51e6\u7406\u3042\u308a Sample2: \u30e6\u30fc\u30b6\u30fc\u5b9a\u7fa9\u69cb\u9020\u5316\u51e6\u7406\u306a\u3057"},{"location":"#next-step","title":"Next Step","text":"<ul> <li> rdetoolkit\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb</li> <li> \u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8</li> <li> RDE\u69cb\u9020\u5316\u51e6\u7406\u3092\u958b\u767a\u3059\u308b</li> <li> rdetoolkit API Documents</li> <li> Contributing</li> <li> License</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>RDEToolKit\u306e\u4e0d\u5177\u5408\u3084\u304a\u554f\u3044\u5408\u308f\u305b\u306f\u3001\u4ee5\u4e0b\u306eIssue\u306b\u3054\u6295\u7a3f\u304f\u3060\u3055\u3044\u3002</p> <p>Contributing</p>"},{"location":"README_ja/","title":"README ja","text":""},{"location":"README_ja/#rdetoolkit","title":"RDEToolKit","text":"<p>RDEToolKit\u306f\u3001RDE\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e\u57fa\u672c\u7684\u306aPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002 RDEToolKit\u306e\u5404\u7a2e\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001RDE\u3078\u306e\u7814\u7a76\u30fb\u5b9f\u9a13\u30c7\u30fc\u30bf\u306e\u767b\u9332\u51e6\u7406\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002 \u307e\u305f\u3001\u30e6\u30fc\u30b6\u30fc\u304c\u7814\u7a76\u3084\u5b9f\u9a13\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4f7f\u7528\u3055\u308c\u3066\u3044\u308bPython\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u30c7\u30fc\u30bf\u306e\u767b\u9332\u304b\u3089\u52a0\u5de5\u3001\u30b0\u30e9\u30d5\u5316\u306a\u3069\u3088\u308a\u591a\u69d8\u306a\u51e6\u7406\u3092\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"README_ja/#documents","title":"Documents","text":"<p>See documentation for more details.</p>"},{"location":"README_ja/#contributing","title":"Contributing","text":"<p>\u5909\u66f4\u3092\u52a0\u3048\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u4e00\u8aad\u304a\u9858\u3044\u3057\u307e\u3059\u3002</p> <ul> <li>CONTRIBUTING.md</li> </ul>"},{"location":"README_ja/#install","title":"Install","text":"<p>\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306f\u3001\u4e0b\u8a18\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>pip install rdetoolkit\n</code></pre>"},{"location":"README_ja/#usage","title":"Usage","text":"<p>RDE\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u69cb\u7bc9\u306e\u4e00\u4f8b\u3067\u3059\u3002</p>"},{"location":"README_ja/#_1","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b","text":"<p>\u307e\u305a\u3001RDE\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u5fc5\u8981\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u6e96\u5099\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u30bf\u30fc\u30df\u30ca\u30eb\u3084\u30b7\u30a7\u30eb\u4e0a\u3067\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>python3 -m rdetoolkit init\n</code></pre> <p>\u30b3\u30de\u30f3\u30c9\u304c\u6b63\u3057\u304f\u52d5\u4f5c\u3059\u308b\u3068\u3001\u4e0b\u8a18\u3067\u793a\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u30fb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002</p> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001<code>container</code>\u3068\u3044\u3046\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u3001\u958b\u767a\u3092\u9032\u3081\u307e\u3059\u3002</p> <ul> <li>requirements.txt</li> <li>\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u69cb\u7bc9\u3067\u4f7f\u7528\u3057\u305f\u3044Python\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066<code>pip install</code>\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> <li>modules</li> <li>\u69cb\u9020\u5316\u51e6\u7406\u3067\u4f7f\u7528\u3057\u305f\u3044\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u683c\u7d0d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5225\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u8aac\u660e\u3057\u307e\u3059\u3002</li> <li>main.py</li> <li>\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u3092\u5b9a\u7fa9</li> <li>data/inputdata</li> <li>\u69cb\u9020\u5316\u51e6\u7406\u5bfe\u8c61\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> <li>data/invoice</li> <li>\u30ed\u30fc\u30ab\u30eb\u5b9f\u884c\u3055\u305b\u308b\u305f\u3081\u306b\u306f\u7a7a\u30d5\u30a1\u30a4\u30eb\u3067\u3082\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</li> <li>data/tasksupport</li> <li>\u69cb\u9020\u5316\u51e6\u7406\u306e\u88dc\u52a9\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> <pre><code>container\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 invoice\n\u2502   \u2502   \u2514\u2500\u2500 invoice.json\n\u2502   \u2514\u2500\u2500 tasksupport\n\u2502       \u251c\u2500\u2500 invoice.schema.json\n\u2502       \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 modules\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"README_ja/#_2","title":"\u69cb\u9020\u5316\u51e6\u7406\u306e\u5b9f\u88c5","text":"<p>\u5165\u529b\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30c7\u30fc\u30bf\u52a0\u5de5\u30fb\u30b0\u30e9\u30d5\u5316\u30fb\u6a5f\u68b0\u5b66\u7fd2\u7528\u306ecsv\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u306a\u3069\u51e6\u7406\u3092\u5b9f\u884c\u3057\u3001RDE\u3078\u30c7\u30fc\u30bf\u3092\u767b\u9332\u3067\u304d\u307e\u3059\u3002\u4e0b\u8a18\u306e\u66f8\u5f0f\u306b\u5f93\u3063\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u3001\u72ec\u81ea\u306e\u51e6\u7406\u3092RDE\u306e\u69cb\u9020\u5316\u51e6\u7406\u306e\u30d5\u30ed\u30fc\u306b\u7d44\u307f\u8fbc\u307f\u8fbc\u3080\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p> <p><code>dataset()</code>\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u5f15\u6570\u3092\u6e21\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>srcpaths (RdeInputDirPaths): \u51e6\u7406\u306e\u305f\u3081\u306e\u5165\u529b\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30d1\u30b9</li> <li>resource_paths (RdeOutputResourcePath): \u51e6\u7406\u7d50\u679c\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u51fa\u529b\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30d1\u30b9</li> </ul> <pre><code>def dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    ...\n</code></pre> <p>\u4eca\u56de\u306e\u4f8b\u3067\u306f\u3001<code>modules</code>\u4ee5\u4e0b\u306b\u3001<code>def display_messsage()</code>\u3068\u3044\u3046\u30c0\u30df\u30fc\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u3001\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002<code>modules/modules.py</code>\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <pre><code># modules/modules.py\ndef display_messsage(path_list):\n    print(f\"Test Message!: {path_list}\")\n\ndef dataset(srcpaths, resource_paths):\n    display_messsage(srcpaths)\n    display_messsage(resource_paths)\n</code></pre>"},{"location":"README_ja/#_3","title":"\u8d77\u52d5\u51e6\u7406\u306b\u3064\u3044\u3066","text":"<p>\u7d9a\u3044\u3066\u3001<code>rdetoolkit.workflow.run()</code>\u3092\u4f7f\u3063\u3066\u3001\u8d77\u52d5\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u8d77\u52d5\u51e6\u7406\u3067\u4e3b\u306b\u5b9f\u884c\u51e6\u7406\u306f\u3001</p> <ul> <li>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u30c1\u30a7\u30c3\u30af</li> <li>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3068RDE\u69cb\u9020\u5316\u3067\u898f\u5b9a\u3059\u308b\u5404\u7a2e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b</li> <li>\u30e6\u30fc\u30b6\u30fc\u3054\u3068\u3067\u5b9a\u7fa9\u3057\u305f\u5177\u4f53\u7684\u306a\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c</li> </ul> <pre><code>import rdetoolkit\nfrom modules.modules import datase  #\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\n\n#\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\u3092\u5f15\u6570\u3068\u3057\u3066\u6e21\u3059\nrdetoolkit.workflows.run(custom_dataset_function=dataset)\n</code></pre> <p>\u3082\u3057\u3001\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u6e21\u3055\u306a\u3044\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>import rdetoolkit\n\nrdetoolkit.workflows.run()\n</code></pre>"},{"location":"README_ja/#_4","title":"\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u52d5\u4f5c\u3055\u305b\u308b\u5834\u5408","text":"<p>\u5404\u81ea\u306e\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u3001\u30c7\u30d0\u30c3\u30b0\u3084\u30c6\u30b9\u30c8\u7684\u306bRDE\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u3001<code>data</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5fc5\u8981\u306a\u5165\u529b\u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u3082\u5b9f\u884c\u53ef\u80fd\u3067\u3059\u3002\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001main.py\u3068\u540c\u3058\u968e\u5c64\u306bdata\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u914d\u7f6e\u3057\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u52d5\u4f5c\u3057\u307e\u3059\u3002</p> <pre><code>container/\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 modules/\n\u2502   \u2514\u2500\u2500 modules.py\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 inputdata/\n    \u2502   \u2514\u2500\u2500 &lt;\u51e6\u7406\u3057\u305f\u3044\u5b9f\u9a13\u30c7\u30fc\u30bf&gt;\n    \u251c\u2500\u2500 invoice/\n    \u2502   \u2514\u2500\u2500 invoice.json\n    \u2514\u2500\u2500 tasksupport/\n        \u251c\u2500\u2500 metadata-def.json\n        \u2514\u2500\u2500 invoice.schema.json\n</code></pre>"},{"location":"api_documentation_requirements/","title":"API Documentation Requirements Specification","text":""},{"location":"api_documentation_requirements/#document-information","title":"Document Information","text":"<ul> <li>Document Title: RDEToolkit Full-Scratch API Documentation Requirements</li> <li>Version: 1.0</li> <li>Created: 2024-12-23</li> <li>Purpose: Define requirements for creating comprehensive, full-scratch API documentation for RDEToolkit modules</li> </ul>"},{"location":"api_documentation_requirements/#1-project-overview","title":"1. Project Overview","text":""},{"location":"api_documentation_requirements/#11-background","title":"1.1 Background","text":"<p>Currently, RDEToolkit uses MkDocs extensions for automatic API documentation generation. This project aims to create comprehensive, manually written API documentation that provides better control over content, structure, and presentation.</p>"},{"location":"api_documentation_requirements/#12-objectives","title":"1.2 Objectives","text":"<ul> <li>Replace MkDocs extension-based documentation with full-scratch documentation</li> <li>Provide comprehensive, detailed API documentation for all modules</li> <li>Maintain consistency in documentation format and structure</li> <li>Ensure documentation is maintainable and up-to-date</li> </ul>"},{"location":"api_documentation_requirements/#13-scope","title":"1.3 Scope","text":"<ul> <li>All Python modules in <code>src/rdetoolkit/</code></li> <li>Both <code>.py</code> and <code>.pyi</code> files as source material</li> <li>Focus on public APIs and user-facing functionality</li> <li>Maintain existing directory structure in documentation</li> </ul>"},{"location":"api_documentation_requirements/#2-documentation-standards","title":"2. Documentation Standards","text":""},{"location":"api_documentation_requirements/#21-format-requirements","title":"2.1 Format Requirements","text":""},{"location":"api_documentation_requirements/#211-file-format","title":"2.1.1 File Format","text":"<ul> <li>Format: Markdown (.md)</li> <li>Encoding: UTF-8</li> <li>Line Endings: Unix (LF)</li> </ul>"},{"location":"api_documentation_requirements/#212-structure-template","title":"2.1.2 Structure Template","text":"<p>Based on <code>docs/rdetoolkit/core.md</code> as the reference template:</p> <pre><code># Module Name\n\nBrief module description and purpose.\n\n## Overview\n\nHigh-level overview of module functionality:\n- **Feature 1**: Description\n- **Feature 2**: Description\n- **Feature 3**: Description\n\n## Classes\n\n### ClassName\n\nClass description.\n\n#### Constructor\n#### Attributes  \n#### Methods\n\n## Functions\n\n### function_name\n\nFunction description with signature, parameters, returns, raises, examples.\n\n## Complete Usage Examples\n\n## Error Handling\n\n## Performance Notes\n\n## See Also\n</code></pre>"},{"location":"api_documentation_requirements/#22-content-requirements","title":"2.2 Content Requirements","text":""},{"location":"api_documentation_requirements/#221-module-documentation-structure","title":"2.2.1 Module Documentation Structure","text":"<ol> <li>Module Header: Clear module name and brief description</li> <li>Overview Section: High-level functionality summary with bullet points</li> <li>Classes Section: Detailed class documentation</li> <li>Functions Section: Detailed function documentation</li> <li>Usage Examples: Complete, practical examples</li> <li>Error Handling: Common exceptions and handling patterns</li> <li>Performance Notes: Performance considerations</li> <li>See Also: Cross-references to related documentation</li> </ol>"},{"location":"api_documentation_requirements/#222-class-documentation-structure","title":"2.2.2 Class Documentation Structure","text":"<ol> <li>Class Description: Purpose and functionality</li> <li>Constructor: Signature, parameters, exceptions</li> <li>Attributes: Public attributes with types and descriptions</li> <li>Methods: All public methods with full signatures</li> </ol>"},{"location":"api_documentation_requirements/#223-functionmethod-documentation-structure","title":"2.2.3 Function/Method Documentation Structure","text":"<ol> <li>Function Signature: Complete type hints</li> <li>Parameters: Name, type, description for each parameter</li> <li>Returns: Return type and description</li> <li>Raises: Exception types and conditions</li> <li>Examples: Practical usage examples</li> <li>Notes: Additional implementation details</li> </ol>"},{"location":"api_documentation_requirements/#23-code-examples-requirements","title":"2.3 Code Examples Requirements","text":""},{"location":"api_documentation_requirements/#231-example-standards","title":"2.3.1 Example Standards","text":"<ul> <li>Completeness: Examples should be runnable</li> <li>Practicality: Real-world usage scenarios</li> <li>Clarity: Well-commented and easy to understand</li> <li>Variety: Basic, intermediate, and advanced examples</li> </ul>"},{"location":"api_documentation_requirements/#232-example-categories","title":"2.3.2 Example Categories","text":"<ol> <li>Basic Usage: Simple, introductory examples</li> <li>Advanced Usage: Complex scenarios and configurations</li> <li>Integration Examples: Usage with other modules</li> <li>Error Handling Examples: Exception handling patterns</li> <li>Complete Workflows: End-to-end usage scenarios</li> </ol>"},{"location":"api_documentation_requirements/#3-technical-requirements","title":"3. Technical Requirements","text":""},{"location":"api_documentation_requirements/#31-source-material-analysis","title":"3.1 Source Material Analysis","text":""},{"location":"api_documentation_requirements/#311-python-files-py","title":"3.1.1 Python Files (.py)","text":"<ul> <li>Analyze actual implementation code</li> <li>Extract docstrings and comments</li> <li>Identify public APIs and interfaces</li> <li>Document actual parameter types and behaviors</li> </ul>"},{"location":"api_documentation_requirements/#312-type-stub-files-pyi","title":"3.1.2 Type Stub Files (.pyi)","text":"<ul> <li>Use for accurate type information</li> <li>Extract type hints and signatures</li> <li>Ensure type accuracy in documentation</li> </ul>"},{"location":"api_documentation_requirements/#313-cross-reference-analysis","title":"3.1.3 Cross-Reference Analysis","text":"<ul> <li>Identify module dependencies</li> <li>Document integration points</li> <li>Create accurate cross-references</li> </ul>"},{"location":"api_documentation_requirements/#32-directory-structure-mapping","title":"3.2 Directory Structure Mapping","text":""},{"location":"api_documentation_requirements/#321-source-to-documentation-mapping","title":"3.2.1 Source to Documentation Mapping","text":"<pre><code>src/rdetoolkit/module/          -&gt; docs/rdetoolkit/module/\nsrc/rdetoolkit/module/sub/      -&gt; docs/rdetoolkit/module/sub/\nsrc/rdetoolkit/module/file.py   -&gt; docs/rdetoolkit/module/file.md\n</code></pre>"},{"location":"api_documentation_requirements/#322-hierarchy-preservation","title":"3.2.2 Hierarchy Preservation","text":"<ul> <li>Maintain exact directory structure from source</li> <li>Preserve module organization and relationships</li> <li>Ensure consistent navigation paths</li> </ul>"},{"location":"api_documentation_requirements/#33-content-accuracy-requirements","title":"3.3 Content Accuracy Requirements","text":""},{"location":"api_documentation_requirements/#331-implementation-verification","title":"3.3.1 Implementation Verification","text":"<ul> <li>All documented features must exist in source code</li> <li>Parameter types must match actual implementation</li> <li>Examples must be tested and functional</li> <li>Cross-references must be valid and current</li> </ul>"},{"location":"api_documentation_requirements/#332-completeness-requirements","title":"3.3.2 Completeness Requirements","text":"<ul> <li>Document all public classes and functions</li> <li>Include all public methods and attributes</li> <li>Cover all significant parameters and return values</li> <li>Document all major exception types</li> </ul>"},{"location":"api_documentation_requirements/#4-quality-assurance","title":"4. Quality Assurance","text":""},{"location":"api_documentation_requirements/#41-consistency-requirements","title":"4.1 Consistency Requirements","text":""},{"location":"api_documentation_requirements/#411-format-consistency","title":"4.1.1 Format Consistency","text":"<ul> <li>Uniform heading styles and hierarchy</li> <li>Consistent code block formatting</li> <li>Standardized parameter documentation format</li> <li>Uniform cross-reference style</li> </ul>"},{"location":"api_documentation_requirements/#412-content-consistency","title":"4.1.2 Content Consistency","text":"<ul> <li>Consistent terminology throughout</li> <li>Uniform example complexity and style</li> <li>Standardized error handling documentation</li> <li>Consistent performance note format</li> </ul>"},{"location":"api_documentation_requirements/#42-accuracy-verification","title":"4.2 Accuracy Verification","text":""},{"location":"api_documentation_requirements/#421-code-verification","title":"4.2.1 Code Verification","text":"<ul> <li>All examples must be syntactically correct</li> <li>Type annotations must match implementation</li> <li>Function signatures must be accurate</li> <li>Import statements must be correct</li> </ul>"},{"location":"api_documentation_requirements/#422-link-verification","title":"4.2.2 Link Verification","text":"<ul> <li>All internal links must resolve correctly</li> <li>Cross-references must point to existing documentation</li> <li>External links must be valid and relevant</li> </ul>"},{"location":"api_documentation_requirements/#43-maintainability-requirements","title":"4.3 Maintainability Requirements","text":""},{"location":"api_documentation_requirements/#431-update-process","title":"4.3.1 Update Process","text":"<ul> <li>Documentation must be easily updatable</li> <li>Changes should be trackable through version control</li> <li>Update procedures must be documented</li> <li>Automated verification where possible</li> </ul>"},{"location":"api_documentation_requirements/#432-review-process","title":"4.3.2 Review Process","text":"<ul> <li>Peer review for accuracy and completeness</li> <li>Technical review for implementation correctness</li> <li>Editorial review for clarity and consistency</li> </ul>"},{"location":"api_documentation_requirements/#5-deliverables","title":"5. Deliverables","text":""},{"location":"api_documentation_requirements/#51-documentation-files","title":"5.1 Documentation Files","text":""},{"location":"api_documentation_requirements/#511-primary-modules","title":"5.1.1 Primary Modules","text":"<ul> <li><code>docs/rdetoolkit/workflows.md</code> (completed)</li> <li><code>docs/rdetoolkit/processing/processors/descriptions.md</code> (completed)</li> <li>Additional modules as identified</li> </ul>"},{"location":"api_documentation_requirements/#512-supporting-documentation","title":"5.1.2 Supporting Documentation","text":"<ul> <li>This requirements specification document</li> <li>Style guide for contributors</li> <li>Update and maintenance procedures</li> </ul>"},{"location":"api_documentation_requirements/#52-quality-deliverables","title":"5.2 Quality Deliverables","text":""},{"location":"api_documentation_requirements/#521-verification-materials","title":"5.2.1 Verification Materials","text":"<ul> <li>Example code testing results</li> <li>Link verification reports</li> <li>Consistency check results</li> </ul>"},{"location":"api_documentation_requirements/#522-process-documentation","title":"5.2.2 Process Documentation","text":"<ul> <li>Documentation creation procedures</li> <li>Review and approval workflows</li> <li>Maintenance schedules and procedures</li> </ul>"},{"location":"api_documentation_requirements/#6-implementation-guidelines","title":"6. Implementation Guidelines","text":""},{"location":"api_documentation_requirements/#61-development-process","title":"6.1 Development Process","text":""},{"location":"api_documentation_requirements/#611-analysis-phase","title":"6.1.1 Analysis Phase","text":"<ol> <li>Identify target module for documentation</li> <li>Analyze source code (.py files) for implementation details</li> <li>Review type stubs (.pyi files) for accurate type information</li> <li>Identify dependencies and integration points</li> <li>Research existing usage patterns and examples</li> </ol>"},{"location":"api_documentation_requirements/#612-documentation-creation-phase","title":"6.1.2 Documentation Creation Phase","text":"<ol> <li>Create documentation file in appropriate directory</li> <li>Follow established template structure</li> <li>Write comprehensive content following style guidelines</li> <li>Include practical examples and error handling</li> <li>Add cross-references and related links</li> </ol>"},{"location":"api_documentation_requirements/#613-review-and-validation-phase","title":"6.1.3 Review and Validation Phase","text":"<ol> <li>Verify technical accuracy against source code</li> <li>Test all provided examples</li> <li>Check consistency with existing documentation</li> <li>Validate all links and cross-references</li> <li>Conduct peer review for completeness</li> </ol>"},{"location":"api_documentation_requirements/#62-tool-and-resource-requirements","title":"6.2 Tool and Resource Requirements","text":""},{"location":"api_documentation_requirements/#621-development-tools","title":"6.2.1 Development Tools","text":"<ul> <li>Code analysis tools for Python</li> <li>Markdown editors with preview capability</li> <li>Link checking tools</li> <li>Code syntax verification tools</li> </ul>"},{"location":"api_documentation_requirements/#622-reference-materials","title":"6.2.2 Reference Materials","text":"<ul> <li>Python type annotation documentation</li> <li>MkDocs extension outputs for comparison</li> <li>Existing codebase and examples</li> <li>User feedback and usage patterns</li> </ul>"},{"location":"api_documentation_requirements/#7-success-criteria","title":"7. Success Criteria","text":""},{"location":"api_documentation_requirements/#71-completeness-criteria","title":"7.1 Completeness Criteria","text":"<ul> <li>All public APIs documented</li> <li>All examples functional and tested</li> <li>Complete cross-reference network</li> <li>Comprehensive error handling documentation</li> </ul>"},{"location":"api_documentation_requirements/#72-quality-criteria","title":"7.2 Quality Criteria","text":"<ul> <li>Zero broken internal links</li> <li>All examples syntactically correct</li> <li>Consistent formatting throughout</li> <li>Accurate type information</li> </ul>"},{"location":"api_documentation_requirements/#73-usability-criteria","title":"7.3 Usability Criteria","text":"<ul> <li>Clear navigation structure</li> <li>Practical, useful examples</li> <li>Comprehensive search capability</li> <li>Effective cross-referencing</li> </ul>"},{"location":"api_documentation_requirements/#8-maintenance-and-updates","title":"8. Maintenance and Updates","text":""},{"location":"api_documentation_requirements/#81-update-triggers","title":"8.1 Update Triggers","text":"<ul> <li>New module additions</li> <li>API changes or deprecations</li> <li>Bug fixes affecting documented behavior</li> <li>User feedback and improvement requests</li> </ul>"},{"location":"api_documentation_requirements/#82-update-process","title":"8.2 Update Process","text":"<ol> <li>Identify changed modules through code analysis</li> <li>Update affected documentation sections</li> <li>Verify example code and cross-references</li> <li>Test documentation build and navigation</li> <li>Review and approve changes</li> </ol>"},{"location":"api_documentation_requirements/#83-version-control","title":"8.3 Version Control","text":"<ul> <li>Track documentation changes in git</li> <li>Use semantic versioning for major updates</li> <li>Maintain change logs for documentation updates</li> <li>Tag releases with corresponding code versions</li> </ul>"},{"location":"api_documentation_requirements/#9-risk-management","title":"9. Risk Management","text":""},{"location":"api_documentation_requirements/#91-technical-risks","title":"9.1 Technical Risks","text":"<ul> <li>Risk: Documentation becoming outdated</li> <li> <p>Mitigation: Automated checking and regular review cycles</p> </li> <li> <p>Risk: Inconsistent documentation quality</p> </li> <li> <p>Mitigation: Standardized templates and review processes</p> </li> <li> <p>Risk: Examples becoming non-functional</p> </li> <li>Mitigation: Automated testing of documentation examples</li> </ul>"},{"location":"api_documentation_requirements/#92-resource-risks","title":"9.2 Resource Risks","text":"<ul> <li>Risk: High maintenance overhead</li> <li> <p>Mitigation: Efficient update processes and automation</p> </li> <li> <p>Risk: Knowledge transfer challenges</p> </li> <li>Mitigation: Comprehensive process documentation</li> </ul>"},{"location":"api_documentation_requirements/#10-conclusion","title":"10. Conclusion","text":"<p>This requirements specification provides the foundation for creating high-quality, maintainable API documentation for RDEToolkit. By following these guidelines, we can ensure comprehensive, accurate, and useful documentation that serves both current and future users of the toolkit.</p> <p>The documentation should be viewed as a living resource that grows and improves with the codebase, maintaining accuracy and usefulness throughout the project's lifecycle.</p>"},{"location":"install/","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5","text":""},{"location":"install/#pypi","title":"PyPI\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>rdetoolkit\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> Unix/macOSWindows <pre><code>python3 -m pip install rdetoolkit\npython3 -m pip install rdetoolkit==&lt;\u6307\u5b9a\u30d0\u30fc\u30b8\u30e7\u30f3&gt;\n</code></pre> <pre><code>py -m pip install rdetoolkit\npy -m pip install rdetoolkit==&lt;\u6307\u5b9a\u30d0\u30fc\u30b8\u30e7\u30f3&gt;\n</code></pre>"},{"location":"install/#minio","title":"MinIO\u6a5f\u80fd\u4ed8\u304d\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>MinIO\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u306f\u3001extras \u30aa\u30d7\u30b7\u30e7\u30f3 <code>[minio]</code> \u3092\u6307\u5b9a\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> Unix/macOS <pre><code>python3 -m pip install \"rdetoolkit[minio]\"\npython3 -m pip install \"rdetoolkit[minio]==&lt;\u6307\u5b9a\u30d0\u30fc\u30b8\u30e7\u30f3&gt;\"\n</code></pre> Windows <pre><code>py -m pip install \"rdetoolkit[minio]\"\npy -m pip install \"rdetoolkit[minio]==&lt;\u6307\u5b9a\u30d0\u30fc\u30b8\u30e7\u30f3&gt;\"\n</code></pre>"},{"location":"install/#github","title":"Github\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>Github\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u76f4\u63a5\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3044\u5834\u5408\u3084\u3001\u958b\u767a\u7248\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5834\u5408\u3001\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u76f4\u63a5\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> Unix/macOSWindows <pre><code>python3 -m pip install rdetoolkit@git+https://github.com/nims-dpfc/rdetoolkit.git\n</code></pre> <pre><code>py -m pip install \"rdetoolkit@git+https://github.com/nims-dpfc/rdetoolkit.git\"\n</code></pre>"},{"location":"install/#_2","title":"\u958b\u767a\u8005\u5411\u3051: \u30bd\u30fc\u30b9\u304b\u3089\u306e\u30d3\u30eb\u30c9\u3068\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>\u958b\u767a\u8005\u3084\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u304c\u5fc5\u8981\u306a\u30e6\u30fc\u30b6\u30fc\u5411\u3051\u306b\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u30d3\u30eb\u30c9\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u624b\u9806\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"install/#_3","title":"\u524d\u63d0\u6761\u4ef6","text":"<p>\u4ee5\u4e0b\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ul> <li>Python 3.9\u4ee5\u4e0a</li> <li>Rust toolchain (cargo, rustc)</li> <li>Git</li> </ul>"},{"location":"install/#_4","title":"\u624b\u9806","text":""},{"location":"install/#1","title":"1. \u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30af\u30ed\u30fc\u30f3","text":"<pre><code>git clone https://github.com/nims-dpfc/rdetoolkit.git\ncd rdetoolkit\n</code></pre>"},{"location":"install/#2-rust","title":"2. Rust\u74b0\u5883\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7","text":"<p>Rust\u304c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\uff1a</p> Unix/macOS/LinuxWindows <pre><code># Rust\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource ~/.cargo/env\n\n# maturin\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install maturin\n</code></pre> <pre><code># Rust\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff08https://rustup.rs/ \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\uff09\n# \u307e\u305f\u306f Chocolatey \u3092\u4f7f\u7528\nchoco install rust\n\n# maturin\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install maturin\n</code></pre>"},{"location":"install/#3","title":"3. \u4f9d\u5b58\u95a2\u4fc2\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<pre><code># Python\u4f9d\u5b58\u95a2\u4fc2\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install -r requirements.lock\n</code></pre>"},{"location":"install/#4","title":"4. \u30d3\u30eb\u30c9\u3068\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":""},{"location":"install/#_5","title":"\u958b\u767a\u30e2\u30fc\u30c9\uff08\u63a8\u5968\uff09","text":"<p>\u958b\u767a\u4e2d\u306f\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30d3\u30eb\u30c9\u3068\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u540c\u6642\u306b\u884c\u3044\u307e\u3059\uff1a</p> <pre><code># \u958b\u767a\u30e2\u30fc\u30c9\u3067\u30d3\u30eb\u30c9\u30fb\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\nmaturin develop\n\n# \u307e\u305f\u306f editable mode \u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install -e .\n</code></pre>"},{"location":"install/#_6","title":"\u914d\u5e03\u7528\u30d3\u30eb\u30c9","text":"<p>\u914d\u5e03\u7528\u306ewheel\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\uff1a</p> <pre><code># wheel\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\nmaturin build --release\n\n# \u751f\u6210\u3055\u308c\u305fwheel\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install target/wheels/rdetoolkit-*.whl\n</code></pre>"},{"location":"install/#5","title":"5. \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306e\u78ba\u8a8d","text":"<pre><code># \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u306e\u78ba\u8a8d\npython -c \"import rdetoolkit; print(rdetoolkit.__version__)\"\n\n# \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30c4\u30fc\u30eb\u306e\u78ba\u8a8d\npython -m rdetoolkit --help\n</code></pre>"},{"location":"install/#_7","title":"\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0","text":""},{"location":"install/#rust","title":"Rust\u30b3\u30f3\u30d1\u30a4\u30eb\u30a8\u30e9\u30fc","text":"<pre><code>error: Microsoft Visual C++ 14.0 is required (Windows)\n</code></pre> <p>\u89e3\u6c7a\u65b9\u6cd5: Windows \u306e\u5834\u5408\u3001Microsoft C++ Build Tools \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"install/#maturin","title":"maturin\u304c\u898b\u3064\u304b\u3089\u306a\u3044","text":"<pre><code>maturin: command not found\n</code></pre> <p>\u89e3\u6c7a\u65b9\u6cd5:  <pre><code>pip install --upgrade maturin\n</code></pre></p>"},{"location":"install/#pythonh","title":"Python.h\u304c\u898b\u3064\u304b\u3089\u306a\u3044","text":"<pre><code>fatal error: Python.h: No such file or directory\n</code></pre> <p>\u89e3\u6c7a\u65b9\u6cd5: </p> Ubuntu/DebianCentOS/RHELmacOS <pre><code>sudo apt-get install python3-dev\n</code></pre> <pre><code>sudo yum install python3-devel\n</code></pre> <pre><code>xcode-select --install\n</code></pre>"},{"location":"install/#_8","title":"\u4f9d\u5b58\u95a2\u4fc2","text":"<p>\u672c\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u3001\u4ee5\u4e0b\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u7fa4\u306b\u4f9d\u5b58\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>pyproject.toml - nims-dpfc/rdetoolkit</li> </ul>"},{"location":"contribute/documents_contributing/","title":"Contributing to RDEToolKit","text":""},{"location":"contribute/documents_contributing/#_1","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8","text":"<p>RDEToolKit\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u304cRDE\u69cb\u9020\u5316\u51e6\u7406\u3092\u6b63\u3057\u304f\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u306b\u306a\u308a\u307e\u3059\u3002\u5229\u7528\u8005\u306e\u7686\u69d8\u304b\u3089\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u6539\u5584\u306b\u3054\u5354\u529b\u304f\u3060\u3055\u3044\u3002 \u5fc5\u305a\u3057\u3082\u3001RDEToolKit\u3078\u306e\u6df1\u3044\u7406\u89e3\u304c\u3042\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u5185\u5bb9\u304c\u4e0d\u8db3\u3057\u3066\u3044\u308b\u3001\u7406\u89e3\u3057\u306b\u304f\u3044\u3068\u3044\u3046\u7b87\u6240\u306f\u3001\u7a4d\u6975\u7684\u306bIssue\u3067\u306e\u5831\u544a\u3092\u304a\u5f85\u3061\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u5177\u4f53\u7684\u306a\u624b\u9806\u306b\u3064\u3044\u3066\u306f\u3001\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u304b\u3089\u8aac\u660e\u3044\u305f\u3057\u307e\u3059\u3002</p>"},{"location":"contribute/documents_contributing/#rdetoolkit","title":"RDEToolKit\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":"<p>rdetoolkit\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u3001\u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u306e<code>docs</code>\u30d5\u30a9\u30eb\u30c0\u306b\u683c\u7d0d\u3057\u3066\u3044\u307e\u3059\u3002\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u3001MkDocs\u3092\u4f7f\u7528\u3057\u3066\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u69cb\u7bc9\u3057\u3066\u3044\u307e\u3059\u3002\u3059\u3079\u3066\u306e\u30b3\u30fc\u30c9\u304c\u9069\u5207\u306b\u6587\u66f8\u5316\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306f\u3001\u9069\u5207\u306b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3055\u308c\u305f docstring \u3092\u4f7f\u7528\u3057\u3066\u6587\u66f8\u5316\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li>\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>\u30af\u30e9\u30b9\u5b9a\u7fa9</li> <li>\u95a2\u6570\u306e\u5b9a\u7fa9</li> <li>\u30e2\u30b8\u30e5\u30fc\u30eb\u30ec\u30d9\u30eb\u306e\u5909\u6570</li> </ul>"},{"location":"contribute/documents_contributing/#_2","title":"\u6ce8\u610f\u4e8b\u9805","text":"<p>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u66f4\u65b0\u306b\u95a2\u3057\u3066\u77e5\u3063\u3066\u304a\u304f\u3079\u304d\u91cd\u8981\u4e8b\u9805\u306b\u3064\u3044\u3066:</p> <ul> <li>rdetoolkit\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f\u3001\u30b3\u30fc\u30c9\u81ea\u4f53\u306edocstring\u3068\u3001\u305d\u306e\u4ed6\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e2\u3064\u306b\u5927\u5225\u3055\u308c\u307e\u3059\u3002</li> <li>docstring\u306f\u3001\u5404\u7a2e\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u5229\u7528\u6cd5\u304c\u8a18\u8f09\u3055\u308c\u3001GitLab CI/CD\u3067\u3001\u81ea\u52d5\u30d3\u30eb\u30c9\u3055\u308c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u304c\u66f4\u65b0\u3055\u308c\u307e\u3059\u3002</li> <li>docstring\u306f\u3001PEP 257\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u306b\u5f93\u3063\u3066\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3055\u308c\u305fGoogle Style\u306edocstring\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 (\u305d\u306e\u4ed6\u306e\u4f8b\u306b\u3064\u3044\u3066\u306f\u3001\u300cGoogle \u30b9\u30bf\u30a4\u30eb\u306e Python \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u6587\u5b57\u5217\u306e\u4f8b\u300d\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002)</li> <li>Google Style\u306e docstring \u3068 pydocstyle lint \u306e\u9593\u3067\u7af6\u5408\u3059\u308b\u5834\u5408\u306f\u3001pydocstyle lint \u306e\u30d2\u30f3\u30c8\u306b\u5f93\u3063\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul>"},{"location":"contribute/documents_contributing/#_3","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u6539\u5584\u70b9\u3092\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b","text":"<p>\u4ee5\u4e0b\u306eURL\u3088\u308a\u3001RDEToolKit\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30a2\u30af\u30bb\u30b9\u3057\u3001issue\u3092\u767a\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u3053\u306e\u6642\u3001\u30e9\u30d9\u30eb\u306f<code>Type:documentation</code>\u3068\u3044\u3046\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>RDEToolKit - github.com</p>"},{"location":"contribute/documents_contributing/#_4","title":"\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u5909\u66f4\u3059\u308b","text":"<p>\u5909\u66f4\u65b9\u6cd5\u306f\u3001\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u30b8\u30c8\u30ea\u304b\u3089\u30d6\u30e9\u30f3\u30c1\u3092\u5909\u66f4\u3057\u3066\u3001\u5909\u66f4\u3092Push\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30d6\u30e9\u30f3\u30c1\u540d\u306e\u5148\u982d\u306b<code>docs-***</code>\u3068\u3044\u3046\u63a5\u982d\u8f9e\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>git checkout -b docs-***\n# \u5b9f\u884c\u4f8b\ngit checkout -b docs-install-manual\n</code></pre> <p>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u5909\u66f4\u3057\u3001\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306bPush\u3057\u307e\u3059\u3002</p> <pre><code>git add &lt;\u5909\u66f4\u3057\u305f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8&gt;\ngit commit -m \"\u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\"\ngit push origin &lt;\u5bfe\u8c61\u306e\u30d6\u30e9\u30f3\u30c1\u540d&gt;\n</code></pre>"},{"location":"contribute/documents_contributing/#web","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092Web\u4e0a\u3067\u78ba\u8a8d\u3059\u308b","text":"<p>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092Web\u3067\u78ba\u8a8d\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u30b5\u30fc\u30d0\u30fc\u3092\u8d77\u52d5\u3057\u3066\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"contribute/documents_contributing/#rye","title":"rye\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u3044\u308b\u5834\u5408","text":"<pre><code>rye sync\nmkdocs serve\n</code></pre>"},{"location":"contribute/documents_contributing/#rye_1","title":"rye\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u3044\u306a\u3044\u5834\u5408","text":"<pre><code>pip install -r requirements.lock\nmkdocs serve\n</code></pre>"},{"location":"contribute/documents_contributing/#main","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092main\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8\u3059\u308b","text":"<p>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3057\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u7ba1\u7406\u8005\u304c\u78ba\u8a8d\u3057\u3001\u554f\u984c\u304c\u306a\u3051\u308c\u3070\u30de\u30fc\u30b8\u3057\u307e\u3059\u3002</p>"},{"location":"contribute/home/","title":"Contributing to RDEToolKit","text":""},{"location":"contribute/home/#_1","title":"\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u306e\u6e96\u5099","text":"<p>RDEToolKit\u3078\u306e\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u3092\u3057\u3066\u3044\u305f\u3060\u304f\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"contribute/home/#_2","title":"\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30af\u30ed\u30fc\u30f3\u3092\u30ed\u30fc\u30ab\u30eb\u306b\u4f5c\u6210\u3059\u308b","text":"<pre><code>cd &lt;\u4efb\u610f\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea&gt;\n\n# SSH\ngit clone git@github.com:nims-dpfc/rdetoolkit.git\n# HTTPS\ngit clone https://github.com/nims-dpfc/rdetoolkit.git\n\n# \u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u79fb\u52d5\ncd rdetoolkit\n</code></pre>"},{"location":"contribute/home/#_3","title":"\u30d1\u30c3\u30b1\u30fc\u30b8\u7ba1\u7406\u30c4\u30fc\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>rdetoolkit\u3067\u306f\u3001<code>rye</code>\u3092\u5229\u7528\u3057\u3066\u3044\u307e\u3059\u3002rye\u306f\u3001Flask\u306e\u4f5c\u8005\u304c\u4f5c\u6210\u3057\u305f\u3001Python\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u95a2\u4fc2\u7ba1\u7406\u30c4\u30fc\u30eb\u3067\u3059\u3002\u5185\u90e8\u5b9f\u88c5\u306fRust\u306e\u305f\u3081\u3001\u975e\u5e38\u306b\u9ad8\u901f\u3067\u3059\u3002poetry\u3092\u9078\u629e\u305b\u305arye\u3092\u63a1\u7528\u3057\u305f\u7406\u7531\u306f\u3001\u52d5\u4f5c\u901f\u5ea6\u306e\u89b3\u70b9\u3068\u3001<code>pyenv</code>\u3092\u5225\u9014\u5229\u7528\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3067\u3059\u3002rye\u306f\u3001<code>pyenv+poetry</code>\u306e\u3088\u3046\u306b\u3001\u30a4\u30f3\u30bf\u30d7\u30ea\u30bf\u306e\u7ba1\u7406\u3068\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u7ba1\u7406\u304c\u7d71\u5408\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\u306e\u89b3\u70b9\u304b\u3089\u3082rye\u306e\u65b9\u304c\u512a\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u3053\u3061\u3089\u3092\u63a1\u7528\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>rye\u306f\u4ee5\u4e0b\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u8003\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>Installation - Rye</p>"},{"location":"contribute/home/#_4","title":"\u958b\u767a\u74b0\u5883\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7","text":"<p>rye\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u958b\u767a\u74b0\u5883\u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3057\u3066\u304f\u3060\u3055\u3044\u3002<code>rye sync</code>\u3067\u4eee\u60f3\u74b0\u5883\u304c\u4f5c\u6210\u3055\u308c\u3001\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u304c\u4eee\u60f3\u74b0\u5883\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u3002</p> <pre><code>cd &lt;rdetoolkit\u306e\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u30b8\u30c8\u30ea&gt;\nrye sync\n</code></pre> <p>\u4eee\u60f3\u74b0\u5883\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>\u307e\u305f\u3001RDEToolKit\u3067\u306f\u30b3\u30fc\u30c9\u54c1\u8cea\u306e\u89b3\u70b9\u304b\u3089\u3001<code>pre-commit</code>\u3092\u63a1\u7528\u3057\u3066\u3044\u307e\u3059\u3002pre-commit\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u3001\u4ee5\u4e0b\u306e\u51e6\u7406\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>pre-commit install\n</code></pre> <p>\u3082\u3057\u3001Visaul Stdio Code\u3092\u5229\u7528\u3059\u308b\u969b\u306f\u3001\u62e1\u5f35\u6a5f\u80fd<code>pre-commit</code>\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"contribute/home/#contributing","title":"Contributing","text":"<p>RDEToolKit\u3067\u306f\u3001\u4ee5\u4e0b\u306e2\u70b9\u306e\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u3092\u671f\u5f85\u3057\u3066\u3044\u307e\u3059\u3002\u4e0b\u8a18\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u8003\u306b\u3001\u5909\u66f4\u30fb\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u30fb\u6a5f\u80fd\u4fee\u6b63\u3092\u5b9f\u65bd\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8</li> <li>\u30b3\u30fc\u30c9\u30d9\u30fc\u30b9\u306e\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8</li> </ul>"},{"location":"contribute/sourcecode_contributing/","title":"Contributing to RDEToolKit","text":""},{"location":"contribute/sourcecode_contributing/#_1","title":"\u6a5f\u80fd\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u3068\u6a5f\u80fd\u62e1\u5f35\u306e\u30ea\u30af\u30a8\u30b9\u30c8","text":"<p>\u3053\u306e\u30c4\u30fc\u30eb\u3067\u3001\u65b0\u6a5f\u80fd\u30fb\u5909\u66f4\u30fb\u4e0d\u5177\u5408\u7b49\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u5909\u66f4\u3092\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>\u4e0a\u8a18\u30ea\u30dd\u30b8\u30c8\u30ea\u306eissue\u3067\u3001Issue\u3092\u4f5c\u6210\u3057\u65b0\u6a5f\u80fd\u3001\u554f\u984c\u3084\u4e0d\u5177\u5408\u3092\u5831\u544a\u3059\u308b</li> <li>\u5909\u66f4\u3092\u5b9f\u969b\u306b\u52a0\u3048\u308b\u5834\u5408\u3001\u30ed\u30fc\u30ab\u30eb\u3067\u65b0\u898f\u306b\u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\u3057\u3001\u5909\u66f4\u3092\u52a0\u3048\u308b\u3002</li> <li>CI\u30c6\u30b9\u30c8\u3092\u5b9f\u884c\u3057\u3001\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u51fa\u3059</li> <li>CI\u30c6\u30b9\u30c8\u304c\u5168\u3066\u30d1\u30b9\u3001\u30ec\u30d3\u30e5\u30fc\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001\u30de\u30fc\u30b8\u3059\u308b</li> <li>Release\u30da\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b</li> </ul> <p>\u3053\u306e\u624b\u9806\u66f8\u3092\u53c2\u8003\u306b\u3057\u3066\u3001RDEToolKit\u306e\u958b\u767a\u30fb\u5909\u66f4\u3092\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\u5171\u540c\u958b\u767a\u3092\u5186\u6ed1\u306b\u9032\u3081\u308b\u305f\u3081\u306e\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3\u3068\u3057\u3066\u5229\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"contribute/sourcecode_contributing/#issue","title":"Issue\u3092\u4f5c\u6210\u3059\u308b","text":"<p>\u554f\u984c\u3084\u4e0d\u5177\u5408\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306eisuue\u3078\u8d77\u7968\u304a\u9858\u3044\u3057\u307e\u3059\u3002\u3053\u306e\u6642\u3001\u30e9\u30d9\u30eb\u306f<code>Type:improvement</code>, <code>Type: new feature</code>\u306e\u3069\u3061\u3089\u304b\u306e\u4ed8\u4e0e\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002</p> <p>https://github.com/nims-dpfc/rdetoolkit/issues</p>"},{"location":"contribute/sourcecode_contributing/#_2","title":"\u30d6\u30e9\u30f3\u30c1\u306e\u4f5c\u6210","text":"<p>\u65b0\u3057\u3044\u6a5f\u80fd\u3084\u4fee\u6b63\u3092\u884c\u3046\u969b\u306f\u3001\u65b0\u3057\u3044\u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>\u30d6\u30e9\u30f3\u30c1\u540d\u306e\u63a5\u982d\u8f9e\u306f\u3001<code>develop-v&lt;x.y.z&gt;</code>\u3068\u3044\u3046\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u3001\u672b\u5c3e\u306b\u4efb\u610f\u306e\u6587\u5b57\u5217\u3092\u8ffd\u52a0\u3057\u3066\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> <pre><code>git checkout -b develop-v&lt;x.y.z&gt;-&lt;\u4efb\u610f\u306e\u6a5f\u80fd\u540d\u306a\u3069&gt; origin/develop-v&lt;x.y.z&gt;\n</code></pre>"},{"location":"contribute/sourcecode_contributing/#_3","title":"\u65b0\u6a5f\u80fd\u30fb\u4fee\u6b63\u3092\u52a0\u3048\u308b","text":""},{"location":"contribute/sourcecode_contributing/#_4","title":"\u5fc5\u8981\u306a\u30c4\u30fc\u30eb","text":"<ul> <li><code>Python</code> (\u63a8\u5968\u30d0\u30fc\u30b8\u30e7\u30f3: 3.9\u4ee5\u4e0a)</li> <li>rye or Poetry(\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u7ba1\u7406\u30c4\u30fc\u30eb)</li> <li><code>git</code> (\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406)</li> </ul>"},{"location":"contribute/sourcecode_contributing/#_5","title":"\u30b3\u30fc\u30c9\u306e\u30b9\u30bf\u30a4\u30eb\u3068\u8981\u4ef6","text":""},{"location":"contribute/sourcecode_contributing/#pep8","title":"\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u898f\u7d04(PEP8)\u306b\u3064\u3044\u3066","text":"<p>\u4e00\u8cab\u3057\u305f\u30b3\u30fc\u30c9\u30b9\u30bf\u30a4\u30eb\u306b\u5f93\u3046\u3053\u3068\u3067\u3001\u30b3\u30fc\u30c9\u3092\u3088\u308a\u6271\u3044\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u3001RDE\u306e\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u958b\u767a\u3067\u306f\u3001PEP8(Python Enhance Proposal #8) \u306b\u5f93\u3063\u3066\u958b\u767a\u3092\u884c\u3046\u3002</p>"},{"location":"contribute/sourcecode_contributing/#_6","title":"\u57fa\u672c\u7684\u306b\u5f93\u3046\u898f\u7d04","text":"<p>\u5f15\u7528: PEP 8 \u2013 Style Guide for Python Code / Python Enhancement Proposals \u53c2\u8003: Python \u30b3\u30fc\u30c9\u306e\u30b9\u30bf\u30a4\u30eb\u30ac\u30a4\u30c9 - pep8-ja</p>"},{"location":"contribute/sourcecode_contributing/#_7","title":"\u7a7a\u767d","text":"<p>Python\u306f\u7a7a\u767d\u304c\u69cb\u6587\u4e0a\u610f\u5473\u3092\u6301\u3061\u307e\u3059\u3002Python\u4f7f\u3046\u5834\u5408\u3001\u7a7a\u767d\u306e\u52b9\u679c\u3068\u305d\u306e\u5f71\u97ff\u306b\u7279\u306b\u610f\u8b58\u3057\u3066\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u305f\u65b9\u304c\u3044\u3044\u3067\u3059\u3002</p> <ul> <li>\u30a4\u30f3\u30c7\u30f3\u30c8\u306b\u306f\u3001\u30bf\u30d6\u3067\u306f\u306a\u304f\u7a7a\u767d\u3092\u4f7f\u3046</li> <li>\u69cb\u6587\u4e0a\u610f\u5473\u3092\u6301\u3064\u30ec\u30d9\u30eb\u306e\u30a4\u30f3\u30c7\u30f3\u30c8\u306b\u306f\u30014\u500b\u306e\u7a7a\u767d\u3092\u4f7f\u3046</li> <li>\u5404\u884c\u306e\u9577\u3055\u304c~~79\u6587\u5b57\u304b\u305d\u308c\u4ee5\u4e0b\u3068\u3059\u308b~~ -&gt; \u4e0a\u9650\u306a\u3057\u3002</li> <li>\u9577\u3044\u5f0f\u3092\u7d9a\u3051\u308b\u305f\u3081\u306b\u306b\u6b21\u306e\u884c\u3092\u4f7f\u3046\u6642\u3001\u901a\u5e38\u306e\u30a4\u30f3\u30c7\u30f3\u30c8\u304b\u30894\u500b\u306e\u8ffd\u52a0\u7a7a\u767d\u3092\u4f7f\u3063\u3066\u30a4\u30f3\u30c7\u30f3\u30c8\u3059\u308b\u3002</li> <li>\u30d5\u30a1\u30a4\u30eb\u3067\u306f\u3001\u95a2\u6570\u3068\u30af\u30e9\u30b9\u306f\u3001\u7a7a\u767d2\u884c\u3067\u5206\u3051\u308b\u3002</li> <li>\u30af\u30e9\u30b9\u3067\u306f\u3001\u30e1\u30bd\u30c3\u30c9\u306f\u3001\u7a7a\u767d\u884c\u3067\u5206\u3051\u308b\u3002</li> <li>\u8f9e\u66f8\u3067\u306f\u3001\u30ad\u30fc\u3068\u30b3\u30ed\u30f3(<code>:</code>)\u306e\u9593\u306b\u306f\u7a7a\u767d\u3092\u304a\u304b\u305a\u306b\u3001\u540c\u3058\u884c\u306b\u5024\u3092\u66f8\u304f\u5834\u5408\u306b\u306f\u5024\u306e\u524d\u306b\u7a7a\u767d\u30921\u3064\u7f6e\u304f\u3002</li> <li>\u5909\u6570\u4ee3\u5165\u306e\u524d\u5f8c\u306b\u306f\u3001\u7a7a\u767d\u30921\u3064\u3001\u5fc5\u305a1\u3064\u3060\u3051\u304a\u304f</li> <li>\u578b\u30d2\u30f3\u30c8(\u578b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3)\u3067\u306f\u3001\u5909\u6570\u540d\u306e\u76f4\u5f8c\u306b\u30b3\u30ed\u30f3\u3092\u304a\u304d\u3001\u578b\u60c5\u5831\u306e\u524d\u306b\u7a7a\u767d\u30921\u3064\u304a\u304f</li> </ul>"},{"location":"contribute/sourcecode_contributing/#_8","title":"\u540d\u524d\u4ed8\u3051","text":"<p>PEP8\u306f\u3001\u8a00\u8a9e\u306e\u7570\u306a\u308b\u7b87\u6240\u3054\u3068\u306b\u4ed6\u3068\u7570\u306a\u308b\u30b9\u30bf\u30a4\u30eb\u3092\u63a8\u5968\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>\u95a2\u6570\u3001\u5909\u6570\u3001\u5c5e\u6027\u306f\u3001<code>lowercase_underscore</code>\u306e\u3088\u3046\u306b\u5c0f\u6587\u5b57\u3067\u30a2\u30f3\u30c0\u30fc\u30b9\u30b3\u30a2\u3092\u631f\u3080</li> <li>\u30d7\u30ed\u30c6\u30af\u30c6\u30c3\u30c9\u5c5e\u6027\u306f\u3001_leading_underscore\u306e\u3088\u3046\u306b\u30a2\u30f3\u30c0\u30fc\u30b9\u30b3\u30a2\u3092\u5148\u982d\u306b\u3064\u3051\u308b</li> <li>\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u5c5e\u6027\u306f\u3001__double_underscore\u306e\u3088\u3046\u306b\u30a2\u30f3\u30c0\u30fc\u30b9\u30b3\u30a2\u30922\u3064\u5148\u982d\u306b\u3064\u3051\u308b</li> <li>\u30af\u30e9\u30b9\u3068\u4f8b\u5916\u306f\u3001<code>CapitalizedWord</code>\u306e\u3088\u3046\u306b\u5148\u982d\u3092\u5927\u6587\u5b57\u306b\u3059\u308b</li> <li>\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u306e\u5b9a\u6570\u306f\u3001ALL_CAPS\u306e\u3088\u3046\u306b\u5168\u3066\u5927\u6587\u5b57\u3067\u30a2\u30f3\u30c0\u30fc\u30b9\u30b3\u30a2\u3067\u631f\u3080</li> <li>\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30e1\u30bd\u30c3\u30c9\u306f\u3001(\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u53c2\u7167\u3059\u308b)\u7b2c\u4e00\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u540d\u524d\u306b<code>self</code>\u3092\u4f7f\u3046\u3002</li> <li>\u30af\u30e9\u30b9\u30e1\u30bd\u30c3\u30c9\u306f\u3001(\u30af\u30e9\u30b9\u3092\u53c2\u7167\u3059\u308b)\u7b2c\u4e00\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u540d\u524d\u306b<code>cls</code>\u3092\u4f7f\u3046\u3002</li> </ul>"},{"location":"contribute/sourcecode_contributing/#_9","title":"\u5f0f\u3068\u6587","text":"<ul> <li>\u5f0f\u306e\u5426\u5b9a(<code>if not a is b</code>)\u3067\u306f\u306a\u304f\u3001\u5185\u5074\u306e\u9805\u306e\u5426\u5b9a(<code>if a is not b</code>)\u3092\u4f7f\u3046</li> <li>\u30b3\u30f3\u30c6\u30ca\u3084\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u9577\u3055(<code>if len(somelist) == 0</code>)\u3092\u4f7f\u3063\u3066\u3001\u7a7a\u5024(<code>[]</code>\u3084<code>''</code>\u306a\u3069)\u304b\u3069\u3046\u304b\u3092\u30c1\u30a7\u30c3\u30af\u3057\u306a\u3044\u3002<code>if not somelist</code>\u3092\u4f7f\u3063\u3066\u3001\u7a7a\u5024\u304c\u6697\u9ed9\u306b<code>False</code>\u3068\u8a55\u4fa1\u3055\u308c\u308b\u3053\u3068\u3092\u4f7f\u3046\u3002</li> <li>\u4e0a\u3068\u540c\u3058\u3053\u3068\u3092\u3001\u975e\u7a7a\u5024(<code>[1]</code>\u3084<code>hi</code>\u306a\u3069)\u306b\u3082\u4f7f\u3046\u3002\u975e\u7a7a\u5024\u306b\u3064\u3044\u3066\u3001<code>if somelist</code>\u306f\u3001\u6697\u9ed9\u7684\u306b<code>True</code>\u3068\u8a55\u4fa1\u3055\u308c\u308b\u3002</li> <li><code>if</code>, <code>for</code>, <code>while</code>\u30eb\u30fc\u30d7\u3001<code>except</code>\u8907\u5408\u6587\u30921\u884c\u3067\u66f8\u304b\u306a\u3044\u3002\u660e\u78ba\u306b\u306a\u308b\u3088\u3046\u306b\u8907\u6570\u884c\u306b\u3059\u308b\u3002</li> <li>\u5f0f\u304c1\u884c\u306b\u53ce\u307e\u3089\u306a\u3044\u5834\u5408\u306f\u3001\u62ec\u5f27\u3067\u62ec\u3063\u3066\u3001\u8907\u6570\u884c\u306b\u3057\u3066\u3001\u8aad\u307f\u3084\u3059\u3044\u3088\u3046\u306b\u30a4\u30f3\u30c7\u30f3\u30c8\u3059\u308b\u3002</li> <li><code>\\</code>\u3067\u884c\u308f\u3051\u3059\u308b\u3088\u308a\u306f\u3001\u62ec\u5f27\u3092\u4f7f\u3063\u3066\u8907\u6570\u306e\u5f0f\u3092\u56f2\u3080\u65b9\u304c\u826f\u3044\u3002</li> </ul>"},{"location":"contribute/sourcecode_contributing/#import","title":"import","text":"<ul> <li><code>import</code>\u6587\u306f\u3001(<code>from x import y</code>\u3082\u542b\u3081\u3066)\u5e38\u306b\u30d5\u30a1\u30a4\u30eb\u306e\u5148\u982d\u306b\u7f6e\u304f\u3002</li> <li>\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u3068\u304d\u306f\u3001\u5e38\u306b\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d76\u5bfe\u540d\u3092\u4f7f\u3044\u3001\u73fe\u30e2\u30b8\u30e5\u30fc\u30eb\u30d1\u30b9\u304b\u3089\u306e\u76f8\u5bfe\u540d\u3092\u4f7f\u308f\u306a\u3044\u3002\u4f8b\u3048\u3070\u3001\u30e2\u30b8\u30e5\u30fc\u30eb<code>foo</code>\u3092\u30d1\u30c3\u30b1\u30fc\u30b8<code>bar</code>\u304b\u3089\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u6642\u306f\u3001<code>import foo</code>\u3067\u306f\u306a\u304f\u3001<code>from bar import foo</code>\u3092\u4f7f\u3046\u3002</li> <li>\u76f8\u5bfe\u30a4\u30f3\u30dd\u30fc\u30c8\u3092\u4f7f\u308f\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u6642\u306b\u306f\u3001\u660e\u793a\u7684\u306a\u69cb\u6587<code>from . import foo</code> \u3092\u4f7f\u3046\u3002</li> <li>\u30a4\u30f3\u30dd\u30fc\u30c8\u306f\u30011.\u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\u30e2\u30b8\u30e5\u30fc\u30eb\u30012.\u30b5\u30fc\u30c9\u30d1\u30fc\u30c6\u30a3\u30e2\u30b8\u30e5\u30fc\u30eb\u30013.\u81ea\u5206\u306e\u4f5c\u6210\u3057\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u9806\u306b\u884c\u3046\u3002\u305d\u308c\u305e\u308c\u306e\u90e8\u5206\u3067\u306f\u3001\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u306b\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b</li> </ul>"},{"location":"contribute/sourcecode_contributing/#rdetoolkit","title":"RDEToolKit\u3067\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30bf\u30fc\u30fb\u30ea\u30f3\u30bf\u30fc\u306b\u3064\u3044\u3066","text":"<p>RDEToolKit\u3067\u306f\u3001<code>Ruff</code>\u3068<code>mypy</code>\u3092\u4f7f\u7528\u3057\u3066\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3001\u30ea\u30f3\u30bf\u30fc\u3092\u52d5\u4f5c\u3055\u305b\u3066\u30b3\u30fc\u30c9\u54c1\u8cea\u3092\u4e00\u5b9a\u306b\u4fdd\u3064\u3053\u3068\u3092\u76ee\u6a19\u3068\u3057\u3066\u3044\u307e\u3059\u3002<code>Ruff</code>\u306f\u3001isort, black, flake8\u306e\u6a5f\u80fd\u306b\u5909\u308f\u308b\u30c4\u30fc\u30eb\u3067\u3059\u3002Rust\u3067\u958b\u767a\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001isort, black, flake8\u3067\u52d5\u4f5c\u3055\u305b\u308b\u3088\u308a\u6bb5\u9055\u3044\u306b\u9ad8\u901f\u3067\u3059\u3002\u307e\u305f\u3001<code>mypy</code>\u306f\u3001\u9759\u7684\u578b\u30c1\u30a7\u30c3\u30af\u30c4\u30fc\u30eb\u3067\u3059\u3002RDEToolKit\u306f\u578b\u306e\u8a73\u7d30\u306a\u5b9a\u7fa9\u3092\u5f37\u5236\u3059\u308b\u3053\u3068\u3067\u3001\u30b3\u30fc\u30c9\u306e\u53ef\u8aad\u6027\u3068\u4fdd\u5b88\u6027\u306e\u5411\u4e0a\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>Ruff: https://docs.astral.sh/ruff/</li> <li>mypy: https://mypy.readthedocs.io/en/stable/</li> </ul>"},{"location":"contribute/sourcecode_contributing/#_10","title":"\u30c6\u30b9\u30c8\u306e\u5b9f\u884c","text":"<p>\u5909\u66f4\u3092\u884c\u3063\u305f\u5f8c\u306f\u3001\u30c6\u30b9\u30c8\u3092\u5b9f\u884c\u3057\u3066\u6b63\u5e38\u306b\u52d5\u4f5c\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>tox\n</code></pre>"},{"location":"contribute/sourcecode_contributing/#_11","title":"\u30b3\u30df\u30c3\u30c8\u3068\u30d7\u30c3\u30b7\u30e5","text":"<p>\u5909\u66f4\u3092\u30b3\u30df\u30c3\u30c8\u3057\u3001\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30d7\u30c3\u30b7\u30e5\u3057\u307e\u3059\u3002</p> <pre><code>git add .\ngit commit -m \"#[issue\u756a\u53f7] [\u5909\u66f4\u5185\u5bb9\u306e\u7c21\u5358\u306a\u8aac\u660e]\"\ngit push origin develop-v&lt;x.y.z&gt;-&lt;\u5148\u307b\u3069\u3064\u3051\u305f\u540d\u79f0&gt;\n</code></pre> <p>\u3082\u3057\u3001pre-commit\u306e\u30c1\u30a7\u30c3\u30af\u3067\u30b3\u30df\u30c3\u30c8\u3067\u304d\u306a\u3044\u5834\u5408\u3001\u5168\u3066\u306e\u30a8\u30e9\u30fc\u3092\u89e3\u6d88\u3057\u305f\u4e0a\u3067\u30b3\u30df\u30c3\u30c8\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002</p>"},{"location":"contribute/sourcecode_contributing/#pull-request","title":"\u5909\u66f4\u30ea\u30af\u30a8\u30b9\u30c8 (Pull Request)","text":"<p> \u3053\u306e\u6642\u3001<code>main</code>\u30d6\u30e9\u30f3\u30c1\u306b\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u767a\u884c\u3057\u3066\u306a\u3044\u3067\u304f\u3060\u3055\u3044\u3002</p> <p>\u5909\u66f4\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001GitHub\u7b49\u306e\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3092\u4f7f\u7528\u3057\u3066\u5909\u66f4\u30ea\u30af\u30a8\u30b9\u30c8 (PR) \u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u3053\u306e\u6642\u3001\u30ec\u30d3\u30e5\u30fc\u3092\u53d7\u3051\u3001\u5fc5\u305aCI\u30c6\u30b9\u30c8\u304c\u5168\u3066\u30d1\u30b9\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u3082\u3057\u3001CI\u4e0a\u306e\u30c6\u30b9\u30c8\u304c\u30d1\u30b9\u3057\u306a\u3044\u5834\u5408\u3001\u5168\u3066\u306e\u30a8\u30e9\u30fc\u3092\u89e3\u6d88\u3057\u305f\u4e0a\u3067\u3001\u30ec\u30d3\u30e5\u30fc\u306e\u4f9d\u983c\u3092\u767a\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"contribute/sourcecode_contributing/#_12","title":"\u30de\u30fc\u30b8","text":"<p>\u30ec\u30d3\u30e5\u30fc\u304c\u5b8c\u4e86\u3057\u3001\u554f\u984c\u304c\u306a\u3044\u3068\u5224\u65ad\u3055\u308c\u305f\u3089\u3001\u5bfe\u8c61\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8\u3057\u307e\u3059\u3002</p> <p>\u307e\u305f\u3001\u5168\u3066\u306e\u958b\u767a\u304cfix\u3057\u305f\u3089\u3001main\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8\u3057\u3066\u304f\u3060\u3055\u3044\u3002main\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8\u5f8c\u3001\u30c7\u30d7\u30ed\u30a4\u304c\u6b63\u3057\u304f\u5b9f\u884c\u3067\u304d\u305f\u3089\u3001tag\u306e\u4f5c\u6210\u3068Release\u30da\u30fc\u30b8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>Release\u30da\u30fc\u30b8: https://github.com/nims-dpfc/rdetoolkit/releases</p>"},{"location":"rdetoolkit/","title":"RDE Toolkit API Documentation","text":""},{"location":"rdetoolkit/#core-modules","title":"Core Modules","text":"<ul> <li>config: \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u3068\u7ba1\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>core: \u30b3\u30a2\u6a5f\u80fd\u3092\u63d0\u4f9b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>errors: \u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>exceptions: \u4f8b\u5916\u51e6\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>fileops: RDE\u95a2\u9023\u306e\u30d5\u30a1\u30a4\u30eb\u64cd\u4f5c\u3092\u63d0\u4f9b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>img2thumb: \u753b\u50cf\u3092\u30b5\u30e0\u30cd\u30a4\u30eb\u306b\u5909\u63db\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>invoicefile: \u9001\u308a\u72b6\u30d5\u30a1\u30a4\u30eb\u306e\u51e6\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>modeproc: \u30e2\u30fc\u30c9\u51e6\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>rde2util: RDE\u95a2\u9023\u306e\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u95a2\u6570\u3092\u63d0\u4f9b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>rdelogger: \u30ed\u30ae\u30f3\u30b0\u6a5f\u80fd\u3092\u63d0\u4f9b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>validation: \u30c7\u30fc\u30bf\u306e\u691c\u8a3c\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>workflows: \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u306e\u5b9a\u7fa9\u3068\u7ba1\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> </ul>"},{"location":"rdetoolkit/#models","title":"Models","text":"<ul> <li>config: \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u3068\u7ba1\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>invoice: \u9001\u308a\u72b6\u3084Excelinvoice\u306e\u60c5\u5831\u3092\u5b9a\u7fa9\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>invoice_schema: \u9001\u308a\u72b6\u306e\u30b9\u30ad\u30fc\u30de\u3092\u5b9a\u7fa9\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>metadata: \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u7ba1\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>rde2types: RDE\u95a2\u9023\u306e\u578b\u5b9a\u7fa9\u3092\u63d0\u4f9b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>report: \u30ec\u30dd\u30fc\u30c8\u95a2\u9023\u306e\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>result: \u51e6\u7406\u7d50\u679c\u3092\u7ba1\u7406\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb</li> </ul>"},{"location":"rdetoolkit/#implementation","title":"Implementation","text":"<ul> <li>compressed_controller: \u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306e\u7ba1\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> <li>input_controller: \u5165\u529b\u30e2\u30fc\u30c9\u306e\u7ba1\u7406\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb</li> </ul>"},{"location":"rdetoolkit/#interface","title":"Interface","text":"<ul> <li>filechecker: \u30d5\u30a1\u30a4\u30eb\u30c1\u30a7\u30c3\u30af\u6a5f\u80fd\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9</li> </ul>"},{"location":"rdetoolkit/#commands","title":"Commands","text":"<ul> <li>archive: \u30a2\u30fc\u30ab\u30a4\u30d6\u95a2\u9023\u306e\u30b3\u30de\u30f3\u30c9</li> <li>command: \u30b3\u30de\u30f3\u30c9\u51e6\u7406\u306e\u57fa\u672c\u6a5f\u80fd</li> <li>gen_excelinvoice: Excel\u9001\u308a\u72b6\u751f\u6210\u30b3\u30de\u30f3\u30c9</li> </ul>"},{"location":"rdetoolkit/#processing","title":"Processing","text":"<ul> <li>processing: \u30c7\u30fc\u30bf\u51e6\u7406\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3</li> <li>context: \u51e6\u7406\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u7ba1\u7406</li> <li>factories: \u30d7\u30ed\u30bb\u30c3\u30b5\u30d5\u30a1\u30af\u30c8\u30ea</li> <li>pipeline: \u51e6\u7406\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3</li> <li>processors: \u5404\u7a2e\u30d7\u30ed\u30bb\u30c3\u30b5<ul> <li>datasets: \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u51e6\u7406</li> <li>descriptions: \u8aac\u660e\u6587\u51e6\u7406</li> <li>files: \u30d5\u30a1\u30a4\u30eb\u51e6\u7406</li> <li>invoice: \u9001\u308a\u72b6\u51e6\u7406</li> <li>thumbnails: \u30b5\u30e0\u30cd\u30a4\u30eb\u51e6\u7406</li> <li>validation: \u691c\u8a3c\u51e6\u7406</li> <li>variables: \u5909\u6570\u51e6\u7406</li> </ul> </li> </ul>"},{"location":"rdetoolkit/#storage","title":"Storage","text":"<ul> <li>minio: MinIO \u30b9\u30c8\u30ec\u30fc\u30b8\u9023\u643a\u6a5f\u80fd</li> </ul>"},{"location":"rdetoolkit/#artifacts","title":"Artifacts","text":"<ul> <li>report: \u30ec\u30dd\u30fc\u30c8\u751f\u6210\u6a5f\u80fd</li> </ul>"},{"location":"rdetoolkit/config/","title":"Configuration Module","text":"<p>The <code>rdetoolkit.config</code> module provides comprehensive configuration management functionality for RDE (Research Data Exchange) processing workflows. This module handles loading, parsing, and validation of configuration files in multiple formats, with automatic discovery and flexible configuration options.</p>"},{"location":"rdetoolkit/config/#overview","title":"Overview","text":"<p>The config module offers robust configuration management capabilities:</p> <ul> <li>Multiple Format Support: Support for YAML, YML, and TOML configuration files</li> <li>Automatic Discovery: Intelligent configuration file discovery in directories</li> <li>Validation: Pydantic-based configuration validation with detailed error reporting</li> <li>Priority Loading: Configurable loading priority for different file types</li> <li>Default Configurations: Fallback to default configurations when files are missing</li> <li>Project Integration: Special support for pyproject.toml configuration sections</li> <li>Path Flexibility: Support for both string and Path object inputs</li> </ul>"},{"location":"rdetoolkit/config/#constants","title":"Constants","text":"<ul> <li><code>CONFIG_FILE</code>: <code>[\"rdeconfig.yaml\", \"rdeconfig.yml\"]</code> - Standard configuration filenames</li> <li><code>PYPROJECT_CONFIG_FILES</code>: <code>[\"pyproject.toml\"]</code> - Project configuration filenames</li> <li><code>CONFIG_FILES</code>: Combined list of all supported configuration filenames</li> </ul>"},{"location":"rdetoolkit/config/#functions","title":"Functions","text":""},{"location":"rdetoolkit/config/#parse_config_file","title":"parse_config_file","text":"<p>Parse a configuration file and return a validated Config object.</p> <pre><code>def parse_config_file(*, path: str | None = None) -&gt; Config\n</code></pre> <p>Parameters: - <code>path</code> (str | None): Path to the configuration file (optional)</p> <p>Returns: - <code>Config</code>: Parsed and validated configuration object</p> <p>Raises: - <code>FileNotFoundError</code>: If the specified configuration file does not exist</p> <p>File Loading Priority: 1. If <code>path</code> is provided with <code>.toml</code> extension: Read as TOML file 2. If <code>path</code> is provided with <code>.yaml</code>/<code>.yml</code> extension: Read as YAML file 3. If <code>path</code> is None: Search for <code>pyproject.toml</code> in current working directory 4. If file not found or invalid: Return default Config object</p> <p>Supported Configuration Files: - <code>rdeconfig.yaml</code> - <code>rdeconfig.yml</code> - <code>pyproject.toml</code></p> <p>Example:</p> <pre><code>from rdetoolkit.config import parse_config_file\n\n# Parse specific configuration file\nconfig = parse_config_file(path=\"config/rdeconfig.yaml\")\nprint(f\"Extended mode: {config.system.extended_mode}\")\n\n# Parse default pyproject.toml\ndefault_config = parse_config_file()\nprint(f\"Save raw: {config.system.save_raw}\")\n\n# Handle missing configuration gracefully\ntry:\n    config = parse_config_file(path=\"nonexistent.yaml\")\n    # Returns default config instead of raising error\nexcept FileNotFoundError:\n    print(\"Configuration file not found\")\n</code></pre>"},{"location":"rdetoolkit/config/#find_config_files","title":"find_config_files","text":"<p>Find and return a list of configuration files in a specified directory.</p> <pre><code>def find_config_files(target_dir_path: RdeFsPath) -&gt; list[str]\n</code></pre> <p>Parameters: - <code>target_dir_path</code> (RdeFsPath): Directory path to search for configuration files</p> <p>Returns: - <code>list[str]</code>: Sorted list of configuration file paths</p> <p>Sorting Priority: - Files are sorted by type: TOML files first, then YAML files - Within each type, files are sorted alphabetically</p> <p>Example:</p> <pre><code>from rdetoolkit.config import find_config_files\nfrom pathlib import Path\n\n# Find configuration files in directory\nconfig_dir = Path(\"data/tasksupport\")\nconfig_files = find_config_files(config_dir)\n\nfor config_file in config_files:\n    print(f\"Found config: {config_file}\")\n\n# Check if any configuration files exist\nif config_files:\n    print(f\"Found {len(config_files)} configuration files\")\nelse:\n    print(\"No configuration files found\")\n</code></pre>"},{"location":"rdetoolkit/config/#get_config","title":"get_config","text":"<p>Retrieve configuration from a specified directory with fallback options.</p> <pre><code>def get_config(target_dir_path: RdeFsPath) -&gt; Config | None\n</code></pre> <p>Parameters: - <code>target_dir_path</code> (RdeFsPath): Directory path to search for configuration</p> <p>Returns: - <code>Config | None</code>: First valid configuration found, or None if no valid configuration exists</p> <p>Raises: - <code>ValueError</code>: If configuration file exists but contains invalid data</p> <p>Search Strategy: 1. Search for configuration files in the specified directory 2. Parse each found file until a valid configuration is found 3. If no valid configuration in directory, check for <code>pyproject.toml</code> in current working directory 4. Return the first valid configuration, or None if none found</p> <p>Example:</p> <pre><code>from rdetoolkit.config import get_config\nfrom pathlib import Path\n\n# Get configuration from tasksupport directory\nconfig = get_config(\"data/tasksupport\")\n\nif config:\n    print(f\"Configuration loaded successfully\")\n    print(f\"Extended mode: {config.system.extended_mode}\")\n    print(f\"Save raw files: {config.system.save_raw}\")\nelse:\n    print(\"No valid configuration found\")\n\n# Handle validation errors\ntry:\n    config = get_config(\"data/invalid_config\")\nexcept ValueError as e:\n    print(f\"Configuration validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/config/#load_config","title":"load_config","text":"<p>Load configuration for RDE Toolkit with optional override.</p> <pre><code>def load_config(tasksupport_path: RdeFsPath, *, config: Config | None = None) -&gt; Config\n</code></pre> <p>Parameters: - <code>tasksupport_path</code> (RdeFsPath): Path to the tasksupport directory - <code>config</code> (Config | None): Optional existing configuration object to use instead</p> <p>Returns: - <code>Config</code>: Loaded configuration object (never None)</p> <p>Behavior: - If <code>config</code> parameter is provided, returns that configuration - Otherwise, attempts to load configuration from <code>tasksupport_path</code> - If no configuration is found, returns default Config object - Guarantees non-None return value</p> <p>Example:</p> <pre><code>from rdetoolkit.config import load_config\nfrom rdetoolkit.models.config import Config, SystemSettings\n\n# Load configuration from tasksupport directory\nconfig = load_config(\"data/tasksupport\")\n\n# Load with custom configuration override\ncustom_config = Config(\n    system=SystemSettings(extended_mode=\"rdeformat\", save_raw=True)\n)\nconfig = load_config(\"data/tasksupport\", config=custom_config)\n\n# Always returns a valid configuration\nprint(f\"Extended mode: {config.system.extended_mode}\")\n</code></pre>"},{"location":"rdetoolkit/config/#utility-functions","title":"Utility Functions","text":""},{"location":"rdetoolkit/config/#is_toml","title":"is_toml","text":"<p>Check if a filename has a TOML extension.</p> <pre><code>def is_toml(filename: str) -&gt; bool\n</code></pre> <p>Parameters: - <code>filename</code> (str): Filename to check</p> <p>Returns: - <code>bool</code>: True if filename ends with <code>.toml</code>, False otherwise</p>"},{"location":"rdetoolkit/config/#is_yaml","title":"is_yaml","text":"<p>Check if a filename has a YAML extension.</p> <pre><code>def is_yaml(filename: str) -&gt; bool\n</code></pre> <p>Parameters: - <code>filename</code> (str): Filename to check</p> <p>Returns: - <code>bool</code>: True if filename ends with <code>.yaml</code> or <code>.yml</code>, False otherwise</p>"},{"location":"rdetoolkit/config/#get_pyproject_toml","title":"get_pyproject_toml","text":"<p>Get the pyproject.toml file path from the current working directory.</p> <pre><code>def get_pyproject_toml() -&gt; Path | None\n</code></pre> <p>Returns: - <code>Path | None</code>: Path to pyproject.toml if it exists, None otherwise</p> <p>Example:</p> <pre><code>from rdetoolkit.config import is_toml, is_yaml, get_pyproject_toml\n\n# Check file types\nprint(is_toml(\"config.toml\"))        # True\nprint(is_yaml(\"config.yaml\"))       # True\nprint(is_yaml(\"config.yml\"))        # True\nprint(is_toml(\"config.json\"))       # False\n\n# Get pyproject.toml path\npyproject_path = get_pyproject_toml()\nif pyproject_path:\n    print(f\"Found pyproject.toml at: {pyproject_path}\")\nelse:\n    print(\"No pyproject.toml found in current directory\")\n</code></pre>"},{"location":"rdetoolkit/config/#configuration-file-formats","title":"Configuration File Formats","text":""},{"location":"rdetoolkit/config/#yaml-configuration-rdeconfigyamlrdeconfigyml","title":"YAML Configuration (rdeconfig.yaml/rdeconfig.yml)","text":"<pre><code>system:\n  extended_mode: \"rdeformat\"\n  save_raw: true\n  save_main_image: false\n  save_thumbnail_image: true\n  save_nonshared_raw: false\n  magic_variable: true\n\nmultidata_tile:\n  ignore_errors: false\n</code></pre>"},{"location":"rdetoolkit/config/#toml-configuration-pyprojecttoml","title":"TOML Configuration (pyproject.toml)","text":"<pre><code>[tool.rdetoolkit.system]\nextended_mode = \"multidatatile\"\nsave_raw = true\nsave_main_image = false\nsave_thumbnail_image = true\nsave_nonshared_raw = false\nmagic_variable = true\n\n[tool.rdetoolkit.multidata_tile]\nignore_errors = false\n</code></pre>"},{"location":"rdetoolkit/config/#configuration-loading-examples","title":"Configuration Loading Examples","text":""},{"location":"rdetoolkit/config/#basic-configuration-loading","title":"Basic Configuration Loading","text":"<pre><code>from rdetoolkit.config import load_config, parse_config_file\n\n# Load from tasksupport directory\nconfig = load_config(\"data/tasksupport\")\n\n# Parse specific file\nconfig = parse_config_file(path=\"config/custom.yaml\")\n\n# Use configuration\nif config.system.extended_mode == \"rdeformat\":\n    print(\"Using RDE format mode\")\n\nif config.system.save_raw:\n    print(\"Raw files will be saved\")\n</code></pre>"},{"location":"rdetoolkit/config/#configuration-discovery-and-validation","title":"Configuration Discovery and Validation","text":"<pre><code>from rdetoolkit.config import find_config_files, get_config\nfrom pathlib import Path\n\ndef setup_configuration(config_dir: str):\n    \"\"\"Setup configuration with discovery and validation.\"\"\"\n\n    # Find available configuration files\n    config_files = find_config_files(config_dir)\n    print(f\"Available configurations: {config_files}\")\n\n    # Load configuration with validation\n    try:\n        config = get_config(config_dir)\n        if config:\n            print(\"\u2713 Configuration loaded successfully\")\n            return config\n        else:\n            print(\"\u26a0 No configuration found, using defaults\")\n            from rdetoolkit.models.config import Config\n            return Config()\n\n    except ValueError as e:\n        print(f\"\u2717 Configuration validation failed: {e}\")\n        return None\n\n# Usage\nconfig = setup_configuration(\"data/tasksupport\")\nif config:\n    print(f\"Extended mode: {config.system.extended_mode}\")\n</code></pre>"},{"location":"rdetoolkit/config/#configuration-priority-and-fallback","title":"Configuration Priority and Fallback","text":"<pre><code>from rdetoolkit.config import load_config\nfrom rdetoolkit.models.config import Config, SystemSettings, MultiDataTileSettings\n\ndef load_configuration_with_fallback(primary_path: str, fallback_path: str = None):\n    \"\"\"Load configuration with fallback options.\"\"\"\n\n    # Try primary configuration\n    try:\n        config = load_config(primary_path)\n        if config.system.extended_mode or config.system.save_raw:\n            print(f\"\u2713 Loaded configuration from: {primary_path}\")\n            return config\n    except Exception as e:\n        print(f\"\u26a0 Primary configuration failed: {e}\")\n\n    # Try fallback configuration\n    if fallback_path:\n        try:\n            config = load_config(fallback_path)\n            print(f\"\u2713 Loaded fallback configuration from: {fallback_path}\")\n            return config\n        except Exception as e:\n            print(f\"\u26a0 Fallback configuration failed: {e}\")\n\n    # Use programmatic default\n    print(\"Using programmatic default configuration\")\n    return Config(\n        system=SystemSettings(\n            extended_mode=\"invoice\",\n            save_raw=True,\n            magic_variable=True\n        ),\n        multidata_tile=MultiDataTileSettings(\n            ignore_errors=False\n        )\n    )\n\n# Usage\nconfig = load_configuration_with_fallback(\n    primary_path=\"data/tasksupport\",\n    fallback_path=\"config\"\n)\n</code></pre>"},{"location":"rdetoolkit/config/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/config/#configuration-validation-errors","title":"Configuration Validation Errors","text":"<pre><code>from rdetoolkit.config import get_config\nfrom pydantic import ValidationError\n\ndef safe_config_loading(config_path: str):\n    \"\"\"Safely load configuration with comprehensive error handling.\"\"\"\n\n    try:\n        config = get_config(config_path)\n        if config is None:\n            print(\"No configuration found\")\n            return None\n\n        # Validate specific settings\n        if config.system.extended_mode not in [\"invoice\", \"rdeformat\", \"multidatatile\"]:\n            print(f\"Warning: Unknown extended_mode: {config.system.extended_mode}\")\n\n        return config\n\n    except ValueError as e:\n        print(f\"Configuration validation error: {e}\")\n        return None\n    except FileNotFoundError as e:\n        print(f\"Configuration file not found: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected configuration error: {e}\")\n        return None\n\n# Usage\nconfig = safe_config_loading(\"data/tasksupport\")\nif config:\n    print(\"Configuration loaded successfully\")\nelse:\n    print(\"Using default configuration\")\n</code></pre>"},{"location":"rdetoolkit/config/#file-format-detection-and-handling","title":"File Format Detection and Handling","text":"<pre><code>from rdetoolkit.config import is_toml, is_yaml, parse_config_file\n\ndef load_config_by_type(config_file: str):\n    \"\"\"Load configuration with format-specific handling.\"\"\"\n\n    if is_toml(config_file):\n        print(f\"Loading TOML configuration: {config_file}\")\n        try:\n            return parse_config_file(path=config_file)\n        except Exception as e:\n            print(f\"TOML parsing failed: {e}\")\n            return None\n\n    elif is_yaml(config_file):\n        print(f\"Loading YAML configuration: {config_file}\")\n        try:\n            return parse_config_file(path=config_file)\n        except Exception as e:\n            print(f\"YAML parsing failed: {e}\")\n            return None\n\n    else:\n        print(f\"Unsupported configuration format: {config_file}\")\n        return None\n\n# Usage\nconfig_files = [\"config.toml\", \"config.yaml\", \"config.json\"]\nfor config_file in config_files:\n    config = load_config_by_type(config_file)\n    if config:\n        print(f\"Successfully loaded: {config_file}\")\n        break\n</code></pre>"},{"location":"rdetoolkit/config/#best-practices","title":"Best Practices","text":""},{"location":"rdetoolkit/config/#configuration-management","title":"Configuration Management","text":"<pre><code># Always use load_config for guaranteed non-None results\nconfig = load_config(\"data/tasksupport\")\n\n# Use get_config when you need to handle None results\nconfig = get_config(\"data/tasksupport\")\nif config is None:\n    # Handle missing configuration\n    pass\n\n# Validate configuration after loading\nif config.system.extended_mode not in [\"invoice\", \"rdeformat\", \"multidatatile\"]:\n    raise ValueError(f\"Invalid extended_mode: {config.system.extended_mode}\")\n</code></pre>"},{"location":"rdetoolkit/config/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code>def load_environment_config(environment: str = \"production\"):\n    \"\"\"Load configuration based on environment.\"\"\"\n\n    config_paths = {\n        \"development\": \"config/dev\",\n        \"testing\": \"config/test\",\n        \"production\": \"data/tasksupport\"\n    }\n\n    config_path = config_paths.get(environment, \"data/tasksupport\")\n    return load_config(config_path)\n\n# Usage\nimport os\nenv = os.getenv(\"RDE_ENVIRONMENT\", \"production\")\nconfig = load_environment_config(env)\n</code></pre>"},{"location":"rdetoolkit/config/#see-also","title":"See Also","text":"<ul> <li>Models - Config - For Config, SystemSettings, and MultiDataTileSettings data structures</li> <li>Models - RDE2 Types - For RdeFsPath type definitions</li> <li>Workflows - For configuration usage in workflow processing</li> <li>Usage - Configuration - For practical configuration examples and patterns</li> <li>Usage - Mode - For configuration mode usage examples</li> </ul>"},{"location":"rdetoolkit/core/","title":"Core Module","text":"<p>The <code>rdetoolkit.core</code> module provides essential functionality implemented in Rust for high-performance operations. This module includes directory management utilities, image processing functions, and file encoding detection capabilities.</p>"},{"location":"rdetoolkit/core/#overview","title":"Overview","text":"<p>The core module is built using PyO3 and provides Python bindings for Rust implementations of performance-critical operations:</p> <ul> <li>Directory Management: Efficient directory creation and management with support for indexed subdirectories</li> <li>Image Processing: High-performance image resizing with aspect ratio preservation</li> <li>File Operations: Fast encoding detection and file reading with automatic encoding handling</li> </ul>"},{"location":"rdetoolkit/core/#classes","title":"Classes","text":""},{"location":"rdetoolkit/core/#manageddirectory","title":"ManagedDirectory","text":"<p>A directory manager that handles index-based subdirectories with automatic path construction.</p>"},{"location":"rdetoolkit/core/#constructor","title":"Constructor","text":"<pre><code>ManagedDirectory(base_dir: str, dirname: str, n_digit: Optional[int] = None, idx: Optional[int] = None)\n</code></pre> <p>Parameters: - <code>base_dir</code> (str): Base directory path - <code>dirname</code> (str): Directory name to manage - <code>n_digit</code> (Optional[int]): Number of digits for index formatting (default: 4) - <code>idx</code> (Optional[int]): Initial index (default: 0)</p>"},{"location":"rdetoolkit/core/#attributes","title":"Attributes","text":"<ul> <li><code>path</code> (str): The full path to the managed directory</li> <li><code>idx</code> (int): Current index of the directory</li> </ul>"},{"location":"rdetoolkit/core/#methods","title":"Methods","text":""},{"location":"rdetoolkit/core/#create","title":"create()","text":"<p>Create the managed directory if it doesn't exist.</p> <pre><code>def create() -&gt; None\n</code></pre> <p>Raises: - <code>OSError</code>: If directory creation fails</p> <p>Example: <pre><code>from rdetoolkit.core import ManagedDirectory\n\n# Create a managed directory\nmanaged_dir = ManagedDirectory(\"/data\", \"output\")\nmanaged_dir.create()  # Creates /data/output\n</code></pre></p>"},{"location":"rdetoolkit/core/#list","title":"list()","text":"<p>List all files and directories in the managed directory.</p> <pre><code>def list() -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of paths as strings</p> <p>Raises: - <code>FileNotFoundError</code>: If the directory does not exist - <code>OSError</code>: If reading the directory fails</p> <p>Example: <pre><code># List contents of the managed directory\ncontents = managed_dir.list()\nprint(contents)  # ['file1.txt', 'subdir', ...]\n</code></pre></p>"},{"location":"rdetoolkit/core/#__call__idx","title":"__call__(idx)","text":"<p>Create a new ManagedDirectory instance with the specified index.</p> <pre><code>def __call__(idx: int) -&gt; ManagedDirectory\n</code></pre> <p>Parameters: - <code>idx</code> (int): Index for the new directory</p> <p>Returns: - <code>ManagedDirectory</code>: New instance with the specified index</p> <p>Raises: - <code>ValueError</code>: If idx is negative - <code>OSError</code>: If directory creation fails</p> <p>Example: <pre><code># Create indexed subdirectories\nbase_dir = ManagedDirectory(\"/data\", \"output\")\nindexed_dir = base_dir(1)  # Creates /data/divided/0001/output\n</code></pre></p>"},{"location":"rdetoolkit/core/#directory-path-structure","title":"Directory Path Structure","text":"<p>The directory path is constructed as follows:</p> <ul> <li>For idx=0: <code>{base_dir}/{dirname}</code></li> <li>For idx&gt;0: <code>{base_dir}/divided/{idx:0{n_digit}d}/{dirname}</code></li> </ul> <p>Example: <pre><code># Base directory (idx=0)\ndir0 = ManagedDirectory(\"/data\", \"docs\")\nprint(dir0.path)  # \"/data/docs\"\n\n# Indexed directory\ndir1 = dir0(1)\nprint(dir1.path)  # \"/data/divided/0001/docs\"\n\ndir2 = dir0(10)\nprint(dir2.path)  # \"/data/divided/0010/docs\"\n</code></pre></p>"},{"location":"rdetoolkit/core/#directoryops","title":"DirectoryOps","text":"<p>Utility class for managing multiple directories with support for indexed subdirectories.</p>"},{"location":"rdetoolkit/core/#constructor_1","title":"Constructor","text":"<pre><code>DirectoryOps(base_dir: str, n_digit: Optional[int] = None)\n</code></pre> <p>Parameters: - <code>base_dir</code> (str): Base directory path - <code>n_digit</code> (Optional[int]): Number of digits for index formatting (default: 4)</p>"},{"location":"rdetoolkit/core/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/core/#__getattr__name","title":"__getattr__(name)","text":"<p>Access directories via attribute-style notation.</p> <pre><code>def __getattr__(name: str) -&gt; ManagedDirectory\n</code></pre> <p>Parameters: - <code>name</code> (str): Directory name</p> <p>Returns: - <code>ManagedDirectory</code>: New ManagedDirectory instance for the specified directory</p> <p>Example: <pre><code>from rdetoolkit.core import DirectoryOps\n\nops = DirectoryOps(\"/data\")\ninvoice_dir = ops.invoice  # Creates ManagedDirectory for \"invoice\"\nraw_dir = ops.raw         # Creates ManagedDirectory for \"raw\"\n</code></pre></p>"},{"location":"rdetoolkit/core/#allidx","title":"all(idx)","text":"<p>Create all supported directories and optionally their indexed subdirectories.</p> <pre><code>def all(idx: Optional[int] = None) -&gt; list[str]\n</code></pre> <p>Parameters: - <code>idx</code> (Optional[int]): Maximum index for divided subdirectories</p> <p>Returns: - <code>list[str]</code>: List of created directory paths</p> <p>Raises: - <code>ValueError</code>: If idx is negative - <code>OSError</code>: If directory creation fails</p> <p>Example: <pre><code># Create all base directories\nops = DirectoryOps(\"/data\")\ncreated_dirs = ops.all()\n\n# Create base directories and indexed subdirectories\nindexed_dirs = ops.all(2)  # Creates base + divided/0001, divided/0002 subdirs\n</code></pre></p>"},{"location":"rdetoolkit/core/#supported-directory-types","title":"Supported Directory Types","text":"<p>The following directories are automatically supported:</p> <ul> <li><code>invoice</code></li> <li><code>raw</code></li> <li><code>structured</code></li> <li><code>main_image</code></li> <li><code>other_image</code></li> <li><code>thumbnail</code></li> <li><code>meta</code></li> <li><code>logs</code></li> <li><code>temp</code></li> <li><code>nonshared_raw</code></li> <li><code>invoice_patch</code></li> <li><code>attachment</code></li> </ul>"},{"location":"rdetoolkit/core/#functions","title":"Functions","text":""},{"location":"rdetoolkit/core/#resize_image_aspect_ratio","title":"resize_image_aspect_ratio","text":"<p>Resize an image while preserving aspect ratio.</p> <pre><code>def resize_image_aspect_ratio(input_path: str, output_path: str, width: int, height: int) -&gt; None\n</code></pre> <p>Parameters: - <code>input_path</code> (str): Path to the input image file - <code>output_path</code> (str): Path where the resized image will be saved - <code>width</code> (int): Target width in pixels - <code>height</code> (int): Target height in pixels</p> <p>Raises: - <code>ValueError</code>: If width or height is zero or negative - <code>IOError</code>: If input file cannot be read or output file cannot be written - <code>OSError</code>: If file operations fail</p> <p>Example: <pre><code>from rdetoolkit.core import resize_image_aspect_ratio\n\n# Resize image to fit within 800x600 while preserving aspect ratio\nresize_image_aspect_ratio(\n    \"input.jpg\",\n    \"output.jpg\",\n    800,\n    600\n)\n</code></pre></p>"},{"location":"rdetoolkit/core/#detect_encoding","title":"detect_encoding","text":"<p>Detect the character encoding of a text file.</p> <pre><code>def detect_encoding(path: str) -&gt; str\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to the file</p> <p>Returns: - <code>str</code>: Detected encoding name (e.g., \"utf-8\", \"shift_jis\", \"euc-jp\")</p> <p>Raises: - <code>IOError</code>: If file not found or cannot be read - <code>OSError</code>: If file operations fail</p> <p>Example: <pre><code>from rdetoolkit.core import detect_encoding\n\n# Detect encoding of a text file\nencoding = detect_encoding(\"data.csv\")\nprint(f\"File encoding: {encoding}\")  # File encoding: utf-8\n</code></pre></p>"},{"location":"rdetoolkit/core/#read_file_with_encoding","title":"read_file_with_encoding","text":"<p>Read a file with automatic encoding detection.</p> <pre><code>def read_file_with_encoding(file_path: str) -&gt; str\n</code></pre> <p>Parameters: - <code>file_path</code> (str): Path to the file to read</p> <p>Returns: - <code>str</code>: File contents as a string</p> <p>Raises: - <code>IOError</code>: If file not found or cannot be read - <code>UnicodeDecodeError</code>: If file cannot be decoded - <code>OSError</code>: If file operations fail</p> <p>Example: <pre><code>from rdetoolkit.core import read_file_with_encoding\n\n# Read file with automatic encoding detection\ncontent = read_file_with_encoding(\"data.txt\")\nprint(content)\n</code></pre></p>"},{"location":"rdetoolkit/core/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/core/#basic-directory-management","title":"Basic Directory Management","text":"<pre><code>from rdetoolkit.core import DirectoryOps\n\n# Initialize directory operations\nops = DirectoryOps(\"/project/data\")\n\n# Create individual directories\ninvoice_dir = ops.invoice\ninvoice_dir.create()\n\nraw_dir = ops.raw\nraw_dir.create()\n\n# Create all standard directories\nall_dirs = ops.all()\nprint(f\"Created {len(all_dirs)} directories\")\n</code></pre>"},{"location":"rdetoolkit/core/#working-with-indexed-directories","title":"Working with Indexed Directories","text":"<pre><code>from rdetoolkit.core import DirectoryOps\n\nops = DirectoryOps(\"/project/data\")\n\n# Create directories for multiple datasets\nfor i in range(1, 4):\n    # Create indexed subdirectories\n    invoice_dir = ops.invoice(i)\n    raw_dir = ops.raw(i)\n\n    invoice_dir.create()  # /project/data/divided/0001/invoice\n    raw_dir.create()      # /project/data/divided/0001/raw\n\n    print(f\"Created directories for dataset {i}\")\n</code></pre>"},{"location":"rdetoolkit/core/#image-processing-workflow","title":"Image Processing Workflow","text":"<pre><code>from rdetoolkit.core import resize_image_aspect_ratio, DirectoryOps\nfrom pathlib import Path\n\n# Setup directories\nops = DirectoryOps(\"/project/data\")\nmain_image_dir = ops.main_image\nthumbnail_dir = ops.thumbnail\n\nmain_image_dir.create()\nthumbnail_dir.create()\n\n# Process images\ninput_images = Path(\"input\").glob(\"*.jpg\")\n\nfor img_path in input_images:\n    # Create main image (resized)\n    main_output = Path(main_image_dir.path) / img_path.name\n    resize_image_aspect_ratio(\n        str(img_path),\n        str(main_output),\n        1920, 1080\n    )\n\n    # Create thumbnail\n    thumb_output = Path(thumbnail_dir.path) / f\"thumb_{img_path.name}\"\n    resize_image_aspect_ratio(\n        str(img_path),\n        str(thumb_output),\n        300, 200\n    )\n</code></pre>"},{"location":"rdetoolkit/core/#file-encoding-detection-and-processing","title":"File Encoding Detection and Processing","text":"<pre><code>from rdetoolkit.core import detect_encoding, read_file_with_encoding\nfrom pathlib import Path\n\ndef process_text_files(directory: str):\n    \"\"\"Process all text files in a directory with encoding detection.\"\"\"\n\n    for file_path in Path(directory).glob(\"*.txt\"):\n        try:\n            # Detect encoding\n            encoding = detect_encoding(str(file_path))\n            print(f\"{file_path.name}: {encoding}\")\n\n            # Read with automatic encoding handling\n            content = read_file_with_encoding(str(file_path))\n\n            # Process content\n            lines = content.splitlines()\n            print(f\"  Lines: {len(lines)}\")\n\n        except Exception as e:\n            print(f\"Error processing {file_path.name}: {e}\")\n\n# Usage\nprocess_text_files(\"/data/input\")\n</code></pre>"},{"location":"rdetoolkit/core/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/core/#common-exceptions","title":"Common Exceptions","text":"<p>The core module functions may raise the following exceptions:</p>"},{"location":"rdetoolkit/core/#oserror","title":"OSError","text":"<p>Raised for general file system operations failures: <pre><code>try:\n    managed_dir.create()\nexcept OSError as e:\n    print(f\"Failed to create directory: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/core/#valueerror","title":"ValueError","text":"<p>Raised for invalid parameter values: <pre><code>try:\n    resize_image_aspect_ratio(\"input.jpg\", \"output.jpg\", 0, 100)\nexcept ValueError as e:\n    print(f\"Invalid dimensions: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/core/#ioerror","title":"IOError","text":"<p>Raised for file I/O operations: <pre><code>try:\n    encoding = detect_encoding(\"nonexistent.txt\")\nexcept IOError as e:\n    print(f\"File not found: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/core/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always handle exceptions when working with file operations:    <pre><code>try:\n    content = read_file_with_encoding(file_path)\nexcept (IOError, UnicodeDecodeError) as e:\n    print(f\"Failed to read {file_path}: {e}\")\n</code></pre></p> </li> <li> <p>Validate inputs before calling functions:    <pre><code>if width &gt; 0 and height &gt; 0:\n    resize_image_aspect_ratio(input_path, output_path, width, height)\nelse:\n    print(\"Invalid dimensions\")\n</code></pre></p> </li> <li> <p>Use Path objects for better path handling:    <pre><code>from pathlib import Path\n\ninput_path = Path(\"data\") / \"input.jpg\"\nif input_path.exists():\n    resize_image_aspect_ratio(str(input_path), str(output_path), 800, 600)\n</code></pre></p> </li> <li> <p>Create parent directories when needed:    <pre><code>output_path = Path(\"output\") / \"processed\" / \"image.jpg\"\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nresize_image_aspect_ratio(str(input_path), str(output_path), 800, 600)\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/core/#performance-notes","title":"Performance Notes","text":"<ul> <li>All functions in this module are implemented in Rust for optimal performance</li> <li>Image processing operations use efficient algorithms with minimal memory allocation</li> <li>Directory operations are optimized for batch creation of multiple directories</li> <li>Encoding detection uses fast heuristic algorithms suitable for large files</li> </ul>"},{"location":"rdetoolkit/core/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - For configuring directory structures</li> <li>Workflows - For integrating core functions into processing workflows</li> <li>File Operations - For additional file handling utilities</li> </ul>"},{"location":"rdetoolkit/errors/","title":"errors","text":""},{"location":"rdetoolkit/errors/#catch_exception_with_message","title":"catch_exception_with_message","text":""},{"location":"rdetoolkit/errors/#src.rdetoolkit.errors.catch_exception_with_message","title":"<code>src.rdetoolkit.errors.catch_exception_with_message(*, error_message=None, error_code=None, eobj=None, verbose=False)</code>","text":"<p>A decorator that catches exceptions and re-raises a StructuredError with a customized message and error code.</p> <p>This decorator catches exceptions thrown within the decorated function. If a StructuredError is raised, it re-raises it with the specified error message, error code, and optional additional error object. For other exceptions, it re-raises them as standard Exceptions. The verbosity level of the error message can be controlled via the verbose parameter.</p> <p>Parameters:</p> Name Type Description Default <code>error_message</code> <code>Optional[str]</code> <p>Customized message to be used in case of an error. Defaults to None.</p> <code>None</code> <code>error_code</code> <code>Optional[int]</code> <p>Error code to be used in case of an error. Defaults to None.</p> <code>None</code> <code>eobj</code> <code>Optional[Any]</code> <p>Additional object to include in the error. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If set to True, provides detailed error messages. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A function decorator that provides customized error handling on exception occurrence.</p>"},{"location":"rdetoolkit/errors/#format_simplified_traceback","title":"format_simplified_traceback","text":""},{"location":"rdetoolkit/errors/#src.rdetoolkit.errors.format_simplified_traceback","title":"<code>src.rdetoolkit.errors.format_simplified_traceback(tb_list)</code>","text":"<p>Formats a simplified version of the traceback information.</p> <p>This function takes a list of traceback frame summaries and constructs a formatted string representing the call stack. The formatted string includes indentation and node characters to indicate the call path, highlighting the file, line number, and function name. The final line of the traceback is marked with a fire emoji.</p> <p>Parameters:</p> Name Type Description Default <code>tb_list</code> <code>list[FrameSummary]</code> <p>A list of traceback frame summaries to format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string representing the simplified traceback information.</p>"},{"location":"rdetoolkit/errors/#handle_exception","title":"handle_exception","text":""},{"location":"rdetoolkit/errors/#src.rdetoolkit.errors.handle_exception","title":"<code>src.rdetoolkit.errors.handle_exception(e, error_message=None, error_code=None, eobj=None, verbose=False)</code>","text":"<p>Handles exceptions and formats them into a StructuredError with optional custom message, error code, and additional object.</p> <p>This function captures the exception type and traceback, then formats a simplified version of the traceback. It constructs a custom error message, optionally including the full original traceback if verbose mode is enabled. The function returns a StructuredError containing the error message, error code, optional additional object, and simplified traceback information.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>The exception to handle.</p> required <code>error_message</code> <code>Optional[str]</code> <p>Customized message to be used in case of an error. Defaults to the exception message.</p> <code>None</code> <code>error_code</code> <code>Optional[int]</code> <p>Error code to be used in case of an error. Defaults to 1.</p> <code>None</code> <code>eobj</code> <code>Optional[Any]</code> <p>Additional object to include in the error. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If set to True, includes the original traceback in the error message. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>StructuredError</code> <code>StructuredError</code> <p>A structured error object containing the error message, error code, additional object,</p> <code>StructuredError</code> <p>and simplified traceback information.</p>"},{"location":"rdetoolkit/errors/#handle_and_exit_on_structured_error","title":"handle_and_exit_on_structured_error","text":""},{"location":"rdetoolkit/errors/#src.rdetoolkit.errors.handle_and_exit_on_structured_error","title":"<code>src.rdetoolkit.errors.handle_and_exit_on_structured_error(e, logger)</code>","text":"<p>Catch StructuredError and write to log file.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>StructuredError</code> <p>StructuredError instance</p> required <code>logger</code> <code>Logger</code> <p>Logger instance</p> required"},{"location":"rdetoolkit/errors/#handle_generic_error","title":"handle_generic_error","text":""},{"location":"rdetoolkit/errors/#src.rdetoolkit.errors.handle_generic_error","title":"<code>src.rdetoolkit.errors.handle_generic_error(e, logger)</code>","text":"<p>Catch generic error and write to log file.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>Exception instance</p> required <code>logger</code> <code>Logger</code> <p>Logger instance</p> required"},{"location":"rdetoolkit/errors/#write_job_errorlog_file","title":"write_job_errorlog_file","text":""},{"location":"rdetoolkit/errors/#src.rdetoolkit.errors.write_job_errorlog_file","title":"<code>src.rdetoolkit.errors.write_job_errorlog_file(code, message, *, filename='job.failed')</code>","text":"<p>Write the error log to a file.</p> <p>This function writes the given error code and message to a specified file. The file will be saved in a directory determined by <code>StorageDir.get_datadir(False)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>int</code> <p>The error code to be written to the log file.</p> required <code>message</code> <code>str</code> <p>The error message to be written to the log file.</p> required <code>filename</code> <code>str</code> <p>The name of the file to which the error log will be written. Defaults to \"job.failed\".</p> <code>'job.failed'</code> Example <pre><code>write_job_errorlog_file(404, 'Not Found', filename='error.log')\n</code></pre>"},{"location":"rdetoolkit/exceptions/","title":"Exceptions Module","text":"<p>The <code>rdetoolkit.exceptions</code> module provides a comprehensive set of custom exception classes for RDE (Research Data Exchange) processing workflows. These exceptions enable precise error handling, detailed error reporting, and structured error management across all RDE operations.</p>"},{"location":"rdetoolkit/exceptions/#overview","title":"Overview","text":"<p>The exceptions module offers specialized exception types for different RDE components and operations:</p> <ul> <li>Structured Error Handling: Base exception with detailed error information including codes and context</li> <li>Mode-Specific Exceptions: Specialized exceptions for different processing modes (Invoice, Excel, MultiDataTile, RDE Format)</li> <li>Validation Exceptions: Dedicated exceptions for schema and metadata validation errors</li> <li>Data Retrieval Exceptions: Specialized exceptions for data access and search operations</li> <li>Enhanced Error Context: Support for error codes, additional objects, and traceback information</li> <li>Consistent Error Messaging: Standardized error message formatting across all exception types</li> </ul>"},{"location":"rdetoolkit/exceptions/#exception-classes","title":"Exception Classes","text":""},{"location":"rdetoolkit/exceptions/#structurederror","title":"StructuredError","text":"<p>A comprehensive base exception class providing structured error information with detailed context.</p>"},{"location":"rdetoolkit/exceptions/#constructor","title":"Constructor","text":"<pre><code>StructuredError(\n    emsg: str = \"\",\n    ecode: int = 1,\n    eobj: Any | None = None,\n    traceback_info: str | None = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>emsg</code> (str): The error message describing what went wrong - <code>ecode</code> (int): Numeric error code for programmatic error identification (default: 1) - <code>eobj</code> (Any | None): Additional error object providing context (optional) - <code>traceback_info</code> (str | None): Additional traceback information (optional)</p> <p>Attributes: - <code>emsg</code> (str): The error message - <code>ecode</code> (int): The error code - <code>eobj</code> (Any | None): Additional error object - <code>traceback_info</code> (str | None): Traceback information</p> <p>Use Cases: - General RDE processing errors - File operation failures - Configuration errors - Data processing failures - Base class for more specific exceptions</p> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import StructuredError\nimport traceback\n\n# Basic usage\ntry:\n    # Some operation that might fail\n    if not file_exists:\n        raise StructuredError(\"Required file not found\", ecode=404)\nexcept StructuredError as e:\n    print(f\"Error {e.ecode}: {e.emsg}\")\n\n# Enhanced usage with context object\ndef process_data_file(file_path: str, config: dict) -&gt; dict:\n    \"\"\"Process data file with structured error handling.\"\"\"\n    try:\n        # Validate file\n        if not Path(file_path).exists():\n            raise StructuredError(\n                emsg=f\"Data file not found: {file_path}\",\n                ecode=404,\n                eobj={\"file_path\": file_path, \"config\": config}\n            )\n\n        # Validate configuration\n        required_keys = [\"input_format\", \"output_format\", \"validation_rules\"]\n        missing_keys = [key for key in required_keys if key not in config]\n        if missing_keys:\n            raise StructuredError(\n                emsg=f\"Missing required configuration keys: {missing_keys}\",\n                ecode=400,\n                eobj={\"missing_keys\": missing_keys, \"provided_config\": config}\n            )\n\n        # Process file (example)\n        return {\"status\": \"success\", \"processed_file\": file_path}\n\n    except Exception as e:\n        # Wrap unexpected errors\n        raise StructuredError(\n            emsg=f\"Unexpected error during file processing: {str(e)}\",\n            ecode=500,\n            eobj={\"original_exception\": e, \"file_path\": file_path},\n            traceback_info=traceback.format_exc()\n        ) from e\n\n# Usage with comprehensive error handling\ndef robust_file_processor():\n    \"\"\"Demonstrate robust file processing with StructuredError.\"\"\"\n\n    files_to_process = [\n        (\"data/valid_file.json\", {\"input_format\": \"json\", \"output_format\": \"csv\", \"validation_rules\": [\"schema\"]}),\n        (\"data/missing_file.json\", {\"input_format\": \"json\", \"output_format\": \"csv\"}),\n        (\"data/another_file.json\", {\"input_format\": \"json\", \"output_format\": \"csv\", \"validation_rules\": [\"schema\"]})\n    ]\n\n    results = []\n\n    for file_path, config in files_to_process:\n        try:\n            result = process_data_file(file_path, config)\n            results.append({\"file\": file_path, \"status\": \"success\", \"result\": result})\n            print(f\"\u2713 Successfully processed: {file_path}\")\n\n        except StructuredError as e:\n            error_info = {\n                \"file\": file_path,\n                \"status\": \"error\",\n                \"error_code\": e.ecode,\n                \"error_message\": e.emsg,\n                \"error_context\": e.eobj\n            }\n            results.append(error_info)\n\n            print(f\"\u2717 Error processing {file_path}:\")\n            print(f\"  Code: {e.ecode}\")\n            print(f\"  Message: {e.emsg}\")\n            if e.eobj:\n                print(f\"  Context: {e.eobj}\")\n            if e.traceback_info:\n                print(f\"  Traceback: {e.traceback_info}\")\n\n    return results\n\n# Advanced error context usage\nclass DataProcessor:\n    \"\"\"Example class demonstrating StructuredError usage.\"\"\"\n\n    def __init__(self, processor_id: str):\n        self.processor_id = processor_id\n        self.processed_count = 0\n        self.error_count = 0\n\n    def process_item(self, item: dict) -&gt; dict:\n        \"\"\"Process a single item with detailed error context.\"\"\"\n        try:\n            # Validate item structure\n            required_fields = [\"id\", \"type\", \"data\"]\n            for field in required_fields:\n                if field not in item:\n                    raise StructuredError(\n                        emsg=f\"Missing required field: {field}\",\n                        ecode=400,\n                        eobj={\n                            \"processor_id\": self.processor_id,\n                            \"item\": item,\n                            \"required_fields\": required_fields,\n                            \"processed_count\": self.processed_count\n                        }\n                    )\n\n            # Process based on type\n            if item[\"type\"] == \"numeric\":\n                result = self._process_numeric(item[\"data\"])\n            elif item[\"type\"] == \"text\":\n                result = self._process_text(item[\"data\"])\n            else:\n                raise StructuredError(\n                    emsg=f\"Unknown item type: {item['type']}\",\n                    ecode=422,\n                    eobj={\n                        \"processor_id\": self.processor_id,\n                        \"item_id\": item.get(\"id\"),\n                        \"unknown_type\": item[\"type\"],\n                        \"supported_types\": [\"numeric\", \"text\"]\n                    }\n                )\n\n            self.processed_count += 1\n            return {\"id\": item[\"id\"], \"status\": \"processed\", \"result\": result}\n\n        except StructuredError:\n            self.error_count += 1\n            raise\n        except Exception as e:\n            self.error_count += 1\n            raise StructuredError(\n                emsg=f\"Unexpected processing error: {str(e)}\",\n                ecode=500,\n                eobj={\n                    \"processor_id\": self.processor_id,\n                    \"item\": item,\n                    \"original_exception\": str(e),\n                    \"processed_count\": self.processed_count,\n                    \"error_count\": self.error_count\n                }\n            ) from e\n\n    def _process_numeric(self, data: any) -&gt; float:\n        \"\"\"Process numeric data.\"\"\"\n        try:\n            return float(data) * 2  # Example processing\n        except (ValueError, TypeError) as e:\n            raise StructuredError(\n                emsg=f\"Invalid numeric data: {data}\",\n                ecode=422,\n                eobj={\"data\": data, \"data_type\": type(data).__name__}\n            ) from e\n\n    def _process_text(self, data: any) -&gt; str:\n        \"\"\"Process text data.\"\"\"\n        try:\n            return str(data).upper()  # Example processing\n        except Exception as e:\n            raise StructuredError(\n                emsg=f\"Text processing failed: {str(e)}\",\n                ecode=422,\n                eobj={\"data\": data, \"data_type\": type(data).__name__}\n            ) from e\n\n# Usage example\nprocessor = DataProcessor(\"PROC_001\")\ntest_items = [\n    {\"id\": \"1\", \"type\": \"numeric\", \"data\": \"42.5\"},\n    {\"id\": \"2\", \"type\": \"text\", \"data\": \"hello world\"},\n    {\"id\": \"3\", \"type\": \"unknown\", \"data\": \"test\"},  # Will cause error\n    {\"id\": \"4\", \"data\": \"missing type\"},  # Will cause error\n    {\"id\": \"5\", \"type\": \"numeric\", \"data\": \"invalid\"}  # Will cause error\n]\n\nfor item in test_items:\n    try:\n        result = processor.process_item(item)\n        print(f\"\u2713 {result}\")\n    except StructuredError as e:\n        print(f\"\u2717 Item {item.get('id', 'unknown')}: {e.emsg} (Code: {e.ecode})\")\n</code></pre>"},{"location":"rdetoolkit/exceptions/#mode-specific-exception-classes","title":"Mode-Specific Exception Classes","text":""},{"location":"rdetoolkit/exceptions/#invoicemodeerror","title":"InvoiceModeError","text":"<p>Exception for errors in standard invoice processing mode.</p> <pre><code>InvoiceModeError(\n    emsg: str = \"\",\n    ecode: int = 100,\n    eobj: Any | None = None,\n    traceback_info: str | None = None\n) -&gt; None\n</code></pre> <p>Default Error Code: 100</p> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import InvoiceModeError\n\ndef process_invoice_file(invoice_path: str) -&gt; dict:\n    \"\"\"Process invoice file with mode-specific error handling.\"\"\"\n    try:\n        # Invoice processing logic\n        if not invoice_path.endswith('.json'):\n            raise InvoiceModeError(\n                emsg=\"Invalid invoice file format. Expected .json file.\",\n                ecode=101,\n                eobj={\"file_path\": invoice_path, \"expected_format\": \".json\"}\n            )\n\n        # More processing...\n        return {\"status\": \"processed\"}\n\n    except Exception as e:\n        raise InvoiceModeError(\n            emsg=f\"Invoice processing failed: {str(e)}\",\n            ecode=102,\n            eobj={\"file_path\": invoice_path, \"original_error\": str(e)}\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#excelinvoicemodeerror","title":"ExcelInvoiceModeError","text":"<p>Exception for errors in Excel invoice processing mode.</p> <pre><code>ExcelInvoiceModeError(\n    emsg: str = \"\",\n    ecode: int = 101,\n    eobj: Any | None = None,\n    traceback_info: str | None = None\n) -&gt; None\n</code></pre> <p>Default Error Code: 101</p> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import ExcelInvoiceModeError\n\ndef process_excel_invoice(excel_path: str) -&gt; dict:\n    \"\"\"Process Excel invoice with specific error handling.\"\"\"\n    try:\n        if not excel_path.endswith(('.xlsx', '.xls')):\n            raise ExcelInvoiceModeError(\n                emsg=\"Invalid Excel file format\",\n                ecode=111,\n                eobj={\"file_path\": excel_path, \"supported_formats\": [\".xlsx\", \".xls\"]}\n            )\n\n        # Excel processing logic...\n        return {\"status\": \"processed\"}\n\n    except Exception as e:\n        raise ExcelInvoiceModeError(\n            emsg=f\"Excel invoice processing failed: {str(e)}\",\n            ecode=112,\n            eobj={\"file_path\": excel_path}\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#multidatatilemodeerror","title":"MultiDataTileModeError","text":"<p>Exception for errors in multi-data tile processing mode.</p> <pre><code>MultiDataTileModeError(\n    emsg: str = \"\",\n    ecode: int = 102,\n    eobj: Any | None = None,\n    traceback_info: str | None = None\n) -&gt; None\n</code></pre> <p>Default Error Code: 102</p> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import MultiDataTileModeError\n\ndef process_multi_data_tiles(tile_configs: list) -&gt; dict:\n    \"\"\"Process multiple data tiles with error handling.\"\"\"\n    try:\n        if not tile_configs:\n            raise MultiDataTileModeError(\n                emsg=\"No data tile configurations provided\",\n                ecode=121,\n                eobj={\"provided_configs\": tile_configs}\n            )\n\n        # Multi-tile processing logic...\n        return {\"tiles_processed\": len(tile_configs)}\n\n    except Exception as e:\n        raise MultiDataTileModeError(\n            emsg=f\"Multi-data tile processing failed: {str(e)}\",\n            ecode=122,\n            eobj={\"tile_count\": len(tile_configs) if tile_configs else 0}\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#rdeformatmodeerror","title":"RdeFormatModeError","text":"<p>Exception for errors in RDE format processing mode.</p> <pre><code>RdeFormatModeError(\n    emsg: str = \"\",\n    ecode: int = 103,\n    eobj: Any | None = None,\n    traceback_info: str | None = None\n) -&gt; None\n</code></pre> <p>Default Error Code: 103</p> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import RdeFormatModeError\n\ndef process_rde_format(format_spec: dict) -&gt; dict:\n    \"\"\"Process RDE format with specific error handling.\"\"\"\n    try:\n        required_fields = [\"version\", \"schema\", \"data\"]\n        missing_fields = [field for field in required_fields if field not in format_spec]\n\n        if missing_fields:\n            raise RdeFormatModeError(\n                emsg=f\"Missing required RDE format fields: {missing_fields}\",\n                ecode=131,\n                eobj={\"missing_fields\": missing_fields, \"provided_spec\": format_spec}\n            )\n\n        # RDE format processing logic...\n        return {\"status\": \"processed\"}\n\n    except Exception as e:\n        raise RdeFormatModeError(\n            emsg=f\"RDE format processing failed: {str(e)}\",\n            ecode=132,\n            eobj={\"format_spec\": format_spec}\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#validation-exception-classes","title":"Validation Exception Classes","text":""},{"location":"rdetoolkit/exceptions/#invoiceschemavalidationerror","title":"InvoiceSchemaValidationError","text":"<p>Exception for invoice schema validation failures.</p> <pre><code>InvoiceSchemaValidationError(message: str = \"Validation error\") -&gt; None\n</code></pre> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import InvoiceSchemaValidationError\n\ndef validate_invoice_schema(invoice_data: dict, schema: dict) -&gt; bool:\n    \"\"\"Validate invoice against schema.\"\"\"\n    try:\n        # Schema validation logic\n        if \"basic\" not in invoice_data:\n            raise InvoiceSchemaValidationError(\n                \"Missing required 'basic' section in invoice\"\n            )\n\n        if \"title\" not in invoice_data[\"basic\"]:\n            raise InvoiceSchemaValidationError(\n                \"Missing required field 'title' in basic section\"\n            )\n\n        return True\n\n    except Exception as e:\n        raise InvoiceSchemaValidationError(\n            f\"Schema validation failed: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#metadatavalidationerror","title":"MetadataValidationError","text":"<p>Exception for metadata validation failures.</p> <pre><code>MetadataValidationError(message: str = \"Validation error\") -&gt; None\n</code></pre> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import MetadataValidationError\n\ndef validate_metadata(metadata: dict) -&gt; bool:\n    \"\"\"Validate metadata structure.\"\"\"\n    try:\n        required_sections = [\"constant\", \"variable\"]\n        for section in required_sections:\n            if section not in metadata:\n                raise MetadataValidationError(\n                    f\"Missing required metadata section: {section}\"\n                )\n\n        return True\n\n    except Exception as e:\n        raise MetadataValidationError(\n            f\"Metadata validation failed: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#data-retrieval-exception-classes","title":"Data Retrieval Exception Classes","text":""},{"location":"rdetoolkit/exceptions/#dataretrievalerror","title":"DataRetrievalError","text":"<p>Exception for data access and retrieval failures.</p> <pre><code>DataRetrievalError(message: str = \"Data retrieval error\") -&gt; None\n</code></pre>"},{"location":"rdetoolkit/exceptions/#noresultsfounderror","title":"NoResultsFoundError","text":"<p>Exception when search operations return no results.</p> <pre><code>NoResultsFoundError(message: str = \"No results found\") -&gt; None\n</code></pre>"},{"location":"rdetoolkit/exceptions/#invalidsearchparameterserror","title":"InvalidSearchParametersError","text":"<p>Exception for invalid search parameters or terms.</p> <pre><code>InvalidSearchParametersError(message: str = \"Invalid search term\") -&gt; None\n</code></pre> <p>Example:</p> <pre><code>from rdetoolkit.exceptions import (\n    DataRetrievalError,\n    NoResultsFoundError,\n    InvalidSearchParametersError\n)\n\ndef search_data(query: str, filters: dict) -&gt; list:\n    \"\"\"Search data with comprehensive error handling.\"\"\"\n\n    # Validate search parameters\n    if not query or len(query.strip()) &lt; 2:\n        raise InvalidSearchParametersError(\n            f\"Search query too short: '{query}'. Minimum 2 characters required.\"\n        )\n\n    try:\n        # Simulate data retrieval\n        results = []  # Would contain actual search results\n\n        if not results:\n            raise NoResultsFoundError(\n                f\"No results found for query: '{query}' with filters: {filters}\"\n            )\n\n        return results\n\n    except Exception as e:\n        raise DataRetrievalError(\n            f\"Failed to retrieve data for query '{query}': {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"rdetoolkit/exceptions/#see-also","title":"See Also","text":"<ul> <li>Core Module - For core functionality that may raise these exceptions</li> <li>Validation - For validation functions that raise validation exceptions</li> <li>Workflows - For workflow processing that uses structured error handling</li> <li>File Operations - For file operations that may raise StructuredError</li> <li>Invoice File - For invoice processing that uses mode-specific exceptions</li> <li>RDE Logger - For logging integration with exception handling</li> <li>Usage - Error Handling - For practical error handling examples</li> </ul>"},{"location":"rdetoolkit/fileops/","title":"File Operations Module","text":"<p>The <code>rdetoolkit.fileops</code> module provides essential file operation utilities for RDE (Research Data Exchange) processing workflows. This module focuses on robust JSON file handling with automatic encoding detection and comprehensive error management.</p>"},{"location":"rdetoolkit/fileops/#overview","title":"Overview","text":"<p>The fileops module offers reliable file operations with built-in safety features:</p> <ul> <li>Automatic Encoding Detection: Intelligent detection of file encodings for robust reading</li> <li>Error Handling: Comprehensive error management with structured exceptions</li> <li>JSON Processing: Specialized functions for JSON file reading and writing</li> <li>Encoding Normalization: Consistent encoding handling across different platforms</li> <li>Logging Integration: Built-in logging for debugging and monitoring</li> <li>Path Flexibility: Support for both string and Path object inputs</li> </ul>"},{"location":"rdetoolkit/fileops/#functions","title":"Functions","text":""},{"location":"rdetoolkit/fileops/#readf_json","title":"readf_json","text":"<p>Read a JSON file and return the parsed JSON object with automatic encoding detection.</p> <pre><code>def readf_json(path: str | Path) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path to the JSON file to read</p> <p>Returns: - <code>dict[str, Any]</code>: The parsed JSON object as a dictionary</p> <p>Raises: - <code>StructuredError</code>: If an error occurs while processing the file (file not found, invalid JSON, encoding issues, etc.)</p> <p>Features: - Automatic encoding detection using the core module's <code>detect_encoding</code> function - Encoding normalization for cross-platform compatibility - Comprehensive error handling with detailed error messages - Logging integration for debugging and monitoring - Support for both string paths and Path objects</p> <p>Example:</p> <pre><code>from rdetoolkit.fileops import readf_json\nfrom pathlib import Path\nimport json\n\n# Basic usage with string path\ntry:\n    data = readf_json(\"data/config/settings.json\")\n    print(f\"Loaded settings: {data}\")\nexcept StructuredError as e:\n    print(f\"Failed to load settings: {e}\")\n\n# Usage with Path object\nconfig_path = Path(\"data/invoice/invoice.json\")\ntry:\n    invoice_data = readf_json(config_path)\n    print(f\"Invoice title: {invoice_data.get('basic', {}).get('title', 'Unknown')}\")\n    print(f\"Sample count: {len(invoice_data.get('sample', {}).get('names', []))}\")\nexcept StructuredError as e:\n    print(f\"Failed to load invoice: {e}\")\n\n# Reading files with various encodings\nfile_paths = [\n    \"data/utf8_file.json\",\n    \"data/shift_jis_file.json\",\n    \"data/utf16_file.json\"\n]\n\nfor file_path in file_paths:\n    try:\n        data = readf_json(file_path)\n        print(f\"Successfully read {file_path}: {len(data)} keys\")\n    except StructuredError as e:\n        print(f\"Failed to read {file_path}: {e}\")\n\n# Error handling example\ndef safe_json_read(file_path: str) -&gt; dict | None:\n    \"\"\"Safely read JSON file with error handling.\"\"\"\n    try:\n        return readf_json(file_path)\n    except StructuredError as e:\n        print(f\"Error reading {file_path}: {e}\")\n        return None\n\n# Usage in batch processing\ndef load_multiple_configs(config_dir: Path) -&gt; dict[str, dict]:\n    \"\"\"Load multiple JSON configuration files.\"\"\"\n    configs = {}\n\n    for json_file in config_dir.glob(\"*.json\"):\n        try:\n            config_data = readf_json(json_file)\n            configs[json_file.stem] = config_data\n            print(f\"\u2713 Loaded config: {json_file.name}\")\n        except StructuredError as e:\n            print(f\"\u2717 Failed to load {json_file.name}: {e}\")\n\n    return configs\n\n# Example usage\nconfig_directory = Path(\"data/configs\")\nall_configs = load_multiple_configs(config_directory)\nprint(f\"Loaded {len(all_configs)} configuration files\")\n</code></pre>"},{"location":"rdetoolkit/fileops/#writef_json","title":"writef_json","text":"<p>Write a dictionary object to a JSON file with specified encoding.</p> <pre><code>def writef_json(path: str | Path, obj: dict[str, Any], *, enc: str = \"utf_8\") -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path to the destination JSON file - <code>obj</code> (dict[str, Any]): Dictionary object to be serialized and written - <code>enc</code> (str): Encoding to use when writing the file (default: \"utf_8\")</p> <p>Returns: - <code>dict[str, Any]</code>: The same dictionary object that was written (for chaining operations)</p> <p>Features: - Pretty-printed JSON output with 4-space indentation - Unicode support with <code>ensure_ascii=False</code> for international characters - Flexible encoding specification - Support for both string paths and Path objects - Returns the input object for method chaining</p> <p>Example:</p> <pre><code>from rdetoolkit.fileops import writef_json\nfrom pathlib import Path\nimport datetime\n\n# Basic usage\ndata = {\n    \"title\": \"Sample Dataset\",\n    \"description\": \"This is a test dataset\",\n    \"created\": datetime.datetime.now().isoformat(),\n    \"version\": \"1.0.0\"\n}\n\noutput_path = \"data/output/sample.json\"\nwritten_data = writef_json(output_path, data)\nprint(f\"Written data keys: {list(written_data.keys())}\")\n\n# Usage with Path object and custom encoding\nmetadata = {\n    \"experiment_id\": \"EXP_2024_001\",\n    \"researcher\": \"\u7530\u4e2d\u592a\u90ce\",  # Japanese characters\n    \"location\": \"\u6771\u4eac\u5927\u5b66\",\n    \"measurements\": [\n        {\"temperature\": 25.5, \"unit\": \"\u00b0C\"},\n        {\"pressure\": 1013.25, \"unit\": \"hPa\"}\n    ]\n}\n\noutput_path = Path(\"data/metadata/experiment_metadata.json\")\nwritef_json(output_path, metadata, enc=\"utf_8\")\n\n# Method chaining example\ndef create_and_save_config(name: str, settings: dict) -&gt; dict:\n    \"\"\"Create configuration and save to file.\"\"\"\n    config = {\n        \"name\": name,\n        \"created_at\": datetime.datetime.now().isoformat(),\n        \"settings\": settings,\n        \"version\": \"1.0\"\n    }\n\n    file_path = f\"data/configs/{name.lower()}.json\"\n    return writef_json(file_path, config)\n\n# Usage\napp_config = create_and_save_config(\"Application\", {\n    \"debug\": True,\n    \"log_level\": \"INFO\",\n    \"max_workers\": 4\n})\n\n# Batch writing example\ndef save_multiple_files(data_dict: dict[str, dict], output_dir: Path) -&gt; list[Path]:\n    \"\"\"Save multiple dictionaries as separate JSON files.\"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    saved_files = []\n\n    for filename, data in data_dict.items():\n        file_path = output_dir / f\"{filename}.json\"\n        try:\n            writef_json(file_path, data)\n            saved_files.append(file_path)\n            print(f\"\u2713 Saved: {file_path}\")\n        except Exception as e:\n            print(f\"\u2717 Failed to save {file_path}: {e}\")\n\n    return saved_files\n\n# Example data\ndatasets = {\n    \"dataset_a\": {\n        \"samples\": 100,\n        \"type\": \"experimental\",\n        \"status\": \"completed\"\n    },\n    \"dataset_b\": {\n        \"samples\": 250,\n        \"type\": \"observational\",\n        \"status\": \"in_progress\"\n    }\n}\n\noutput_directory = Path(\"data/datasets\")\nsaved_paths = save_multiple_files(datasets, output_directory)\nprint(f\"Saved {len(saved_paths)} dataset files\")\n</code></pre>"},{"location":"rdetoolkit/fileops/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/fileops/#json-configuration-manager","title":"JSON Configuration Manager","text":"<pre><code>from rdetoolkit.fileops import readf_json, writef_json\nfrom rdetoolkit.exceptions import StructuredError\nfrom pathlib import Path\nimport datetime\nimport json\nfrom typing import Dict, Any, Optional, List\n\nclass JSONConfigManager:\n    \"\"\"A comprehensive JSON configuration file manager.\"\"\"\n\n    def __init__(self, config_dir: Path):\n        self.config_dir = config_dir\n        self.config_dir.mkdir(parents=True, exist_ok=True)\n        self.loaded_configs: Dict[str, Dict[str, Any]] = {}\n\n    def load_config(self, config_name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Load a single configuration file.\"\"\"\n        config_path = self.config_dir / f\"{config_name}.json\"\n\n        try:\n            config_data = readf_json(config_path)\n            self.loaded_configs[config_name] = config_data\n            print(f\"\u2713 Loaded config: {config_name}\")\n            return config_data\n        except StructuredError as e:\n            print(f\"\u2717 Failed to load config {config_name}: {e}\")\n            return None\n\n    def save_config(self, config_name: str, config_data: Dict[str, Any]) -&gt; bool:\n        \"\"\"Save a configuration file.\"\"\"\n        config_path = self.config_dir / f\"{config_name}.json\"\n\n        try:\n            # Add metadata\n            config_with_meta = {\n                \"_metadata\": {\n                    \"saved_at\": datetime.datetime.now().isoformat(),\n                    \"version\": \"1.0\",\n                    \"config_name\": config_name\n                },\n                **config_data\n            }\n\n            writef_json(config_path, config_with_meta)\n            self.loaded_configs[config_name] = config_with_meta\n            print(f\"\u2713 Saved config: {config_name}\")\n            return True\n        except Exception as e:\n            print(f\"\u2717 Failed to save config {config_name}: {e}\")\n            return False\n\n    def load_all_configs(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Load all JSON configuration files in the directory.\"\"\"\n        configs = {}\n\n        for json_file in self.config_dir.glob(\"*.json\"):\n            config_name = json_file.stem\n            config_data = self.load_config(config_name)\n            if config_data:\n                configs[config_name] = config_data\n\n        return configs\n\n    def get_config(self, config_name: str, key: str = None, default: Any = None) -&gt; Any:\n        \"\"\"Get configuration value with optional key path.\"\"\"\n        if config_name not in self.loaded_configs:\n            config_data = self.load_config(config_name)\n            if not config_data:\n                return default\n\n        config = self.loaded_configs[config_name]\n\n        if key is None:\n            return config\n\n        # Support nested key access like \"database.host\"\n        keys = key.split('.')\n        value = config\n\n        try:\n            for k in keys:\n                value = value[k]\n            return value\n        except (KeyError, TypeError):\n            return default\n\n    def update_config(self, config_name: str, updates: Dict[str, Any]) -&gt; bool:\n        \"\"\"Update an existing configuration.\"\"\"\n        if config_name not in self.loaded_configs:\n            if not self.load_config(config_name):\n                return False\n\n        config = self.loaded_configs[config_name].copy()\n        config.update(updates)\n\n        return self.save_config(config_name, config)\n\n    def backup_configs(self, backup_dir: Path) -&gt; List[Path]:\n        \"\"\"Create backup copies of all configuration files.\"\"\"\n        backup_dir.mkdir(parents=True, exist_ok=True)\n        backed_up_files = []\n\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        for config_name, config_data in self.loaded_configs.items():\n            backup_filename = f\"{config_name}_backup_{timestamp}.json\"\n            backup_path = backup_dir / backup_filename\n\n            try:\n                writef_json(backup_path, config_data)\n                backed_up_files.append(backup_path)\n                print(f\"\u2713 Backed up: {backup_filename}\")\n            except Exception as e:\n                print(f\"\u2717 Failed to backup {config_name}: {e}\")\n\n        return backed_up_files\n\n    def validate_config_schema(self, config_name: str, required_keys: List[str]) -&gt; bool:\n        \"\"\"Validate that a configuration contains required keys.\"\"\"\n        config = self.get_config(config_name)\n        if not config:\n            return False\n\n        missing_keys = [key for key in required_keys if key not in config]\n        if missing_keys:\n            print(f\"Config {config_name} missing required keys: {missing_keys}\")\n            return False\n\n        return True\n\n# Usage example\ndef demonstrate_config_manager():\n    \"\"\"Demonstrate the JSON configuration manager.\"\"\"\n\n    # Setup manager\n    config_dir = Path(\"data/configs\")\n    manager = JSONConfigManager(config_dir)\n\n    # Create sample configurations\n    database_config = {\n        \"host\": \"localhost\",\n        \"port\": 5432,\n        \"database\": \"research_db\",\n        \"username\": \"researcher\",\n        \"pool_size\": 10,\n        \"ssl\": True\n    }\n\n    app_config = {\n        \"name\": \"RDE Toolkit\",\n        \"debug\": False,\n        \"log_level\": \"INFO\",\n        \"features\": {\n            \"auto_backup\": True,\n            \"compression\": True,\n            \"validation\": True\n        },\n        \"limits\": {\n            \"max_file_size\": \"100MB\",\n            \"max_concurrent_jobs\": 4\n        }\n    }\n\n    processing_config = {\n        \"batch_size\": 50,\n        \"timeout\": 300,\n        \"retry_attempts\": 3,\n        \"output_format\": \"json\",\n        \"compression\": \"gzip\"\n    }\n\n    # Save configurations\n    manager.save_config(\"database\", database_config)\n    manager.save_config(\"application\", app_config)\n    manager.save_config(\"processing\", processing_config)\n\n    # Load and use configurations\n    db_host = manager.get_config(\"database\", \"host\")\n    app_name = manager.get_config(\"application\", \"name\")\n    debug_mode = manager.get_config(\"application\", \"debug\", False)\n\n    print(f\"Database host: {db_host}\")\n    print(f\"Application name: {app_name}\")\n    print(f\"Debug mode: {debug_mode}\")\n\n    # Update configuration\n    manager.update_config(\"application\", {\n        \"debug\": True,\n        \"log_level\": \"DEBUG\"\n    })\n\n    # Validate configurations\n    required_db_keys = [\"host\", \"port\", \"database\", \"username\"]\n    is_valid = manager.validate_config_schema(\"database\", required_db_keys)\n    print(f\"Database config valid: {is_valid}\")\n\n    # Load all configurations\n    all_configs = manager.load_all_configs()\n    print(f\"Loaded {len(all_configs)} configurations\")\n\n    # Create backups\n    backup_dir = Path(\"data/backups\")\n    backed_up_files = manager.backup_configs(backup_dir)\n    print(f\"Created {len(backed_up_files)} backup files\")\n\nif __name__ == \"__main__\":\n    demonstrate_config_manager()\n</code></pre>"},{"location":"rdetoolkit/fileops/#advanced-json-processing-pipeline","title":"Advanced JSON Processing Pipeline","text":"<pre><code>from rdetoolkit.fileops import readf_json, writef_json\nfrom rdetoolkit.exceptions import StructuredError\nfrom pathlib import Path\nimport datetime\nimport hashlib\nimport shutil\nfrom typing import Dict, Any, List, Optional, Callable\nimport json\n\nclass JSONProcessingPipeline:\n    \"\"\"Advanced JSON file processing pipeline with validation and transformation.\"\"\"\n\n    def __init__(self, input_dir: Path, output_dir: Path, working_dir: Path):\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.working_dir = working_dir\n        self.backup_dir = working_dir / \"backups\"\n\n        # Create directories\n        for directory in [self.output_dir, self.working_dir, self.backup_dir]:\n            directory.mkdir(parents=True, exist_ok=True)\n\n        self.processing_log = []\n        self.transformations: List[Callable[[Dict[str, Any]], Dict[str, Any]]] = []\n\n    def add_transformation(self, transform_func: Callable[[Dict[str, Any]], Dict[str, Any]]) -&gt; None:\n        \"\"\"Add a transformation function to the pipeline.\"\"\"\n        self.transformations.append(transform_func)\n\n    def process_file(self, input_path: Path) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Process a single JSON file through the pipeline.\"\"\"\n\n        start_time = datetime.datetime.now()\n\n        try:\n            # Read input file\n            original_data = readf_json(input_path)\n\n            # Create backup\n            backup_path = self.backup_dir / f\"{input_path.stem}_backup_{start_time.strftime('%Y%m%d_%H%M%S')}.json\"\n            writef_json(backup_path, original_data)\n\n            # Apply transformations\n            processed_data = original_data.copy()\n\n            for i, transform_func in enumerate(self.transformations):\n                try:\n                    processed_data = transform_func(processed_data)\n                    print(f\"  \u2713 Applied transformation {i+1}\")\n                except Exception as e:\n                    print(f\"  \u2717 Transformation {i+1} failed: {e}\")\n                    # Continue with previous data\n\n            # Add processing metadata\n            processed_data[\"_processing_info\"] = {\n                \"original_file\": str(input_path),\n                \"processed_at\": datetime.datetime.now().isoformat(),\n                \"transformations_applied\": len(self.transformations),\n                \"backup_file\": str(backup_path),\n                \"file_hash\": self._calculate_file_hash(input_path)\n            }\n\n            # Save processed file\n            output_path = self.output_dir / f\"processed_{input_path.name}\"\n            writef_json(output_path, processed_data)\n\n            # Log processing\n            processing_time = (datetime.datetime.now() - start_time).total_seconds()\n            log_entry = {\n                \"input_file\": str(input_path),\n                \"output_file\": str(output_path),\n                \"backup_file\": str(backup_path),\n                \"processing_time\": processing_time,\n                \"status\": \"success\",\n                \"transformations_count\": len(self.transformations)\n            }\n            self.processing_log.append(log_entry)\n\n            print(f\"\u2713 Processed: {input_path.name} -&gt; {output_path.name} ({processing_time:.2f}s)\")\n            return processed_data\n\n        except StructuredError as e:\n            error_entry = {\n                \"input_file\": str(input_path),\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"processing_time\": (datetime.datetime.now() - start_time).total_seconds()\n            }\n            self.processing_log.append(error_entry)\n            print(f\"\u2717 Failed to process: {input_path.name} - {e}\")\n            return None\n\n    def process_batch(self, pattern: str = \"*.json\") -&gt; Dict[str, Any]:\n        \"\"\"Process all JSON files matching the pattern.\"\"\"\n\n        input_files = list(self.input_dir.glob(pattern))\n        print(f\"Found {len(input_files)} files to process\")\n\n        results = {\n            \"total_files\": len(input_files),\n            \"successful\": 0,\n            \"failed\": 0,\n            \"processed_files\": []\n        }\n\n        for input_file in input_files:\n            processed_data = self.process_file(input_file)\n            if processed_data:\n                results[\"successful\"] += 1\n                results[\"processed_files\"].append(str(input_file))\n            else:\n                results[\"failed\"] += 1\n\n        # Save processing log\n        log_path = self.working_dir / f\"processing_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        writef_json(log_path, {\n            \"batch_summary\": results,\n            \"processing_log\": self.processing_log\n        })\n\n        print(f\"\\nBatch processing completed:\")\n        print(f\"  Successful: {results['successful']}\")\n        print(f\"  Failed: {results['failed']}\")\n        print(f\"  Log saved: {log_path}\")\n\n        return results\n\n    def _calculate_file_hash(self, file_path: Path) -&gt; str:\n        \"\"\"Calculate SHA256 hash of a file.\"\"\"\n        hash_sha256 = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_sha256.update(chunk)\n        return hash_sha256.hexdigest()\n\n    def validate_output(self, schema_validator: Optional[Callable[[Dict[str, Any]], bool]] = None) -&gt; Dict[str, Any]:\n        \"\"\"Validate all output files.\"\"\"\n\n        validation_results = {\n            \"total_files\": 0,\n            \"valid_files\": 0,\n            \"invalid_files\": 0,\n            \"validation_errors\": []\n        }\n\n        for output_file in self.output_dir.glob(\"processed_*.json\"):\n            validation_results[\"total_files\"] += 1\n\n            try:\n                data = readf_json(output_file)\n\n                # Basic validation\n                is_valid = True\n                if \"_processing_info\" not in data:\n                    is_valid = False\n                    validation_results[\"validation_errors\"].append({\n                        \"file\": str(output_file),\n                        \"error\": \"Missing processing info\"\n                    })\n\n                # Custom validation\n                if schema_validator and is_valid:\n                    if not schema_validator(data):\n                        is_valid = False\n                        validation_results[\"validation_errors\"].append({\n                            \"file\": str(output_file),\n                            \"error\": \"Schema validation failed\"\n                        })\n\n                if is_valid:\n                    validation_results[\"valid_files\"] += 1\n                else:\n                    validation_results[\"invalid_files\"] += 1\n\n            except StructuredError as e:\n                validation_results[\"invalid_files\"] += 1\n                validation_results[\"validation_errors\"].append({\n                    \"file\": str(output_file),\n                    \"error\": f\"Read error: {e}\"\n                })\n\n        # Save validation report\n        report_path = self.working_dir / f\"validation_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n        writef_json(report_path, validation_results)\n\n        return validation_results\n\n# Example transformations\ndef add_timestamp_transformation(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Add timestamp to data.\"\"\"\n    data[\"timestamp\"] = datetime.datetime.now().isoformat()\n    return data\n\ndef normalize_keys_transformation(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Normalize all keys to lowercase.\"\"\"\n    if isinstance(data, dict):\n        return {k.lower(): normalize_keys_transformation(v) if isinstance(v, dict) else v\n                for k, v in data.items()}\n    return data\n\ndef add_version_transformation(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Add version information.\"\"\"\n    data[\"version\"] = \"1.0.0\"\n    data[\"format_version\"] = \"rde_2024\"\n    return data\n\n# Usage example\ndef demonstrate_json_pipeline():\n    \"\"\"Demonstrate the JSON processing pipeline.\"\"\"\n\n    # Setup pipeline\n    input_dir = Path(\"data/input_json\")\n    output_dir = Path(\"data/processed_json\")\n    working_dir = Path(\"data/working\")\n\n    pipeline = JSONProcessingPipeline(input_dir, output_dir, working_dir)\n\n    # Add transformations\n    pipeline.add_transformation(add_timestamp_transformation)\n    pipeline.add_transformation(normalize_keys_transformation)\n    pipeline.add_transformation(add_version_transformation)\n\n    # Create sample input files\n    input_dir.mkdir(parents=True, exist_ok=True)\n\n    sample_data = [\n        {\n            \"Name\": \"Dataset A\",\n            \"Description\": \"Sample dataset for testing\",\n            \"Type\": \"experimental\",\n            \"Samples\": 100\n        },\n        {\n            \"Title\": \"Research Data B\",\n            \"Author\": \"Research Team\",\n            \"Status\": \"completed\",\n            \"Results\": {\"accuracy\": 0.95, \"precision\": 0.92}\n        }\n    ]\n\n    for i, data in enumerate(sample_data):\n        sample_path = input_dir / f\"sample_{i+1}.json\"\n        writef_json(sample_path, data)\n\n    # Process batch\n    results = pipeline.process_batch()\n\n    # Validate output\n    def custom_validator(data: Dict[str, Any]) -&gt; bool:\n        \"\"\"Custom validation function.\"\"\"\n        required_fields = [\"timestamp\", \"version\", \"_processing_info\"]\n        return all(field in data for field in required_fields)\n\n    validation_results = pipeline.validate_output(custom_validator)\n\n    print(f\"\\nValidation Results:\")\n    print(f\"  Valid files: {validation_results['valid_files']}\")\n    print(f\"  Invalid files: {validation_results['invalid_files']}\")\n\n    if validation_results[\"validation_errors\"]:\n        print(\"  Validation errors:\")\n        for error in validation_results[\"validation_errors\"]:\n            print(f\"    {error['file']}: {error['error']}\")\n\nif __name__ == \"__main__\":\n    demonstrate_json_pipeline()\n</code></pre>"},{"location":"rdetoolkit/fileops/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/fileops/#exception-types","title":"Exception Types","text":"<p>The fileops module raises <code>StructuredError</code> for various error conditions:</p> <pre><code>from rdetoolkit.exceptions import StructuredError\nfrom rdetoolkit.fileops import readf_json, writef_json\n\n# Handle file reading errors\ntry:\n    data = readf_json(\"nonexistent.json\")\nexcept StructuredError as e:\n    print(f\"Read error: {e}\")\n\n# Handle file writing errors\ntry:\n    # This might fail due to permissions, disk space, etc.\n    writef_json(\"/readonly/path/file.json\", {\"test\": \"data\"})\nexcept Exception as e:\n    print(f\"Write error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/fileops/#common-error-scenarios","title":"Common Error Scenarios","text":"<ol> <li> <p>File Not Found:    <pre><code>def safe_json_read(path: str) -&gt; dict | None:\n    \"\"\"Safely read JSON with file existence check.\"\"\"\n    if not Path(path).exists():\n        print(f\"File not found: {path}\")\n        return None\n\n    try:\n        return readf_json(path)\n    except StructuredError as e:\n        print(f\"Error reading {path}: {e}\")\n        return None\n</code></pre></p> </li> <li> <p>Invalid JSON Format:    <pre><code>def validate_json_file(path: str) -&gt; bool:\n    \"\"\"Validate JSON file format.\"\"\"\n    try:\n        readf_json(path)\n        return True\n    except StructuredError as e:\n        if \"JSON\" in str(e).upper():\n            print(f\"Invalid JSON format in {path}\")\n        return False\n</code></pre></p> </li> <li> <p>Encoding Issues:    <pre><code>def handle_encoding_issues(path: str) -&gt; dict | None:\n    \"\"\"Handle files with encoding issues.\"\"\"\n    try:\n        return readf_json(path)\n    except StructuredError as e:\n        if \"encoding\" in str(e).lower():\n            print(f\"Encoding issue in {path}: {e}\")\n            # Could implement fallback encoding detection\n        return None\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/fileops/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Graceful Degradation:    <pre><code>def robust_config_loader(config_paths: list[str]) -&gt; dict:\n    \"\"\"Load configuration with fallback options.\"\"\"\n    for path in config_paths:\n        try:\n            return readf_json(path)\n        except StructuredError:\n            continue\n\n    # Return default configuration\n    return {\"default\": True}\n</code></pre></p> </li> <li> <p>Batch Processing with Error Recovery:    <pre><code>def process_json_files_safely(file_paths: list[str]) -&gt; dict:\n    \"\"\"Process multiple files with error recovery.\"\"\"\n    results = {\"successful\": [], \"failed\": []}\n\n    for path in file_paths:\n        try:\n            data = readf_json(path)\n            # Process data...\n            results[\"successful\"].append(path)\n        except StructuredError as e:\n            results[\"failed\"].append({\"path\": path, \"error\": str(e)})\n\n    return results\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/fileops/#performance-notes","title":"Performance Notes","text":""},{"location":"rdetoolkit/fileops/#optimization-features","title":"Optimization Features","text":"<ol> <li>Automatic Encoding Detection: Efficient encoding detection reduces read errors</li> <li>Streaming JSON: Uses standard library JSON for memory-efficient processing</li> <li>Error Caching: Logging integration helps identify recurring issues</li> <li>Path Object Support: Flexible input types reduce conversion overhead</li> </ol>"},{"location":"rdetoolkit/fileops/#performance-best-practices","title":"Performance Best Practices","text":"<pre><code># Efficient batch processing\ndef process_large_json_files(file_paths: list[Path]) -&gt; None:\n    \"\"\"Process large JSON files efficiently.\"\"\"\n\n    for file_path in file_paths:\n        try:\n            # Read once\n            data = readf_json(file_path)\n\n            # Process in memory\n            processed = transform_data(data)\n\n            # Write once\n            output_path = file_path.parent / f\"processed_{file_path.name}\"\n            writef_json(output_path, processed)\n\n        except StructuredError as e:\n            print(f\"Skipping {file_path}: {e}\")\n\n# Memory-conscious processing for large files\ndef process_large_dataset(input_path: Path, output_path: Path) -&gt; None:\n    \"\"\"Process large JSON datasets with memory management.\"\"\"\n\n    try:\n        # For very large files, consider streaming JSON parsers\n        data = readf_json(input_path)\n\n        # Process in chunks if data is very large\n        if isinstance(data, dict) and len(data) &gt; 10000:\n            # Process in smaller chunks\n            chunk_size = 1000\n            keys = list(data.keys())\n\n            for i in range(0, len(keys), chunk_size):\n                chunk_keys = keys[i:i + chunk_size]\n                chunk_data = {k: data[k] for k in chunk_keys}\n                # Process chunk...\n\n        writef_json(output_path, data)\n\n    except StructuredError as e:\n        print(f\"Processing failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/fileops/#integration-examples","title":"Integration Examples","text":""},{"location":"rdetoolkit/fileops/#integration-with-other-rde-modules","title":"Integration with Other RDE Modules","text":"<pre><code>from rdetoolkit.fileops import readf_json, writef_json\nfrom rdetoolkit.validation import invoice_validate\nfrom rdetoolkit.rde2util import Meta\nfrom pathlib import Path\n\ndef integrated_processing_example():\n    \"\"\"Example of integrating fileops with other RDE modules.\"\"\"\n\n    # Read invoice using fileops\n    invoice_path = Path(\"data/invoice/invoice.json\")\n    invoice_data = readf_json(invoice_path)\n\n    # Validate using validation module\n    schema_path = Path(\"data/tasksupport/invoice.schema.json\")\n    try:\n        invoice_validate(invoice_path, schema_path)\n        print(\"Invoice validation passed\")\n    except Exception as e:\n        print(f\"Invoice validation failed: {e}\")\n\n    # Process metadata using rde2util\n    metadef_path = Path(\"data/tasksupport/metadata-def.json\")\n    meta_output_path = Path(\"data/meta/metadata.json\")\n\n    if metadef_path.exists():\n        meta = Meta(metadef_path)\n        # Use fileops for reading additional data\n        metadata_input = readf_json(\"data/input/metadata_input.json\")\n        meta.assign_vals(metadata_input)\n        meta.writefile(str(meta_output_path))\n\n    # Save processed results using fileops\n    results = {\n        \"invoice_valid\": True,\n        \"metadata_processed\": True,\n        \"processing_timestamp\": \"2024-01-01T00:00:00Z\"\n    }\n\n    writef_json(\"data/output/processing_results.json\", results)\n</code></pre>"},{"location":"rdetoolkit/fileops/#see-also","title":"See Also","text":"<ul> <li>Core Module - For encoding detection and file handling utilities</li> <li>Validation - For JSON schema validation functionality</li> <li>RDE2 Utilities - For metadata processing utilities</li> <li>Invoice File - For invoice-specific file operations</li> <li>Exceptions - For StructuredError and other exception types</li> <li>RDE Logger - For logging functionality used in fileops</li> <li>Usage - Structured Process - For file operations in workflows</li> <li>Usage - Configuration - For configuration file handling</li> </ul>"},{"location":"rdetoolkit/img2thumb/","title":"Image to Thumbnail Module","text":"<p>The <code>rdetoolkit.img2thumb</code> module provides specialized functionality for creating and managing thumbnail images in RDE (Research Data Exchange) processing workflows. This module handles image copying, resizing operations, and thumbnail generation with comprehensive error handling and format support.</p>"},{"location":"rdetoolkit/img2thumb/#overview","title":"Overview","text":"<p>The img2thumb module offers essential image processing capabilities for RDE workflows:</p> <ul> <li>Thumbnail Generation: Automated creation of thumbnail images from source images</li> <li>Image Copying: Efficient copying of images between directories with format validation</li> <li>Aspect Ratio Preservation: Intelligent resizing that maintains original image proportions</li> <li>Multiple Format Support: Support for common image formats (JPG, PNG, GIF, BMP, SVG, WebP)</li> <li>Flexible Target Selection: Options for selecting specific images or automatic selection</li> <li>Error Handling: Comprehensive error management with structured exceptions</li> <li>Path Flexibility: Support for both string and Path object inputs</li> </ul>"},{"location":"rdetoolkit/img2thumb/#functions","title":"Functions","text":""},{"location":"rdetoolkit/img2thumb/#copy_images_to_thumbnail","title":"copy_images_to_thumbnail","text":"<p>Copy image files from main image directories to thumbnail folders with optional filtering.</p> <pre><code>def copy_images_to_thumbnail(\n    out_dir_thumb_img: str | Path,\n    out_dir_main_img: str | Path,\n    *,\n    target_image_name: str | None = None,\n    img_ext: str | None = None,\n) -&gt; None\n</code></pre> <p>Parameters: - <code>out_dir_thumb_img</code> (str | Path): Directory path where thumbnail images will be saved - <code>out_dir_main_img</code> (str | Path): Directory path where main images are located - <code>target_image_name</code> (str | None): Specific image file name to copy (optional) - <code>img_ext</code> (str | None): Specific image file extension to filter (optional)</p> <p>Returns: - <code>None</code></p> <p>Raises: - Decorated with error handling that catches exceptions and converts them to structured errors with code 50</p> <p>Supported Image Formats: - <code>.jpg</code>, <code>.jpeg</code> - JPEG images - <code>.png</code> - PNG images - <code>.gif</code> - GIF images - <code>.bmp</code> - Bitmap images - <code>.svg</code> - SVG vector images - <code>.webp</code> - WebP images</p> <p>Behavior: - If <code>target_image_name</code> is specified, searches for that specific image file - If multiple images exist and no target is specified, copies the first image found - Performs recursive search in subdirectories when looking for target image - Preserves original file names in the thumbnail directory</p> <p>Example:</p> <pre><code>from rdetoolkit.img2thumb import copy_images_to_thumbnail\nfrom pathlib import Path\n\n# Basic usage - copy first available image\nthumbnail_dir = Path(\"data/thumbnail\")\nmain_image_dir = Path(\"data/main_image\")\n\ncopy_images_to_thumbnail(\n    out_dir_thumb_img=thumbnail_dir,\n    out_dir_main_img=main_image_dir\n)\n\n# Copy specific image by name\ncopy_images_to_thumbnail(\n    out_dir_thumb_img=\"data/thumbnail\",\n    out_dir_main_img=\"data/main_image\",\n    target_image_name=\"sample_image.jpg\"\n)\n\n# Filter by specific extension\ncopy_images_to_thumbnail(\n    out_dir_thumb_img=\"data/thumbnail\",\n    out_dir_main_img=\"data/main_image\",\n    img_ext=\".png\"\n)\n\n# Batch processing example\ndef process_multiple_image_sets(image_sets: list[dict]) -&gt; None:\n    \"\"\"Process multiple sets of images for thumbnail generation.\"\"\"\n\n    for image_set in image_sets:\n        try:\n            copy_images_to_thumbnail(\n                out_dir_thumb_img=image_set[\"thumbnail_dir\"],\n                out_dir_main_img=image_set[\"main_dir\"],\n                target_image_name=image_set.get(\"target_name\"),\n                img_ext=image_set.get(\"extension\")\n            )\n            print(f\"\u2713 Processed: {image_set['main_dir']}\")\n        except Exception as e:\n            print(f\"\u2717 Failed: {image_set['main_dir']} - {e}\")\n\n# Example usage\nimage_configurations = [\n    {\n        \"thumbnail_dir\": \"data/experiment1/thumbnail\",\n        \"main_dir\": \"data/experiment1/main_image\",\n        \"target_name\": \"result_plot.png\"\n    },\n    {\n        \"thumbnail_dir\": \"data/experiment2/thumbnail\",\n        \"main_dir\": \"data/experiment2/main_image\",\n        \"extension\": \".jpg\"\n    }\n]\n\nprocess_multiple_image_sets(image_configurations)\n</code></pre>"},{"location":"rdetoolkit/img2thumb/#resize_image","title":"resize_image","text":"<p>Resize an image to specified dimensions while maintaining aspect ratio.</p> <pre><code>def resize_image(\n    path: str | Path,\n    width: int = 640,\n    height: int = 480,\n    output_path: str | Path | None = None\n) -&gt; str\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path to the source image file - <code>width</code> (int): Target width in pixels (default: 640) - <code>height</code> (int): Target height in pixels (default: 480) - <code>output_path</code> (str | Path | None): Output path for resized image (optional, defaults to overwriting original)</p> <p>Returns: - <code>str</code>: Path to the resized image file</p> <p>Raises: - <code>StructuredError</code>: If width or height is less than or equal to 0 - <code>StructuredError</code>: If image resizing fails due to file issues or invalid image format</p> <p>Features: - Maintains original image aspect ratio during resizing - Supports all common image formats - Flexible output path specification - Automatic path type conversion - Comprehensive error handling with detailed messages</p> <p>Example:</p> <pre><code>from rdetoolkit.img2thumb import resize_image\nfrom rdetoolkit.exceptions import StructuredError\nfrom pathlib import Path\n\n# Basic resizing with default dimensions (640x480)\ntry:\n    output_path = resize_image(\"data/images/large_photo.jpg\")\n    print(f\"Image resized and saved to: {output_path}\")\nexcept StructuredError as e:\n    print(f\"Resize failed: {e}\")\n\n# Custom dimensions with separate output file\ninput_image = Path(\"data/images/high_res_chart.png\")\noutput_image = Path(\"data/thumbnails/chart_thumb.png\")\n\ntry:\n    result_path = resize_image(\n        path=input_image,\n        width=320,\n        height=240,\n        output_path=output_image\n    )\n    print(f\"Thumbnail created: {result_path}\")\nexcept StructuredError as e:\n    print(f\"Thumbnail creation failed: {e}\")\n\n# Batch resizing with different sizes\ndef create_multiple_thumbnails(source_image: Path, output_dir: Path) -&gt; dict[str, str]:\n    \"\"\"Create thumbnails in multiple sizes.\"\"\"\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    thumbnail_sizes = {\n        \"small\": (160, 120),\n        \"medium\": (320, 240),\n        \"large\": (640, 480),\n        \"extra_large\": (1024, 768)\n    }\n\n    results = {}\n\n    for size_name, (width, height) in thumbnail_sizes.items():\n        try:\n            output_filename = f\"{source_image.stem}_{size_name}{source_image.suffix}\"\n            output_path = output_dir / output_filename\n\n            result_path = resize_image(\n                path=source_image,\n                width=width,\n                height=height,\n                output_path=output_path\n            )\n\n            results[size_name] = result_path\n            print(f\"\u2713 Created {size_name} thumbnail: {width}x{height}\")\n\n        except StructuredError as e:\n            print(f\"\u2717 Failed to create {size_name} thumbnail: {e}\")\n            results[size_name] = None\n\n    return results\n\n# Usage example\nsource_img = Path(\"data/images/sample_photo.jpg\")\nthumb_dir = Path(\"data/thumbnails/sample_photo\")\n\nif source_img.exists():\n    thumbnail_results = create_multiple_thumbnails(source_img, thumb_dir)\n\n    successful_thumbs = [size for size, path in thumbnail_results.items() if path]\n    print(f\"Successfully created {len(successful_thumbs)} thumbnails: {successful_thumbs}\")\n\n# Validation and error handling example\ndef safe_resize_with_validation(\n    image_path: Path,\n    target_width: int,\n    target_height: int,\n    output_dir: Path\n) -&gt; bool:\n    \"\"\"Safely resize image with comprehensive validation.\"\"\"\n\n    # Validate input parameters\n    if target_width &lt;= 0 or target_height &lt;= 0:\n        print(f\"Invalid dimensions: {target_width}x{target_height}\")\n        return False\n\n    if not image_path.exists():\n        print(f\"Source image not found: {image_path}\")\n        return False\n\n    # Check if file is actually an image\n    valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp'}\n    if image_path.suffix.lower() not in valid_extensions:\n        print(f\"Invalid image format: {image_path.suffix}\")\n        return False\n\n    try:\n        # Create output directory\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Generate output path\n        output_filename = f\"{image_path.stem}_resized_{target_width}x{target_height}{image_path.suffix}\"\n        output_path = output_dir / output_filename\n\n        # Resize image\n        result_path = resize_image(\n            path=image_path,\n            width=target_width,\n            height=target_height,\n            output_path=output_path\n        )\n\n        print(f\"\u2713 Image resized successfully: {result_path}\")\n        return True\n\n    except StructuredError as e:\n        print(f\"\u2717 Resize operation failed: {e}\")\n        return False\n    except Exception as e:\n        print(f\"\u2717 Unexpected error: {e}\")\n        return False\n\n# Usage\nimage_file = Path(\"data/images/test_image.png\")\noutput_directory = Path(\"data/processed\")\n\nsuccess = safe_resize_with_validation(\n    image_path=image_file,\n    target_width=800,\n    target_height=600,\n    output_dir=output_directory\n)\n</code></pre>"},{"location":"rdetoolkit/img2thumb/#see-also","title":"See Also","text":"<ul> <li>Core Module - For resize_image_aspect_ratio function used internally</li> <li>Error Handling - For catch_exception_with_message decorator</li> <li>Exceptions - For StructuredError exception type</li> <li>Workflows - For integration with RDE processing workflows</li> <li>Mode Processing - For image processing in different modes</li> <li>Usage - Structured Process - For image processing in RDE workflows</li> </ul>"},{"location":"rdetoolkit/invoicefile/","title":"Invoice File Module","text":"<p>The <code>rdetoolkit.invoicefile</code> module provides comprehensive functionality for handling various types of invoice files in RDE (Research Data Exchange) processing workflows. This module supports standard JSON invoices, Excel-based invoices, template generation, and advanced file processing with rule-based transformations.</p>"},{"location":"rdetoolkit/invoicefile/#overview","title":"Overview","text":"<p>The invoicefile module offers extensive capabilities for invoice processing:</p> <ul> <li>Invoice File Management: Reading, writing, and manipulation of JSON-based invoice files</li> <li>Excel Invoice Processing: Specialized handling of Excel-based invoice files with multiple sheets</li> <li>Template Generation: Automated creation of Excel invoice templates based on schemas</li> <li>SmartTable Integration: Processing of SmartTable files for automated invoice generation</li> <li>Rule-Based Replacement: Flexible file naming and content transformation rules</li> <li>Magic Variable Processing: Dynamic variable substitution in invoice content</li> <li>Backup and Recovery: Automated backup of invoice files during processing</li> <li>Validation and Error Handling: Comprehensive validation with detailed error reporting</li> </ul>"},{"location":"rdetoolkit/invoicefile/#classes","title":"Classes","text":""},{"location":"rdetoolkit/invoicefile/#invoicefile","title":"InvoiceFile","text":"<p>A class representing standard JSON invoice files with utilities for reading and writing.</p>"},{"location":"rdetoolkit/invoicefile/#constructor","title":"Constructor","text":"<pre><code>InvoiceFile(invoice_path: Path)\n</code></pre> <p>Parameters: - <code>invoice_path</code> (Path): Path to the invoice JSON file</p> <p>Attributes: - <code>invoice_path</code> (Path): Path to the invoice file - <code>invoice_obj</code> (dict[str, Any]): Dictionary representation of the invoice JSON</p>"},{"location":"rdetoolkit/invoicefile/#methods","title":"Methods","text":""},{"location":"rdetoolkit/invoicefile/#read","title":"read","text":"<p>Read the content of the invoice file and return as a dictionary.</p> <pre><code>def read(self, *, target_path: Path | None = None) -&gt; dict\n</code></pre> <p>Parameters: - <code>target_path</code> (Path | None): Alternative path to read from (optional)</p> <p>Returns: - <code>dict</code>: Dictionary representation of the invoice JSON file</p>"},{"location":"rdetoolkit/invoicefile/#overwrite","title":"overwrite","text":"<p>Overwrite the contents of the destination file with invoice JSON data.</p> <pre><code>def overwrite(self, dst_file_path: Path, *, src_obj: Path | None = None) -&gt; None\n</code></pre> <p>Parameters: - <code>dst_file_path</code> (Path): Destination file path - <code>src_obj</code> (Path | None): Source object path (optional)</p> <p>Returns: - <code>None</code></p> <p>Raises: - <code>StructuredError</code>: If the destination file cannot be created</p>"},{"location":"rdetoolkit/invoicefile/#copy_original_invoice","title":"copy_original_invoice","text":"<p>Copy the original invoice file from source to destination.</p> <pre><code>@classmethod\ndef copy_original_invoice(cls, src_file_path: Path, dst_file_path: Path) -&gt; None\n</code></pre> <p>Parameters: - <code>src_file_path</code> (Path): Source file path - <code>dst_file_path</code> (Path): Destination file path</p> <p>Raises: - <code>StructuredError</code>: If the source file does not exist</p> <p>Example:</p> <pre><code>from rdetoolkit.invoicefile import InvoiceFile\nfrom pathlib import Path\n\n# Create and manipulate invoice file\ninvoice_path = Path(\"data/invoice/invoice.json\")\ninvoice = InvoiceFile(invoice_path)\n\n# Access and modify invoice data\nprint(f\"Original data name: {invoice.invoice_obj['basic']['dataName']}\")\ninvoice.invoice_obj[\"basic\"][\"dataName\"] = \"Updated Dataset Name\"\ninvoice.invoice_obj[\"basic\"][\"description\"] = \"Updated description\"\n\n# Save changes\noutput_path = Path(\"data/invoice/invoice_updated.json\")\ninvoice.overwrite(output_path)\n\n# Copy original invoice\nbackup_path = Path(\"data/backup/invoice_backup.json\")\nInvoiceFile.copy_original_invoice(invoice_path, backup_path)\n\n# Read from different file\ninvoice.read(target_path=backup_path)\nprint(f\"Backup data name: {invoice.invoice_obj['basic']['dataName']}\")\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#excelinvoicefile","title":"ExcelInvoiceFile","text":"<p>A comprehensive class for handling Excel-based invoice files with multiple sheets.</p>"},{"location":"rdetoolkit/invoicefile/#constructor_1","title":"Constructor","text":"<pre><code>ExcelInvoiceFile(invoice_path: Path)\n</code></pre> <p>Parameters: - <code>invoice_path</code> (Path): Path to the Excel invoice file (.xlsx)</p> <p>Attributes: - <code>invoice_path</code> (Path): Path to the Excel invoice file - <code>dfexcelinvoice</code> (pd.DataFrame): DataFrame of the invoice data - <code>df_general</code> (pd.DataFrame): DataFrame of general terms - <code>df_specific</code> (pd.DataFrame): DataFrame of specific terms - <code>template_generator</code> (ExcelInvoiceTemplateGenerator): Template generator instance</p>"},{"location":"rdetoolkit/invoicefile/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/invoicefile/#read_1","title":"read","text":"<p>Read the Excel invoice file and return three DataFrames.</p> <pre><code>def read(self, *, target_path: Path | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Parameters: - <code>target_path</code> (Path | None): Path to the Excel invoice file (optional)</p> <p>Returns: - <code>tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]</code>: Invoice, general terms, and specific terms DataFrames</p> <p>Raises: - <code>StructuredError</code>: If file not found or multiple/no invoice sheets exist</p>"},{"location":"rdetoolkit/invoicefile/#generate_template","title":"generate_template","text":"<p>Generate Excel invoice template based on schema.</p> <pre><code>@classmethod\ndef generate_template(\n    cls,\n    invoice_schema_path: str | Path,\n    save_path: str | Path,\n    file_mode: Literal[\"file\", \"folder\"] = \"file\"\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Parameters: - <code>invoice_schema_path</code> (str | Path): Path to invoice schema file - <code>save_path</code> (str | Path): Path to save the generated template - <code>file_mode</code> (Literal[\"file\", \"folder\"]): Input mode specification</p> <p>Returns: - <code>tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]</code>: Template, general terms, and specific terms DataFrames</p>"},{"location":"rdetoolkit/invoicefile/#overwrite_1","title":"overwrite","text":"<p>Overwrite original invoice file based on Excel invoice data.</p> <pre><code>def overwrite(self, invoice_org: Path, dist_path: Path, invoice_schema_path: Path, idx: int) -&gt; None\n</code></pre> <p>Parameters: - <code>invoice_org</code> (Path): Path to original invoice file - <code>dist_path</code> (Path): Path for output invoice file - <code>invoice_schema_path</code> (Path): Path to invoice schema - <code>idx</code> (int): Row index in the invoice DataFrame</p>"},{"location":"rdetoolkit/invoicefile/#save","title":"save","text":"<p>Save the invoice DataFrame to an Excel file.</p> <pre><code>def save(\n    self,\n    save_path: str | Path,\n    *,\n    invoice: pd.DataFrame | None = None,\n    sheet_name: str = \"invoice_form\",\n    index: list[str] | None = None,\n    header: list[str] | None = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>save_path</code> (str | Path): Path to save the Excel file - <code>invoice</code> (pd.DataFrame | None): DataFrame to save (optional) - <code>sheet_name</code> (str): Sheet name in Excel file - <code>index</code> (list[str] | None): Index labels (optional) - <code>header</code> (list[str] | None): Column headers (optional)</p>"},{"location":"rdetoolkit/invoicefile/#check_intermittent_empty_rows","title":"check_intermittent_empty_rows","text":"<p>Detect empty rows between data rows in Excel invoice.</p> <pre><code>@staticmethod\ndef check_intermittent_empty_rows(df: pd.DataFrame) -&gt; None\n</code></pre> <p>Parameters: - <code>df</code> (pd.DataFrame): DataFrame to check</p> <p>Raises: - <code>StructuredError</code>: If empty rows exist between data rows</p> <p>Example:</p> <pre><code>from rdetoolkit.invoicefile import ExcelInvoiceFile\nfrom pathlib import Path\nimport pandas as pd\n\n# Read Excel invoice\nexcel_path = Path(\"data/input/sample_excel_invoice.xlsx\")\nexcel_invoice = ExcelInvoiceFile(excel_path)\n\nprint(f\"Invoice data shape: {excel_invoice.dfexcelinvoice.shape}\")\nprint(f\"General terms: {len(excel_invoice.df_general)}\")\nprint(f\"Specific terms: {len(excel_invoice.df_specific)}\")\n\n# Generate template\nschema_path = Path(\"data/tasksupport/invoice.schema.json\")\ntemplate_path = Path(\"data/templates/new_template.xlsx\")\n\ntemplate_df, general_df, specific_df = ExcelInvoiceFile.generate_template(\n    invoice_schema_path=schema_path,\n    save_path=template_path,\n    file_mode=\"file\"\n)\n\nprint(f\"Generated template with {template_df.shape[1]} columns\")\n\n# Process Excel invoice data\noriginal_invoice = Path(\"data/invoice/invoice.json\")\noutput_invoice = Path(\"data/processed/invoice_processed.json\")\n\n# Process each row in the Excel invoice\nfor idx in range(len(excel_invoice.dfexcelinvoice)):\n    row_output = Path(f\"data/processed/invoice_{idx:04d}.json\")\n    excel_invoice.overwrite(\n        invoice_org=original_invoice,\n        dist_path=row_output,\n        invoice_schema_path=schema_path,\n        idx=idx\n    )\n    print(f\"Processed row {idx} -&gt; {row_output}\")\n\n# Save modified invoice data\nmodified_path = Path(\"data/output/modified_invoice.xlsx\")\nexcel_invoice.save(\n    save_path=modified_path,\n    sheet_name=\"processed_invoice\"\n)\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#excelinvoicetemplategenerator","title":"ExcelInvoiceTemplateGenerator","text":"<p>A specialized class for generating Excel invoice templates with proper formatting.</p>"},{"location":"rdetoolkit/invoicefile/#constructor_2","title":"Constructor","text":"<pre><code>ExcelInvoiceTemplateGenerator(fixed_header: FixedHeaders)\n</code></pre> <p>Parameters: - <code>fixed_header</code> (FixedHeaders): Fixed header configuration</p>"},{"location":"rdetoolkit/invoicefile/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/invoicefile/#generate","title":"generate","text":"<p>Generate template based on configuration.</p> <pre><code>def generate(self, config: TemplateConfig) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Parameters: - <code>config</code> (TemplateConfig): Template configuration</p> <p>Returns: - <code>tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]</code>: Template, general terms, specific terms, and version DataFrames</p>"},{"location":"rdetoolkit/invoicefile/#save_1","title":"save","text":"<p>Save DataFrames to Excel file with formatting.</p> <pre><code>def save(self, dataframes: dict[str, pd.DataFrame], save_path: str) -&gt; None\n</code></pre> <p>Parameters: - <code>dataframes</code> (dict[str, pd.DataFrame]): DataFrames to save - <code>save_path</code> (str): Output file path</p> <p>Example:</p> <pre><code>from rdetoolkit.invoicefile import ExcelInvoiceTemplateGenerator, FixedHeaders\nfrom rdetoolkit.models.invoice import TemplateConfig\nfrom pathlib import Path\n\n# Create template generator\nfixed_headers = FixedHeaders()\ngenerator = ExcelInvoiceTemplateGenerator(fixed_headers)\n\n# Setup configuration\nconfig = TemplateConfig(\n    schema_path=Path(\"data/tasksupport/invoice.schema.json\"),\n    general_term_path=Path(\"data/terms/general_terms.csv\"),\n    specific_term_path=Path(\"data/terms/specific_terms.csv\"),\n    inputfile_mode=\"file\"\n)\n\n# Generate template\ntemplate_df, general_df, specific_df, version_df = generator.generate(config)\n\n# Prepare DataFrames for saving\ndataframes = {\n    \"invoice_form\": template_df,\n    \"generalTerm\": general_df,\n    \"specificTerm\": specific_df,\n    \"_version\": version_df\n}\n\n# Save with formatting\noutput_path = \"data/templates/formatted_template.xlsx\"\ngenerator.save(dataframes, output_path)\n\nprint(f\"Template saved to: {output_path}\")\nprint(f\"Template dimensions: {template_df.shape}\")\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#rulebasedreplacer","title":"RuleBasedReplacer","text":"<p>A flexible class for managing file name mapping and content transformation rules.</p>"},{"location":"rdetoolkit/invoicefile/#constructor_3","title":"Constructor","text":"<pre><code>RuleBasedReplacer(*, rule_file_path: str | Path | None = None)\n</code></pre> <p>Parameters: - <code>rule_file_path</code> (str | Path | None): Path to rule file (optional)</p> <p>Attributes: - <code>rules</code> (dict[str, str]): Dictionary holding mapping rules - <code>last_apply_result</code> (dict[str, Any]): Result of last applied rules</p>"},{"location":"rdetoolkit/invoicefile/#methods_3","title":"Methods","text":""},{"location":"rdetoolkit/invoicefile/#load_rules","title":"load_rules","text":"<p>Load file mapping rules from JSON file.</p> <pre><code>def load_rules(self, filepath: str | Path) -&gt; None\n</code></pre> <p>Parameters: - <code>filepath</code> (str | Path): Path to JSON file containing mapping rules</p> <p>Raises: - <code>StructuredError</code>: If file extension is not .json</p>"},{"location":"rdetoolkit/invoicefile/#set_rule","title":"set_rule","text":"<p>Set a new mapping rule.</p> <pre><code>def set_rule(self, path: str, variable: str) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str): Target path for replacement - <code>variable</code> (str): Replacement variable</p>"},{"location":"rdetoolkit/invoicefile/#get_apply_rules_obj","title":"get_apply_rules_obj","text":"<p>Convert file mapping rules into JSON format.</p> <pre><code>def get_apply_rules_obj(\n    self,\n    replacements: dict[str, Any],\n    source_json_obj: dict[str, Any] | None,\n    *,\n    mapping_rules: dict[str, str] | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>replacements</code> (dict[str, Any]): Object containing mapping rules - <code>source_json_obj</code> (dict[str, Any] | None): Source JSON object for rule application - <code>mapping_rules</code> (dict[str, str] | None): Rules for mapping (optional)</p> <p>Returns: - <code>dict[str, Any]</code>: Dictionary after rule conversion</p>"},{"location":"rdetoolkit/invoicefile/#write_rule","title":"write_rule","text":"<p>Write file mapping rules to target JSON file.</p> <pre><code>def write_rule(self, replacements_rule: dict[str, Any], save_file_path: str | Path) -&gt; str\n</code></pre> <p>Parameters: - <code>replacements_rule</code> (dict[str, Any]): Object containing mapping rules - <code>save_file_path</code> (str | Path): File path for saving</p> <p>Returns: - <code>str</code>: Result of writing to target JSON</p> <p>Raises: - <code>StructuredError</code>: If file extension is not .json or writing fails</p> <p>Example:</p> <pre><code>from rdetoolkit.invoicefile import RuleBasedReplacer\nfrom pathlib import Path\nimport json\n\n# Create replacer with rules file\nrules_path = Path(\"data/config/mapping_rules.json\")\nreplacer = RuleBasedReplacer(rule_file_path=rules_path)\n\n# Set custom rules\nreplacer.set_rule(\"basic.dataName\", \"${filename}\")\nreplacer.set_rule(\"basic.description\", \"${description}\")\nreplacer.set_rule(\"sample.names\", [\"${sample_name}\"])\n\n# Prepare replacement values\nreplacements = {\n    \"${filename}\": \"experiment_data.csv\",\n    \"${description}\": \"Experimental measurement data\",\n    \"${sample_name}\": \"Sample A\"\n}\n\n# Apply rules to create new JSON structure\nsource_obj = {\n    \"basic\": {\n        \"title\": \"Original Title\"\n    }\n}\n\nresult = replacer.get_apply_rules_obj(replacements, source_obj)\nprint(\"Applied rules result:\")\nprint(json.dumps(result, indent=2))\n\n# Write rules to file\noutput_path = Path(\"data/output/processed_data.json\")\nwritten_content = replacer.write_rule(replacements, output_path)\nprint(f\"Rules written to: {output_path}\")\n\n# Load existing rules\nif rules_path.exists():\n    replacer.load_rules(rules_path)\n    print(f\"Loaded {len(replacer.rules)} rules from file\")\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#smarttablefile","title":"SmartTableFile","text":"<p>A class for handling SmartTable files (Excel/CSV/TSV) for automated invoice generation.</p>"},{"location":"rdetoolkit/invoicefile/#constructor_4","title":"Constructor","text":"<pre><code>SmartTableFile(smarttable_path: Path)\n</code></pre> <p>Parameters: - <code>smarttable_path</code> (Path): Path to SmartTable file (.xlsx, .csv, .tsv)</p> <p>Raises: - <code>StructuredError</code>: If file format unsupported or file doesn't exist</p>"},{"location":"rdetoolkit/invoicefile/#methods_4","title":"Methods","text":""},{"location":"rdetoolkit/invoicefile/#read_table","title":"read_table","text":"<p>Read the SmartTable file and return as DataFrame.</p> <pre><code>def read_table(self) -&gt; pd.DataFrame\n</code></pre> <p>Returns: - <code>pd.DataFrame</code>: Table data with mapping key headers</p> <p>Raises: - <code>StructuredError</code>: If file reading fails or format is invalid</p>"},{"location":"rdetoolkit/invoicefile/#generate_row_csvs_with_file_mapping","title":"generate_row_csvs_with_file_mapping","text":"<p>Generate individual CSV files for each row with file mapping.</p> <pre><code>def generate_row_csvs_with_file_mapping(\n    self,\n    output_dir: Path,\n    extracted_files: list[Path] | None = None,\n) -&gt; list[tuple[Path, tuple[Path, ...]]]\n</code></pre> <p>Parameters: - <code>output_dir</code> (Path): Directory to save individual CSV files - <code>extracted_files</code> (list[Path] | None): List of extracted files from zip (optional)</p> <p>Returns: - <code>list[tuple[Path, tuple[Path, ...]]]</code>: List of tuples with CSV paths and related file paths</p> <p>Raises: - <code>StructuredError</code>: If CSV generation or file mapping fails</p> <p>Example:</p> <pre><code>from rdetoolkit.invoicefile import SmartTableFile\nfrom pathlib import Path\nimport pandas as pd\n\n# Create SmartTable file\nsmarttable_path = Path(\"data/input/smarttable_metadata.xlsx\")\nsmarttable = SmartTableFile(smarttable_path)\n\n# Read table data\ntable_data = smarttable.read_table()\nprint(f\"SmartTable data shape: {table_data.shape}\")\nprint(f\"Columns: {list(table_data.columns)}\")\n\n# Display mapping columns\nmapping_columns = [col for col in table_data.columns\n                  if any(col.startswith(prefix) for prefix in [\"basic/\", \"custom/\", \"sample/\", \"meta/\"])]\nprint(f\"Mapping columns: {mapping_columns}\")\n\n# Generate individual CSV files with file mapping\noutput_dir = Path(\"data/processed/smarttable_rows\")\nextracted_files = [\n    Path(\"data/temp/file1.csv\"),\n    Path(\"data/temp/file2.txt\"),\n    Path(\"data/temp/subdir/file3.json\")\n]\n\ncsv_mappings = smarttable.generate_row_csvs_with_file_mapping(\n    output_dir=output_dir,\n    extracted_files=extracted_files\n)\n\nprint(f\"Generated {len(csv_mappings)} CSV files:\")\nfor csv_path, related_files in csv_mappings:\n    print(f\"  {csv_path.name} -&gt; {len(related_files)} related files\")\n    for file_path in related_files:\n        print(f\"    - {file_path}\")\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#functions","title":"Functions","text":""},{"location":"rdetoolkit/invoicefile/#read_excelinvoice","title":"read_excelinvoice","text":"<p>Read an Excel invoice and process each sheet into DataFrames.</p> <pre><code>def read_excelinvoice(excelinvoice_filepath: RdeFsPath) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Parameters: - <code>excelinvoice_filepath</code> (RdeFsPath): Path to Excel invoice file</p> <p>Returns: - <code>tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]</code>: Invoice list, general terms, and specific terms DataFrames</p> <p>Raises: - <code>StructuredError</code>: If multiple invoice sheets exist or no sheets are present</p> <p>Sheet Requirements: - One sheet with 'invoiceList_format_id' in cell A1 - One sheet named 'generalTerm' - One sheet named 'specificTerm'</p>"},{"location":"rdetoolkit/invoicefile/#check_exist_rawfiles","title":"check_exist_rawfiles","text":"<p>Check existence of raw file paths listed in DataFrame against file list.</p> <pre><code>def check_exist_rawfiles(dfexcelinvoice: pd.DataFrame, excel_rawfiles: list[Path]) -&gt; list[Path]\n</code></pre> <p>Parameters: - <code>dfexcelinvoice</code> (pd.DataFrame): DataFrame with file names in 'data_file_names/name' column - <code>excel_rawfiles</code> (list[Path]): List of file paths</p> <p>Returns: - <code>list[Path]</code>: List of Path objects ordered as they appear in DataFrame</p> <p>Raises: - <code>StructuredError</code>: If any file name in DataFrame is not found in file list</p>"},{"location":"rdetoolkit/invoicefile/#backup_invoice_json_files","title":"backup_invoice_json_files","text":"<p>Backup invoice files and retrieve paths based on processing mode.</p> <pre><code>def backup_invoice_json_files(excel_invoice_file: Path | None, mode: str | None) -&gt; Path\n</code></pre> <p>Parameters: - <code>excel_invoice_file</code> (Path | None): Excel invoice file path (optional) - <code>mode</code> (str | None): Processing mode flags</p> <p>Returns: - <code>Path</code>: Path to backed up invoice_org.json file</p>"},{"location":"rdetoolkit/invoicefile/#update_description_with_features","title":"update_description_with_features","text":"<p>Write metadata features to the description field in invoice.json.</p> <pre><code>def update_description_with_features(\n    rde_resource: RdeOutputResourcePath,\n    dst_invoice_json: Path,\n    metadata_def_json: Path,\n) -&gt; None\n</code></pre> <p>Parameters: - <code>rde_resource</code> (RdeOutputResourcePath): Resource paths for RDE processing - <code>dst_invoice_json</code> (Path): Path to target invoice.json file - <code>metadata_def_json</code> (Path): Path to metadata definition JSON file</p> <p>Returns: - <code>None</code></p>"},{"location":"rdetoolkit/invoicefile/#apply_magic_variable","title":"apply_magic_variable","text":"<p>Convert magic variables like ${filename} in invoice content.</p> <pre><code>def apply_magic_variable(\n    invoice_path: str | Path,\n    rawfile_path: str | Path,\n    *,\n    save_filepath: str | Path | None = None\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>invoice_path</code> (str | Path): Path to invoice.json file - <code>rawfile_path</code> (str | Path): Path to input data file - <code>save_filepath</code> (str | Path | None): Path to save processed file (optional)</p> <p>Returns: - <code>dict[str, Any]</code>: Invoice content after variable replacement</p>"},{"location":"rdetoolkit/invoicefile/#apply_default_filename_mapping_rule","title":"apply_default_filename_mapping_rule","text":"<p>Apply default filename mapping rule based on save file path.</p> <pre><code>def apply_default_filename_mapping_rule(\n    replacement_rule: dict[str, Any],\n    save_file_path: str | Path\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>replacement_rule</code> (dict[str, Any]): Replacement rules to apply - <code>save_file_path</code> (str | Path): File path for saving rules</p> <p>Returns: - <code>dict[str, Any]</code>: Result of applied replacement rules</p>"},{"location":"rdetoolkit/invoicefile/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/invoicefile/#comprehensive-excel-invoice-processing-pipeline","title":"Comprehensive Excel Invoice Processing Pipeline","text":"<pre><code>from rdetoolkit.invoicefile import ExcelInvoiceFile, RuleBasedReplacer, apply_magic_variable\nfrom pathlib import Path\nimport pandas as pd\nimport json\nfrom typing import List, Dict, Any\n\nclass ExcelInvoiceProcessor:\n    \"\"\"Comprehensive Excel invoice processing pipeline.\"\"\"\n\n    def __init__(self, base_dir: Path):\n        self.base_dir = base_dir\n        self.input_dir = base_dir / \"input\"\n        self.output_dir = base_dir / \"output\"\n        self.config_dir = base_dir / \"config\"\n        self.templates_dir = base_dir / \"templates\"\n\n        # Ensure directories exist\n        for directory in [self.output_dir, self.config_dir, self.templates_dir]:\n            directory.mkdir(parents=True, exist_ok=True)\n\n    def generate_template_from_schema(\n        self,\n        schema_path: Path,\n        template_path: Path,\n        mode: str = \"file\"\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Generate Excel invoice template from schema.\"\"\"\n\n        print(f\"Generating template from schema: {schema_path}\")\n\n        try:\n            template_df, general_df, specific_df = ExcelInvoiceFile.generate_template(\n                invoice_schema_path=schema_path,\n                save_path=template_path,\n                file_mode=mode\n            )\n\n            print(f\"Template generated successfully:\")\n            print(f\"  Template columns: {template_df.shape[1]}\")\n            print(f\"  General terms: {len(general_df)}\")\n            print(f\"  Specific terms: {len(specific_df)}\")\n            print(f\"  Saved to: {template_path}\")\n\n            return template_df, general_df, specific_df\n\n        except Exception as e:\n            print(f\"Template generation failed: {e}\")\n            raise\n\n    def process_excel_invoice_batch(\n        self,\n        excel_invoice_path: Path,\n        original_invoice_path: Path,\n        schema_path: Path,\n        raw_files_dir: Path\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Process Excel invoice in batch mode.\"\"\"\n\n        print(f\"Processing Excel invoice: {excel_invoice_path}\")\n\n        try:\n            # Read Excel invoice\n            excel_invoice = ExcelInvoiceFile(excel_invoice_path)\n\n            print(f\"Excel invoice loaded:\")\n            print(f\"  Rows: {len(excel_invoice.dfexcelinvoice)}\")\n            print(f\"  Columns: {excel_invoice.dfexcelinvoice.shape[1]}\")\n\n            # Process each row\n            results = []\n\n            for idx, row in excel_invoice.dfexcelinvoice.iterrows():\n                try:\n                    result = self._process_single_row(\n                        excel_invoice=excel_invoice,\n                        row_index=idx,\n                        original_invoice_path=original_invoice_path,\n                        schema_path=schema_path,\n                        raw_files_dir=raw_files_dir\n                    )\n                    results.append(result)\n                    print(f\"\u2713 Processed row {idx}: {result['output_file']}\")\n\n                except Exception as e:\n                    error_result = {\n                        \"row_index\": idx,\n                        \"status\": \"failed\",\n                        \"error\": str(e),\n                        \"output_file\": None\n                    }\n                    results.append(error_result)\n                    print(f\"\u2717 Failed to process row {idx}: {e}\")\n\n            # Summary\n            successful = sum(1 for r in results if r[\"status\"] == \"success\")\n            print(f\"\\nBatch processing summary:\")\n            print(f\"  Total rows: {len(results)}\")\n            print(f\"  Successful: {successful}\")\n            print(f\"  Failed: {len(results) - successful}\")\n\n            return results\n\n        except Exception as e:\n            print(f\"Batch processing failed: {e}\")\n            raise\n\n    def _process_single_row(\n        self,\n        excel_invoice: ExcelInvoiceFile,\n        row_index: int,\n        original_invoice_path: Path,\n        schema_path: Path,\n        raw_files_dir: Path\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Process a single row from Excel invoice.\"\"\"\n\n        # Generate output filename\n        output_filename = f\"invoice_{row_index:04d}.json\"\n        output_path = self.output_dir / output_filename\n\n        # Get data filename for this row\n        row_data = excel_invoice.dfexcelinvoice.iloc[row_index]\n        data_filename = row_data.get(\"data_file_names/name\", f\"data_{row_index}\")\n\n        # Process invoice\n        excel_invoice.overwrite(\n            invoice_org=original_invoice_path,\n            dist_path=output_path,\n            invoice_schema_path=schema_path,\n            idx=row_index\n        )\n\n        # Apply magic variables if needed\n        raw_file_path = raw_files_dir / data_filename\n        if raw_file_path.exists():\n            apply_magic_variable(\n                invoice_path=output_path,\n                rawfile_path=raw_file_path,\n                save_filepath=output_path\n            )\n\n        return {\n            \"row_index\": row_index,\n            \"status\": \"success\",\n            \"output_file\": output_path,\n            \"data_file\": data_filename,\n            \"processed_at\": pd.Timestamp.now().isoformat()\n        }\n\n    def setup_custom_rules(self, rules_config: Dict[str, str]) -&gt; RuleBasedReplacer:\n        \"\"\"Setup custom replacement rules.\"\"\"\n\n        rules_file = self.config_dir / \"custom_rules.json\"\n        replacer = RuleBasedReplacer()\n\n        # Set custom rules\n        for path, variable in rules_config.items():\n            replacer.set_rule(path, variable)\n\n        print(f\"Setup {len(rules_config)} custom rules:\")\n        for path, variable in rules_config.items():\n            print(f\"  {path} -&gt; {variable}\")\n\n        return replacer\n\n    def apply_bulk_transformations(\n        self,\n        invoice_files: List[Path],\n        transformation_rules: Dict[str, Any]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Apply transformations to multiple invoice files.\"\"\"\n\n        replacer = RuleBasedReplacer()\n        results = []\n\n        for invoice_path in invoice_files:\n            try:\n                # Load invoice\n                with open(invoice_path, 'r', encoding='utf-8') as f:\n                    invoice_data = json.load(f)\n\n                # Apply rules\n                transformed_data = replacer.get_apply_rules_obj(\n                    replacements=transformation_rules,\n                    source_json_obj=invoice_data\n                )\n\n                # Save transformed invoice\n                output_path = self.output_dir / f\"transformed_{invoice_path.name}\"\n                with open(output_path, 'w', encoding='utf-8') as f:\n                    json.dump(transformed_data, f, indent=2, ensure_ascii=False)\n\n                results.append({\n                    \"input_file\": invoice_path,\n                    \"output_file\": output_path,\n                    \"status\": \"success\"\n                })\n\n            except Exception as e:\n                results.append({\n                    \"input_file\": invoice_path,\n                    \"output_file\": None,\n                    \"status\": \"failed\",\n                    \"error\": str(e)\n                })\n\n        return results\n\n# Usage example\ndef main():\n    \"\"\"Main processing example.\"\"\"\n\n    # Setup processor\n    base_dir = Path(\"data/invoice_processing\")\n    processor = ExcelInvoiceProcessor(base_dir)\n\n    # File paths\n    schema_path = Path(\"data/tasksupport/invoice.schema.json\")\n    excel_invoice_path = Path(\"data/input/batch_invoice.xlsx\")\n    original_invoice_path = Path(\"data/invoice/invoice.json\")\n    raw_files_dir = Path(\"data/raw_files\")\n\n    try:\n        # Step 1: Generate template from schema\n        template_path = processor.templates_dir / \"generated_template.xlsx\"\n        template_df, general_df, specific_df = processor.generate_template_from_schema(\n            schema_path=schema_path,\n            template_path=template_path,\n            mode=\"file\"\n        )\n\n        # Step 2: Process Excel invoice batch\n        if excel_invoice_path.exists():\n            batch_results = processor.process_excel_invoice_batch(\n                excel_invoice_path=excel_invoice_path,\n                original_invoice_path=original_invoice_path,\n                schema_path=schema_path,\n                raw_files_dir=raw_files_dir\n            )\n\n            # Step 3: Setup and apply custom rules\n            custom_rules = {\n                \"basic.creator\": \"${creator_name}\",\n                \"basic.organization\": \"${organization}\",\n                \"sample.location\": \"${sample_location}\"\n            }\n\n            replacer = processor.setup_custom_rules(custom_rules)\n\n            # Step 4: Apply bulk transformations\n            processed_invoices = [r[\"output_file\"] for r in batch_results\n                                if r[\"status\"] == \"success\" and r[\"output_file\"]]\n\n            transformation_rules = {\n                \"${creator_name}\": \"Research Team Alpha\",\n                \"${organization}\": \"Advanced Research Institute\",\n                \"${sample_location}\": \"Laboratory Building A\"\n            }\n\n            transformation_results = processor.apply_bulk_transformations(\n                invoice_files=processed_invoices,\n                transformation_rules=transformation_rules\n            )\n\n            print(f\"\\nTransformation results:\")\n            successful_transforms = sum(1 for r in transformation_results if r[\"status\"] == \"success\")\n            print(f\"  Successful transformations: {successful_transforms}/{len(transformation_results)}\")\n\n        print(\"\\n\u2705 Invoice processing pipeline completed successfully\")\n\n    except Exception as e:\n        print(f\"\\n\u274c Pipeline failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#smarttable-processing-with-file-mapping","title":"SmartTable Processing with File Mapping","text":"<pre><code>from rdetoolkit.invoicefile import SmartTableFile\nfrom pathlib import Path\nimport pandas as pd\nimport zipfile\nimport tempfile\nimport shutil\nfrom typing import List, Tuple, Dict, Any\n\nclass SmartTableProcessor:\n    \"\"\"Advanced SmartTable processing with file mapping capabilities.\"\"\"\n\n    def __init__(self, working_dir: Path):\n        self.working_dir = working_dir\n        self.temp_dir = working_dir / \"temp\"\n        self.output_dir = working_dir / \"output\"\n        self.processed_dir = working_dir / \"processed\"\n\n        # Create directories\n        for directory in [self.temp_dir, self.output_dir, self.processed_dir]:\n            directory.mkdir(parents=True, exist_ok=True)\n\n    def extract_zip_file(self, zip_path: Path) -&gt; List[Path]:\n        \"\"\"Extract ZIP file and return list of extracted files.\"\"\"\n\n        print(f\"Extracting ZIP file: {zip_path}\")\n\n        try:\n            extracted_files = []\n\n            with zipfile.ZipFile(zip_path, 'r') as zip_file:\n                # Extract all files\n                zip_file.extractall(self.temp_dir)\n\n                # Get list of extracted files\n                for file_info in zip_file.filelist:\n                    if not file_info.is_dir():\n                        extracted_path = self.temp_dir / file_info.filename\n                        if extracted_path.exists():\n                            extracted_files.append(extracted_path)\n\n            print(f\"Extracted {len(extracted_files)} files\")\n            return extracted_files\n\n        except Exception as e:\n            print(f\"ZIP extraction failed: {e}\")\n            raise\n\n    def process_smarttable_with_zip(\n        self,\n        smarttable_path: Path,\n        zip_path: Path | None = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Process SmartTable with optional ZIP file containing data files.\"\"\"\n\n        print(f\"Processing SmartTable: {smarttable_path}\")\n\n        try:\n            # Extract ZIP file if provided\n            extracted_files = []\n            if zip_path and zip_path.exists():\n                extracted_files = self.extract_zip_file(zip_path)\n\n            # Create SmartTable processor\n            smarttable = SmartTableFile(smarttable_path)\n\n            # Read table data\n            table_data = smarttable.read_table()\n            print(f\"SmartTable data: {table_data.shape[0]} rows, {table_data.shape[1]} columns\")\n\n            # Analyze mapping columns\n            mapping_analysis = self._analyze_mapping_columns(table_data)\n            print(f\"Mapping analysis: {mapping_analysis}\")\n\n            # Generate CSV files with file mapping\n            csv_output_dir = self.processed_dir / \"smarttable_csvs\"\n            csv_mappings = smarttable.generate_row_csvs_with_file_mapping(\n                output_dir=csv_output_dir,\n                extracted_files=extracted_files\n            )\n\n            # Process each generated CSV\n            processing_results = []\n            for csv_path, related_files in csv_mappings:\n                result = self._process_single_csv(csv_path, related_files, table_data)\n                processing_results.append(result)\n\n            # Create summary\n            summary = {\n                \"smarttable_file\": str(smarttable_path),\n                \"zip_file\": str(zip_path) if zip_path else None,\n                \"total_rows\": len(table_data),\n                \"total_csvs\": len(csv_mappings),\n                \"extracted_files\": len(extracted_files),\n                \"mapping_analysis\": mapping_analysis,\n                \"processing_results\": processing_results,\n                \"successful_processing\": sum(1 for r in processing_results if r[\"status\"] == \"success\")\n            }\n\n            # Save summary\n            summary_path = self.output_dir / \"smarttable_processing_summary.json\"\n            import json\n            with open(summary_path, 'w', encoding='utf-8') as f:\n                json.dump(summary, f, indent=2, default=str, ensure_ascii=False)\n\n            print(f\"Processing completed: {summary['successful_processing']}/{summary['total_csvs']} successful\")\n            return summary\n\n        except Exception as e:\n            print(f\"SmartTable processing failed: {e}\")\n            raise\n\n    def _analyze_mapping_columns(self, table_data: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"Analyze mapping columns in SmartTable data.\"\"\"\n\n        mapping_prefixes = [\"basic/\", \"custom/\", \"sample/\", \"meta/\"]\n        inputdata_pattern = \"inputdata\"\n\n        analysis = {\n            \"total_columns\": len(table_data.columns),\n            \"mapping_columns\": {},\n            \"inputdata_columns\": [],\n            \"other_columns\": []\n        }\n\n        for column in table_data.columns:\n            # Check for mapping columns\n            mapped = False\n            for prefix in mapping_prefixes:\n                if column.startswith(prefix):\n                    if prefix not in analysis[\"mapping_columns\"]:\n                        analysis[\"mapping_columns\"][prefix] = []\n                    analysis[\"mapping_columns\"][prefix].append(column)\n                    mapped = True\n                    break\n\n            # Check for inputdata columns\n            if inputdata_pattern in column.lower():\n                analysis[\"inputdata_columns\"].append(column)\n                mapped = True\n\n            # Other columns\n            if not mapped:\n                analysis[\"other_columns\"].append(column)\n\n        return analysis\n\n    def _process_single_csv(\n        self,\n        csv_path: Path,\n        related_files: Tuple[Path, ...],\n        original_table_data: pd.DataFrame\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Process a single CSV file generated from SmartTable row.\"\"\"\n\n        try:\n            # Read CSV data\n            csv_data = pd.read_csv(csv_path)\n            row_data = csv_data.iloc[0] if len(csv_data) &gt; 0 else None\n\n            if row_data is None:\n                return {\n                    \"csv_file\": str(csv_path),\n                    \"status\": \"failed\",\n                    \"error\": \"Empty CSV data\"\n                }\n\n            # Extract metadata from row\n            metadata = self._extract_metadata_from_row(row_data)\n\n            # Process related files\n            file_processing_results = []\n            for file_path in related_files:\n                file_result = self._process_related_file(file_path, metadata)\n                file_processing_results.append(file_result)\n\n            # Create output structure\n            output_data = {\n                \"metadata\": metadata,\n                \"related_files\": [str(f) for f in related_files],\n                \"file_processing_results\": file_processing_results\n            }\n\n            # Save individual result\n            output_filename = f\"processed_{csv_path.stem}.json\"\n            output_path = self.output_dir / output_filename\n\n            import json\n            with open(output_path, 'w', encoding='utf-8') as f:\n                json.dump(output_data, f, indent=2, default=str, ensure_ascii=False)\n\n            return {\n                \"csv_file\": str(csv_path),\n                \"output_file\": str(output_path),\n                \"status\": \"success\",\n                \"metadata_fields\": len(metadata),\n                \"related_files_count\": len(related_files),\n                \"processed_files\": len([r for r in file_processing_results if r[\"status\"] == \"success\"])\n            }\n\n        except Exception as e:\n            return {\n                \"csv_file\": str(csv_path),\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def _extract_metadata_from_row(self, row_data: pd.Series) -&gt; Dict[str, Any]:\n        \"\"\"Extract metadata from SmartTable row.\"\"\"\n\n        metadata = {\n            \"basic\": {},\n            \"custom\": {},\n            \"sample\": {},\n            \"meta\": {}\n        }\n\n        for column, value in row_data.items():\n            if pd.isna(value) or value == \"\":\n                continue\n\n            # Parse mapping columns\n            if column.startswith(\"basic/\"):\n                key = column.replace(\"basic/\", \"\")\n                metadata[\"basic\"][key] = value\n            elif column.startswith(\"custom/\"):\n                key = column.replace(\"custom/\", \"\")\n                metadata[\"custom\"][key] = value\n            elif column.startswith(\"sample/\"):\n                key = column.replace(\"sample/\", \"\")\n                metadata[\"sample\"][key] = value\n            elif column.startswith(\"meta/\"):\n                key = column.replace(\"meta/\", \"\")\n                metadata[\"meta\"][key] = value\n\n        return metadata\n\n    def _process_related_file(self, file_path: Path, metadata: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Process a single related file.\"\"\"\n\n        try:\n            file_info = {\n                \"file_path\": str(file_path),\n                \"file_name\": file_path.name,\n                \"file_size\": file_path.stat().st_size,\n                \"file_extension\": file_path.suffix,\n                \"status\": \"success\"\n            }\n\n            # Add file-specific processing based on extension\n            if file_path.suffix.lower() == '.csv':\n                # Process CSV file\n                csv_data = pd.read_csv(file_path)\n                file_info.update({\n                    \"csv_rows\": len(csv_data),\n                    \"csv_columns\": len(csv_data.columns),\n                    \"csv_headers\": list(csv_data.columns)\n                })\n\n            elif file_path.suffix.lower() in ['.txt', '.log']:\n                # Process text file\n                content = file_path.read_text(encoding='utf-8')\n                file_info.update({\n                    \"text_lines\": len(content.splitlines()),\n                    \"text_chars\": len(content)\n                })\n\n            elif file_path.suffix.lower() in ['.json']:\n                # Process JSON file\n                import json\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    json_data = json.load(f)\n                file_info.update({\n                    \"json_keys\": list(json_data.keys()) if isinstance(json_data, dict) else None,\n                    \"json_type\": type(json_data).__name__\n                })\n\n            return file_info\n\n        except Exception as e:\n            return {\n                \"file_path\": str(file_path),\n                \"status\": \"failed\",\n                \"error\": str(e)\n            }\n\n    def cleanup(self):\n        \"\"\"Clean up temporary files.\"\"\"\n        if self.temp_dir.exists():\n            shutil.rmtree(self.temp_dir)\n        print(\"Cleanup completed\")\n\n# Usage example\ndef demonstrate_smarttable_processing():\n    \"\"\"Demonstrate SmartTable processing capabilities.\"\"\"\n\n    # Setup processor\n    working_dir = Path(\"data/smarttable_processing\")\n    processor = SmartTableProcessor(working_dir)\n\n    try:\n        # Example file paths\n        smarttable_path = Path(\"data/input/smarttable_experiment.xlsx\")\n        zip_path = Path(\"data/input/experiment_data.zip\")\n\n        # Process SmartTable with ZIP file\n        if smarttable_path.exists():\n            summary = processor.process_smarttable_with_zip(\n                smarttable_path=smarttable_path,\n                zip_path=zip_path if zip_path.exists() else None\n            )\n\n            print(f\"\\nSmartTable Processing Summary:\")\n            print(f\"  Total rows processed: {summary['total_rows']}\")\n            print(f\"  Generated CSV files: {summary['total_csvs']}\")\n            print(f\"  Successful processing: {summary['successful_processing']}\")\n            print(f\"  Extracted files: {summary['extracted_files']}\")\n\n        else:\n            print(\"SmartTable file not found, creating example...\")\n            # Create example SmartTable for demonstration\n            example_data = pd.DataFrame({\n                \"Display Name\": [\"Sample A\", \"Sample B\"],\n                \"basic/dataName\": [\"experiment_001\", \"experiment_002\"],\n                \"basic/description\": [\"First experiment\", \"Second experiment\"],\n                \"custom/temperature\": [\"25.5\", \"26.1\"],\n                \"sample/location\": [\"Lab A\", \"Lab B\"],\n                \"inputdata1\": [\"data/sample_a.csv\", \"data/sample_b.csv\"],\n                \"inputdata2\": [\"data/metadata_a.json\", \"data/metadata_b.json\"]\n            })\n\n            example_path = working_dir / \"example_smarttable.xlsx\"\n            example_data.to_excel(example_path, index=False)\n            print(f\"Created example SmartTable: {example_path}\")\n\n    finally:\n        # Cleanup\n        processor.cleanup()\n\nif __name__ == \"__main__\":\n    demonstrate_smarttable_processing()\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/invoicefile/#exception-types","title":"Exception Types","text":"<p>The invoicefile module raises <code>StructuredError</code> for various error conditions:</p> <pre><code>from rdetoolkit.exceptions import StructuredError\nfrom rdetoolkit.invoicefile import InvoiceFile, ExcelInvoiceFile\n\n# Handle invoice file errors\ntry:\n    invoice = InvoiceFile(\"nonexistent.json\")\nexcept StructuredError as e:\n    print(f\"Invoice file error: {e}\")\n\n# Handle Excel invoice errors\ntry:\n    excel_invoice = ExcelInvoiceFile(\"invalid.xlsx\")\nexcept StructuredError as e:\n    print(f\"Excel invoice error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Validate Files Before Processing:    <pre><code>def safe_invoice_processing(invoice_path: Path) -&gt; bool:\n    \"\"\"Safely process invoice with validation.\"\"\"\n    if not invoice_path.exists():\n        print(f\"Invoice file not found: {invoice_path}\")\n        return False\n\n    try:\n        invoice = InvoiceFile(invoice_path)\n        # Process invoice...\n        return True\n    except StructuredError as e:\n        print(f\"Invoice processing failed: {e}\")\n        return False\n</code></pre></p> </li> <li> <p>Handle Template Generation Errors:    <pre><code>def safe_template_generation(schema_path: Path, output_path: Path):\n    \"\"\"Generate template with error handling.\"\"\"\n    try:\n        return ExcelInvoiceFile.generate_template(schema_path, output_path)\n    except StructuredError as e:\n        print(f\"Template generation failed: {e}\")\n        return None, None, None\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/invoicefile/#performance-notes","title":"Performance Notes","text":""},{"location":"rdetoolkit/invoicefile/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Batch Processing: Process multiple invoice files efficiently</li> <li>Memory Management: Use DataFrames efficiently for large Excel files</li> <li>File I/O Optimization: Minimize repeated file reading operations</li> <li>Template Caching: Cache generated templates for reuse</li> </ol>"},{"location":"rdetoolkit/invoicefile/#performance-best-practices","title":"Performance Best Practices","text":"<pre><code># Efficient batch processing\ndef process_invoices_efficiently(invoice_paths: List[Path]):\n    \"\"\"Process multiple invoices efficiently.\"\"\"\n\n    # Load schema once\n    schema_path = Path(\"data/tasksupport/invoice.schema.json\")\n\n    for invoice_path in invoice_paths:\n        try:\n            invoice = InvoiceFile(invoice_path)\n            # Process with shared schema...\n        except Exception as e:\n            print(f\"Skipping {invoice_path}: {e}\")\n</code></pre>"},{"location":"rdetoolkit/invoicefile/#see-also","title":"See Also","text":"<ul> <li>Core Module - For directory operations and file handling</li> <li>File Operations - For JSON file reading and writing utilities</li> <li>Models - Invoice - For invoice data structure definitions</li> <li>Models - Invoice Schema - For invoice schema definitions</li> <li>Validation - For invoice validation functionality</li> <li>RDE2 Utilities - For metadata processing utilities</li> <li>Exceptions - For StructuredError and other exception types</li> <li>Usage - CLI - For command-line invoice processing</li> <li>Usage - Structured Process - For invoice processing in workflows</li> </ul>"},{"location":"rdetoolkit/modeproc/","title":"Mode Processing Module","text":"<p>The <code>rdetoolkit.modeproc</code> module provides specialized processing functions for different input modes in the RDE (Research Data Exchange) structuring pipeline. This module implements the core processing logic for various data input patterns and formats.</p>"},{"location":"rdetoolkit/modeproc/#overview","title":"Overview","text":"<p>The modeproc module handles different types of data processing modes, each tailored to specific input patterns and requirements:</p> <ul> <li>Invoice Mode: Standard invoice-based data processing</li> <li>Excel Invoice Mode: Processing with Excel-based invoice files</li> <li>SmartTable Mode: Advanced table-based processing with automated invoice generation</li> <li>RDE Format Mode: Processing using RDE format specifications</li> <li>Multi-File Mode: Handling multiple data files in parallel</li> <li>Input File Classification: Automatic detection and routing to appropriate processors</li> </ul>"},{"location":"rdetoolkit/modeproc/#type-definitions","title":"Type Definitions","text":""},{"location":"rdetoolkit/modeproc/#_callbacktype","title":"_CallbackType","text":"<pre><code>_CallbackType = Callable[[RdeInputDirPaths, RdeOutputResourcePath], None]\n</code></pre> <p>A type alias for custom dataset processing functions that accept input paths and output resource paths.</p>"},{"location":"rdetoolkit/modeproc/#functions","title":"Functions","text":""},{"location":"rdetoolkit/modeproc/#invoice_mode_process","title":"invoice_mode_process","text":"<p>Process invoice-related data with standard RDE pipeline operations.</p> <pre><code>def invoice_mode_process(\n    index: str,\n    srcpaths: RdeInputDirPaths,\n    resource_paths: RdeOutputResourcePath,\n    datasets_process_function: _CallbackType | None = None,\n) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters:</p> <ul> <li><code>index</code> (str): Unique workflow execution identifier (run_id)</li> <li><code>srcpaths</code> (RdeInputDirPaths): Input directory paths for source data</li> <li><code>resource_paths</code> (RdeOutputResourcePath): Output resource paths for processed data</li> <li><code>datasets_process_function</code> (_CallbackType | None): Optional custom dataset processing function</li> </ul> <p>Returns:</p> <ul> <li><code>WorkflowExecutionStatus</code>: Execution status containing run details, success/failure status, and error information</li> </ul> <p>Processing Steps:</p> <ol> <li>Copy input files to raw file directory</li> <li>Execute custom dataset processing function (if provided)</li> <li>Copy images to thumbnail directory</li> <li>Replace <code>${filename}</code> placeholders in invoice with actual filenames</li> <li>Update descriptions with features (errors ignored)</li> <li>Validate metadata-def.json file</li> <li>Validate invoice file against invoice schema</li> </ol> <p>Raises:</p> <ul> <li>Propagates exceptions from <code>datasets_process_function</code></li> <li>Validation errors during metadata or invoice validation</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import invoice_mode_process\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\nfrom pathlib import Path\n\ndef custom_invoice_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Custom processing for invoice data.\"\"\"\n    print(f\"Processing {len(resource_paths.rawfiles)} files\")\n\n    # Custom invoice processing logic\n    for raw_file in resource_paths.rawfiles:\n        if raw_file.suffix == '.csv':\n            # Process CSV files\n            processed_path = resource_paths.struct / f\"processed_{raw_file.name}\"\n            # ... processing logic\n\n# Setup paths\nsrcpaths = RdeInputDirPaths(\n    inputdata=Path(\"data/inputdata\"),\n    invoice=Path(\"data/invoice\"),\n    tasksupport=Path(\"data/tasksupport\")\n)\n\nresource_paths = RdeOutputResourcePath(\n    raw=Path(\"data/raw\"),\n    rawfiles=(Path(\"data/inputdata/invoice.csv\"),),\n    struct=Path(\"data/structured\"),\n    # ... other paths\n)\n\n# Execute invoice mode processing\nstatus = invoice_mode_process(\n    index=\"001\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_process_function=custom_invoice_processor\n)\n\nprint(f\"Status: {status.status}\")\nprint(f\"Mode: {status.mode}\")\n</code></pre>"},{"location":"rdetoolkit/modeproc/#excel_invoice_mode_process","title":"excel_invoice_mode_process","text":"<p>Process data using Excel-based invoice files with enhanced metadata handling.</p> <pre><code>def excel_invoice_mode_process(\n    srcpaths: RdeInputDirPaths,\n    resource_paths: RdeOutputResourcePath,\n    excel_invoice_file: Path,\n    idx: int,\n    datasets_process_function: _CallbackType | None = None,\n) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters:</p> <ul> <li><code>srcpaths</code> (RdeInputDirPaths): Input directory paths for source data</li> <li><code>resource_paths</code> (RdeOutputResourcePath): Output resource paths for processed data</li> <li><code>excel_invoice_file</code> (Path): Path to the Excel invoice file</li> <li><code>idx</code> (int): Index identifier for the data being processed</li> <li><code>datasets_process_function</code> (_CallbackType | None): Optional custom dataset processing function</li> </ul> <p>Returns:</p> <ul> <li><code>WorkflowExecutionStatus</code>: Detailed execution status information</li> </ul> <p>Processing Steps:</p> <ol> <li>Overwrite Excel invoice file with processed data</li> <li>Copy input files to raw file directory</li> <li>Execute custom dataset processing function (if provided)</li> <li>Replace <code>${filename}</code> placeholders in invoice</li> <li>Copy images to thumbnail directory</li> <li>Update descriptions with features (errors ignored)</li> <li>Validate metadata-def.json file</li> <li>Validate invoice file against schema</li> </ol> <p>Raises:</p> <ul> <li><code>StructuredError</code>: Issues with Excel invoice processing or validation</li> <li>Propagates exceptions from custom processing functions</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import excel_invoice_mode_process\nfrom pathlib import Path\n\ndef excel_data_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Process data with Excel invoice context.\"\"\"\n\n    # Access Excel-specific processing\n    for raw_file in resource_paths.rawfiles:\n        if raw_file.suffix in ['.xlsx', '.xls']:\n            # Excel-specific processing\n            excel_data = pd.read_excel(raw_file)\n            # Process Excel data...\n\n        elif raw_file.suffix == '.csv':\n            # CSV processing in Excel context\n            csv_data = pd.read_csv(raw_file)\n            # Process CSV data...\n\n# Execute Excel invoice processing\nexcel_file = Path(\"data/inputdata/dataset_excel_invoice.xlsx\")\nstatus = excel_invoice_mode_process(\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    excel_invoice_file=excel_file,\n    idx=0,\n    datasets_process_function=excel_data_processor\n)\n</code></pre>"},{"location":"rdetoolkit/modeproc/#smarttable_invoice_mode_process","title":"smarttable_invoice_mode_process","text":"<p>Process SmartTable files to generate automated invoice data with intelligent table parsing.</p> <pre><code>def smarttable_invoice_mode_process(\n    index: str,\n    srcpaths: RdeInputDirPaths,\n    resource_paths: RdeOutputResourcePath,\n    smarttable_file: Path,\n    datasets_process_function: _CallbackType | None = None,\n) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters:</p> <ul> <li><code>index</code> (str): Unique workflow execution identifier</li> <li><code>srcpaths</code> (RdeInputDirPaths): Input directory paths</li> <li><code>resource_paths</code> (RdeOutputResourcePath): Output resource paths</li> <li><code>smarttable_file</code> (Path): Path to SmartTable file (.xlsx, .csv, .tsv)</li> <li><code>datasets_process_function</code> (_CallbackType | None): Optional custom processing function</li> </ul> <p>Returns:</p> <ul> <li><code>WorkflowExecutionStatus</code>: Execution status with SmartTable-specific details</li> </ul> <p>Processing Steps:</p> <ol> <li>Initialize invoice from SmartTable file data</li> <li>Copy input files to raw file directory</li> <li>Execute custom dataset processing function (if provided)</li> <li>Copy images to thumbnail directory</li> <li>Replace filename placeholders in invoice</li> <li>Update descriptions with features (errors ignored)</li> <li>Validate metadata-def.json file</li> <li>Validate invoice file against schema</li> </ol> <p>Raises:</p> <ul> <li><code>StructuredError</code>: SmartTable processing or validation errors</li> <li>Propagates exceptions from custom processing functions</li> </ul> <p>Supported SmartTable Formats:</p> <ul> <li>Excel files (.xlsx, .xls)</li> <li>CSV files (.csv)</li> <li>TSV files (.tsv)</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import smarttable_invoice_mode_process\nfrom pathlib import Path\nimport pandas as pd\n\ndef smarttable_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Custom processing for SmartTable data.\"\"\"\n\n    # SmartTable files are automatically processed for invoice generation\n    # Custom processing can focus on data transformation\n\n    for raw_file in resource_paths.rawfiles:\n        if raw_file.suffix == '.csv':\n            # Enhanced CSV processing with SmartTable context\n            df = pd.read_csv(raw_file)\n\n            # Apply SmartTable-aware transformations\n            enhanced_df = df.copy()\n            enhanced_df['processing_timestamp'] = pd.Timestamp.now()\n            enhanced_df['smarttable_processed'] = True\n\n            # Save enhanced data\n            output_path = resource_paths.struct / f\"enhanced_{raw_file.name}\"\n            enhanced_df.to_csv(output_path, index=False)\n\n# Execute SmartTable processing\nsmarttable_file = Path(\"data/inputdata/smarttable_metadata.xlsx\")\nstatus = smarttable_invoice_mode_process(\n    index=\"001\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    smarttable_file=smarttable_file,\n    datasets_process_function=smarttable_processor\n)\n\nprint(f\"SmartTable processing status: {status.status}\")\n</code></pre>"},{"location":"rdetoolkit/modeproc/#rdeformat_mode_process","title":"rdeformat_mode_process","text":"<p>Process data using RDE format specifications with advanced structure handling.</p> <pre><code>def rdeformat_mode_process(\n    index: str,\n    srcpaths: RdeInputDirPaths,\n    resource_paths: RdeOutputResourcePath,\n    datasets_process_function: _CallbackType | None = None,\n) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters:</p> <ul> <li><code>index</code> (str): Unique workflow execution identifier</li> <li><code>srcpaths</code> (RdeInputDirPaths): Input directory paths</li> <li><code>resource_paths</code> (RdeOutputResourcePath): Output resource paths</li> <li><code>datasets_process_function</code> (_CallbackType | None): Optional custom processing function</li> </ul> <p>Returns:</p> <ul> <li><code>WorkflowExecutionStatus</code>: Execution status for RDE format processing</li> </ul> <p>Processing Steps:</p> <ol> <li>Overwrite invoice file with RDE format specifications</li> <li>Copy input files to raw file directory using RDE format rules</li> <li>Execute custom dataset processing function (if provided)</li> <li>Copy images to thumbnail directory</li> <li>Update descriptions with features (errors ignored)</li> <li>Validate metadata-def.json file</li> <li>Validate invoice file against schema</li> </ol> <p>RDE Format Features:</p> <ul> <li>Structured directory organization</li> <li>Predefined metadata schemas</li> <li>Standardized file naming conventions</li> <li>Automated validation pipelines</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import rdeformat_mode_process\nfrom pathlib import Path\nimport json\n\ndef rdeformat_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Custom processing for RDE format data.\"\"\"\n\n    # RDE format provides structured processing\n    rde_metadata = {\n        \"format_version\": \"2.0\",\n        \"processing_mode\": \"rdeformat\",\n        \"files_processed\": []\n    }\n\n    for raw_file in resource_paths.rawfiles:\n        # Process according to RDE format specifications\n        if 'structured' in str(raw_file):\n            # Handle structured data files\n            structured_output = resource_paths.struct / raw_file.name\n            # Copy and validate structured data\n            import shutil\n            shutil.copy2(raw_file, structured_output)\n\n        elif 'meta' in str(raw_file):\n            # Handle metadata files\n            meta_output = resource_paths.meta / raw_file.name\n            shutil.copy2(raw_file, meta_output)\n\n        rde_metadata[\"files_processed\"].append({\n            \"source\": str(raw_file),\n            \"type\": \"structured\" if 'structured' in str(raw_file) else \"data\"\n        })\n\n    # Save RDE processing metadata\n    metadata_path = resource_paths.meta / \"rde_processing_metadata.json\"\n    with open(metadata_path, 'w') as f:\n        json.dump(rde_metadata, f, indent=2)\n\n# Execute RDE format processing\nstatus = rdeformat_mode_process(\n    index=\"001\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_process_function=rdeformat_processor\n)\n</code></pre>"},{"location":"rdetoolkit/modeproc/#multifile_mode_process","title":"multifile_mode_process","text":"<p>Process multiple source files simultaneously with parallel processing capabilities.</p> <pre><code>def multifile_mode_process(\n    index: str,\n    srcpaths: RdeInputDirPaths,\n    resource_paths: RdeOutputResourcePath,\n    datasets_process_function: _CallbackType | None = None,\n) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters:</p> <ul> <li><code>index</code> (str): Unique workflow execution identifier</li> <li><code>srcpaths</code> (RdeInputDirPaths): Input directory paths</li> <li><code>resource_paths</code> (RdeOutputResourcePath): Output resource paths</li> <li><code>datasets_process_function</code> (_CallbackType | None): Optional custom processing function</li> </ul> <p>Returns:</p> <ul> <li><code>WorkflowExecutionStatus</code>: Execution status for multi-file processing</li> </ul> <p>Processing Steps:</p> <ol> <li>Overwrite invoice file for multi-file context</li> <li>Copy all input files to raw file directory</li> <li>Execute custom dataset processing function (if provided)</li> <li>Replace filename placeholders for all files</li> <li>Copy images to thumbnail directory</li> <li>Update descriptions with features (errors ignored)</li> <li>Validate metadata-def.json file</li> <li>Validate invoice file against schema</li> </ol> <p>Multi-File Features:</p> <ul> <li>Parallel processing of multiple files</li> <li>Batch operations on file groups</li> <li>Coordinated metadata generation</li> <li>Error tolerance configuration</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import multifile_mode_process\nfrom pathlib import Path\nimport concurrent.futures\nimport pandas as pd\n\ndef multifile_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Custom processing for multiple files.\"\"\"\n\n    def process_single_file(raw_file: Path) -&gt; dict:\n        \"\"\"Process a single file and return metadata.\"\"\"\n        file_info = {\n            \"filename\": raw_file.name,\n            \"size\": raw_file.stat().st_size,\n            \"type\": raw_file.suffix,\n            \"processed\": False\n        }\n\n        try:\n            if raw_file.suffix == '.csv':\n                # Process CSV files\n                df = pd.read_csv(raw_file)\n                processed_path = resource_paths.struct / f\"processed_{raw_file.name}\"\n\n                # Add processing metadata to DataFrame\n                df['file_source'] = raw_file.name\n                df['processing_index'] = resource_paths.rawfiles.index(raw_file)\n\n                df.to_csv(processed_path, index=False)\n                file_info[\"processed\"] = True\n                file_info[\"rows\"] = len(df)\n\n            elif raw_file.suffix in ['.txt', '.log']:\n                # Process text files\n                content = raw_file.read_text(encoding='utf-8')\n                processed_path = resource_paths.struct / f\"processed_{raw_file.name}\"\n\n                # Add processing header\n                processed_content = f\"# Processed from {raw_file.name}\\n{content}\"\n                processed_path.write_text(processed_content, encoding='utf-8')\n\n                file_info[\"processed\"] = True\n                file_info[\"lines\"] = len(content.splitlines())\n\n        except Exception as e:\n            file_info[\"error\"] = str(e)\n\n        return file_info\n\n    # Process files in parallel\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        future_to_file = {\n            executor.submit(process_single_file, raw_file): raw_file\n            for raw_file in resource_paths.rawfiles\n        }\n\n        results = []\n        for future in concurrent.futures.as_completed(future_to_file):\n            file_result = future.result()\n            results.append(file_result)\n\n    # Save batch processing results\n    batch_metadata = {\n        \"total_files\": len(resource_paths.rawfiles),\n        \"processed_files\": sum(1 for r in results if r.get(\"processed\", False)),\n        \"failed_files\": sum(1 for r in results if \"error\" in r),\n        \"file_details\": results\n    }\n\n    metadata_path = resource_paths.meta / \"multifile_batch_metadata.json\"\n    import json\n    with open(metadata_path, 'w') as f:\n        json.dump(batch_metadata, f, indent=2)\n\n# Execute multi-file processing\nstatus = multifile_mode_process(\n    index=\"001\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_process_function=multifile_processor\n)\n\nprint(f\"Processed {len(resource_paths.rawfiles)} files with status: {status.status}\")\n</code></pre>"},{"location":"rdetoolkit/modeproc/#copy_input_to_rawfile","title":"copy_input_to_rawfile","text":"<p>Copy input raw files to a specified directory.</p> <pre><code>def copy_input_to_rawfile(raw_dir_path: Path, raw_files: tuple[Path, ...]) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>raw_dir_path</code> (Path): Target directory for copying raw files</li> <li><code>raw_files</code> (tuple[Path, ...]): Tuple of source file paths to copy</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import copy_input_to_rawfile\nfrom pathlib import Path\n\n# Setup source files and target directory\nraw_files = (\n    Path(\"data/input/file1.csv\"),\n    Path(\"data/input/file2.txt\"),\n    Path(\"data/input/file3.json\")\n)\n\ntarget_dir = Path(\"data/raw\")\ntarget_dir.mkdir(parents=True, exist_ok=True)\n\n# Copy files to raw directory\ncopy_input_to_rawfile(target_dir, raw_files)\n\n# Verify files were copied\nfor file in raw_files:\n    copied_file = target_dir / file.name\n    assert copied_file.exists(), f\"File {file.name} not copied\"\n</code></pre>"},{"location":"rdetoolkit/modeproc/#copy_input_to_rawfile_for_rdeformat","title":"copy_input_to_rawfile_for_rdeformat","text":"<p>Copy input files to appropriate directories based on RDE format directory structure.</p> <pre><code>def copy_input_to_rawfile_for_rdeformat(resource_paths: RdeOutputResourcePath) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>resource_paths</code> (RdeOutputResourcePath): Resource paths containing source files and target directories</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Directory Mapping:</p> <ul> <li><code>raw</code> \u2192 <code>resource_paths.raw</code></li> <li><code>main_image</code> \u2192 <code>resource_paths.main_image</code></li> <li><code>other_image</code> \u2192 <code>resource_paths.other_image</code></li> <li><code>meta</code> \u2192 <code>resource_paths.meta</code></li> <li><code>structured</code> \u2192 <code>resource_paths.struct</code></li> <li><code>logs</code> \u2192 <code>resource_paths.logs</code></li> <li><code>nonshared_raw</code> \u2192 <code>resource_paths.nonshared_raw</code></li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import copy_input_to_rawfile_for_rdeformat\nfrom rdetoolkit.models.rde2types import RdeOutputResourcePath\nfrom pathlib import Path\n\n# Setup resource paths with RDE format structure\nresource_paths = RdeOutputResourcePath(\n    raw=Path(\"data/raw\"),\n    rawfiles=(\n        Path(\"data/temp/raw/data.csv\"),\n        Path(\"data/temp/main_image/image.jpg\"),\n        Path(\"data/temp/meta/metadata.json\"),\n        Path(\"data/temp/structured/analysis.txt\")\n    ),\n    struct=Path(\"data/structured\"),\n    main_image=Path(\"data/main_image\"),\n    other_image=Path(\"data/other_image\"),\n    meta=Path(\"data/meta\"),\n    logs=Path(\"data/logs\"),\n    # ... other paths\n)\n\n# Create target directories\nfor attr_name in ['raw', 'struct', 'main_image', 'other_image', 'meta', 'logs', 'nonshared_raw']:\n    getattr(resource_paths, attr_name).mkdir(parents=True, exist_ok=True)\n\n# Copy files according to RDE format rules\ncopy_input_to_rawfile_for_rdeformat(resource_paths)\n\n# Files are automatically placed in correct directories based on their path structure\n</code></pre>"},{"location":"rdetoolkit/modeproc/#selected_input_checker","title":"selected_input_checker","text":"<p>Determine and return the appropriate input file checker based on file patterns and mode settings.</p> <pre><code>def selected_input_checker(src_paths: RdeInputDirPaths, unpacked_dir_path: Path, mode: str | None) -&gt; IInputFileChecker\n</code></pre> <p>Parameters:</p> <ul> <li><code>src_paths</code> (RdeInputDirPaths): Source input file paths</li> <li><code>unpacked_dir_path</code> (Path): Directory path for unpacked files</li> <li><code>mode</code> (str | None): Processing mode specification</li> </ul> <p>Returns:</p> <ul> <li><code>IInputFileChecker</code>: Appropriate checker instance for the detected file type</li> </ul> <p>Checker Selection Logic:</p> <ol> <li>SmartTableChecker: If files starting with <code>smarttable_</code> and extensions <code>.xlsx</code>, <code>.csv</code>, <code>.tsv</code> are found</li> <li>ExcelInvoiceChecker: If Excel files ending with <code>_excel_invoice</code> are found</li> <li>RDEFormatChecker: If mode is <code>\"rdeformat\"</code></li> <li>MultiFileChecker: If mode is <code>\"multidatatile\"</code></li> <li>InvoiceChecker: Default fallback for standard invoice processing</li> </ol> <p>Example:</p> <pre><code>from rdetoolkit.modeproc import selected_input_checker\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths\nfrom pathlib import Path\n\n# Setup input paths\nsrc_paths = RdeInputDirPaths(\n    inputdata=Path(\"data/inputdata\"),\n    invoice=Path(\"data/invoice\"),\n    tasksupport=Path(\"data/tasksupport\")\n)\n\nunpacked_dir = Path(\"data/temp\")\n\n# Test different scenarios\n\n# Scenario 1: SmartTable files detected\n# Files: smarttable_metadata.xlsx, data.csv\nchecker = selected_input_checker(src_paths, unpacked_dir, None)\nprint(f\"Checker type: {checker.checker_type}\")  # \"smarttable\"\n\n# Scenario 2: Excel invoice files detected\n# Files: dataset_excel_invoice.xlsx, data.zip\nchecker = selected_input_checker(src_paths, unpacked_dir, None)\nprint(f\"Checker type: {checker.checker_type}\")  # \"excel_invoice\"\n\n# Scenario 3: RDE format mode\nchecker = selected_input_checker(src_paths, unpacked_dir, \"rdeformat\")\nprint(f\"Checker type: {checker.checker_type}\")  # \"rdeformat\"\n\n# Scenario 4: Multi-file mode\nchecker = selected_input_checker(src_paths, unpacked_dir, \"multidatatile\")\nprint(f\"Checker type: {checker.checker_type}\")  # \"multifile\"\n\n# Scenario 5: Default invoice mode\nchecker = selected_input_checker(src_paths, unpacked_dir, None)\nprint(f\"Checker type: {checker.checker_type}\")  # \"invoice\"\n</code></pre>"},{"location":"rdetoolkit/modeproc/#workflowexecutionstatus","title":"WorkflowExecutionStatus","text":"<p>All processing functions return a <code>WorkflowExecutionStatus</code> object containing:</p>"},{"location":"rdetoolkit/modeproc/#attributes","title":"Attributes","text":"<ul> <li><code>run_id</code> (str): Unique identifier for workflow execution (zero-padded to 4 digits)</li> <li><code>title</code> (str): Descriptive title for the workflow execution</li> <li><code>status</code> (str): Execution status (<code>\"success\"</code> or <code>\"failed\"</code>)</li> <li><code>mode</code> (str): Processing mode used (e.g., <code>\"invoice\"</code>, <code>\"rdeformat\"</code>, <code>\"Excelinvoice\"</code>)</li> <li><code>error_code</code> (int | None): Error code if execution failed</li> <li><code>error_message</code> (str | None): Error message if execution failed</li> <li><code>stacktrace</code> (str | None): Stack trace for debugging if execution failed</li> <li><code>target</code> (str): Target directory or file path related to execution</li> </ul>"},{"location":"rdetoolkit/modeproc/#example-status-handling","title":"Example Status Handling","text":"<pre><code>from rdetoolkit.modeproc import invoice_mode_process\n\nstatus = invoice_mode_process(\n    index=\"001\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths\n)\n\nif status.status == \"success\":\n    print(f\"Processing completed successfully\")\n    print(f\"Mode: {status.mode}\")\n    print(f\"Target: {status.target}\")\nelse:\n    print(f\"Processing failed: {status.error_message}\")\n    print(f\"Error code: {status.error_code}\")\n    if status.stacktrace:\n        print(f\"Stack trace: {status.stacktrace}\")\n</code></pre>"},{"location":"rdetoolkit/modeproc/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/modeproc/#exception-types","title":"Exception Types","text":"<ul> <li>StructuredError: Raised for structured processing failures</li> <li>Validation Errors: During metadata or invoice schema validation</li> <li>File Operation Errors: During file copying or directory operations</li> <li>Custom Function Errors: Propagated from user-defined processing functions</li> </ul>"},{"location":"rdetoolkit/modeproc/#best-practices","title":"Best Practices","text":"<ol> <li>Handle Custom Function Errors:</li> </ol> <pre><code>def safe_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    try:\n        # Your processing logic\n        pass\n    except Exception as e:\n        logger.error(f\"Custom processing failed: {e}\")\n        # Don't re-raise to allow workflow to continue\n</code></pre> <ol> <li>Validate Inputs:</li> </ol> <pre><code>def validated_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    if not resource_paths.rawfiles:\n        print(\"No files to process\")\n        return\n\n    for raw_file in resource_paths.rawfiles:\n        if not raw_file.exists():\n            print(f\"Warning: File not found: {raw_file}\")\n            continue\n        # Process file...\n</code></pre> <ol> <li>Use Appropriate Error Tolerance:</li> </ol> <pre><code># In workflows.run() configuration\nconfig = Config(\n    multidata_tile=MultiDataTileSettings(\n        ignore_errors=True  # Continue processing on individual failures\n    )\n)\n</code></pre>"},{"location":"rdetoolkit/modeproc/#integration-examples","title":"Integration Examples","text":""},{"location":"rdetoolkit/modeproc/#complete-processing-workflow","title":"Complete Processing Workflow","text":"<pre><code>from rdetoolkit.modeproc import selected_input_checker, invoice_mode_process\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\nfrom pathlib import Path\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef comprehensive_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Comprehensive data processing function.\"\"\"\n\n    logger.info(f\"Starting processing of {len(resource_paths.rawfiles)} files\")\n\n    processing_summary = {\n        \"files_processed\": 0,\n        \"files_failed\": 0,\n        \"processing_details\": []\n    }\n\n    for raw_file in resource_paths.rawfiles:\n        try:\n            file_detail = {\"filename\": raw_file.name, \"status\": \"processing\"}\n\n            if raw_file.suffix == '.csv':\n                # CSV processing\n                import pandas as pd\n                df = pd.read_csv(raw_file)\n\n                # Data validation\n                if df.empty:\n                    raise ValueError(\"CSV file is empty\")\n\n                # Data transformation\n                df['processed_timestamp'] = pd.Timestamp.now()\n                df['source_file'] = raw_file.name\n\n                # Save processed data\n                output_path = resource_paths.struct / f\"enhanced_{raw_file.name}\"\n                df.to_csv(output_path, index=False)\n\n                file_detail.update({\n                    \"status\": \"success\",\n                    \"rows_processed\": len(df),\n                    \"output_file\": str(output_path)\n                })\n                processing_summary[\"files_processed\"] += 1\n\n            elif raw_file.suffix in ['.txt', '.log']:\n                # Text file processing\n                content = raw_file.read_text(encoding='utf-8')\n\n                # Text analysis\n                lines = content.splitlines()\n                word_count = len(content.split())\n\n                # Enhanced content with metadata\n                enhanced_content = f\"\"\"# Processing Metadata\n# Original file: {raw_file.name}\n# Lines: {len(lines)}\n# Words: {word_count}\n# Processed: {pd.Timestamp.now()}\n\n{content}\n\"\"\"\n\n                # Save enhanced content\n                output_path = resource_paths.struct / f\"enhanced_{raw_file.name}\"\n                output_path.write_text(enhanced_content, encoding='utf-8')\n\n                file_detail.update({\n                    \"status\": \"success\",\n                    \"lines_processed\": len(lines),\n                    \"words\": word_count,\n                    \"output_file\": str(output_path)\n                })\n                processing_summary[\"files_processed\"] += 1\n\n            else:\n                # Handle other file types\n                import shutil\n                output_path = resource_paths.struct / raw_file.name\n                shutil.copy2(raw_file, output_path)\n\n                file_detail.update({\n                    \"status\": \"copied\",\n                    \"output_file\": str(output_path)\n                })\n                processing_summary[\"files_processed\"] += 1\n\n        except Exception as e:\n            logger.error(f\"Failed to process {raw_file.name}: {e}\")\n            file_detail.update({\n                \"status\": \"failed\",\n                \"error\": str(e)\n            })\n            processing_summary[\"files_failed\"] += 1\n\n        processing_summary[\"processing_details\"].append(file_detail)\n\n    # Save processing summary\n    summary_path = resource_paths.meta / \"processing_summary.json\"\n    import json\n    with open(summary_path, 'w') as f:\n        json.dump(processing_summary, f, indent=2, default=str)\n\n    logger.info(f\"Processing complete: {processing_summary['files_processed']} successful, {processing_summary['files_failed']} failed\")\n\n# Setup and execute processing\nsrcpaths = RdeInputDirPaths(\n    inputdata=Path(\"data/inputdata\"),\n    invoice=Path(\"data/invoice\"),\n    tasksupport=Path(\"data/tasksupport\")\n)\n\n# Automatically select appropriate checker\nchecker = selected_input_checker(srcpaths, Path(\"data/temp\"), None)\nraw_files, excel_file, smarttable_file = checker.parse(srcpaths.inputdata)\n\n# Execute processing based on detected type\nfor idx, raw_file_group in enumerate(raw_files):\n    resource_paths = RdeOutputResourcePath(\n        raw=Path(f\"data/raw/{idx:04d}\"),\n        rawfiles=raw_file_group,\n        struct=Path(f\"data/structured/{idx:04d}\"),\n        # ... setup other paths\n    )\n\n    # Create directories\n    for attr_name in ['raw', 'struct', 'meta', 'main_image', 'thumbnail']:\n        getattr(resource_paths, attr_name).mkdir(parents=True, exist_ok=True)\n\n    # Execute appropriate processing mode\n    if smarttable_file:\n        from rdetoolkit.modeproc import smarttable_invoice_mode_process\n        status = smarttable_invoice_mode_process(\n            str(idx), srcpaths, resource_paths, smarttable_file, comprehensive_processor\n        )\n    elif excel_file:\n        from rdetoolkit.modeproc import excel_invoice_mode_process\n        status = excel_invoice_mode_process(\n            srcpaths, resource_paths, excel_file, idx, comprehensive_processor\n        )\n    else:\n        status = invoice_mode_process(\n            str(idx), srcpaths, resource_paths, comprehensive_processor\n        )\n\n    print(f\"Batch {idx}: {status.status} ({status.mode})\")\n</code></pre>"},{"location":"rdetoolkit/modeproc/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>File I/O Optimization: Use efficient file operations for large datasets</li> <li>Memory Management: Process files in chunks for large datasets</li> <li>Parallel Processing: Utilize multi-threading for independent file processing</li> <li>Error Recovery: Implement graceful error handling to continue processing other files</li> <li>Progress Tracking: Provide feedback for long-running operations</li> </ul>"},{"location":"rdetoolkit/modeproc/#see-also","title":"See Also","text":"<ul> <li>Workflows Module - For orchestrating mode processing</li> <li>Core Module - For directory management and file operations</li> <li>Configuration Guide - For configuring processing behavior</li> <li>Input Controllers - For file type detection and validation</li> </ul>"},{"location":"rdetoolkit/rde2util/","title":"rde2util Module","text":"<p>The <code>rdetoolkit.rde2util</code> module provides essential utility functions and classes for RDE (Research Data Exchange) data processing and manipulation. This module includes metadata handling, file encoding detection, storage directory management, and various data conversion utilities.</p>"},{"location":"rdetoolkit/rde2util/#overview","title":"Overview","text":"<p>The rde2util module offers comprehensive utilities for RDE processing workflows:</p> <ul> <li>Metadata Management: Creation, validation, and processing of metadata structures</li> <li>Storage Directory Operations: Organized directory creation and management for RDE data</li> <li>Character Encoding Detection: Robust encoding detection for text files, including Japanese encodings</li> <li>Data Type Conversion: Flexible value casting and format conversion utilities</li> <li>ZIP File Handling: Specialized ZIP extraction for Japanese-encoded file names</li> <li>JSON Operations: Convenient JSON file reading and writing with encoding support</li> <li>Value Processing: Unit-value pair splitting and conversion utilities</li> </ul>"},{"location":"rdetoolkit/rde2util/#classes","title":"Classes","text":""},{"location":"rdetoolkit/rde2util/#meta","title":"Meta","text":"<p>A comprehensive class for initializing, processing, and managing metadata from definition files.</p>"},{"location":"rdetoolkit/rde2util/#constructor","title":"Constructor","text":"<pre><code>Meta(metadef_filepath: RdeFsPath, *, metafilepath: RdeFsPath | None = None)\n</code></pre> <p>Parameters: - <code>metadef_filepath</code> (RdeFsPath): Path to the metadata definition file (metadata-def.json) - <code>metafilepath</code> (RdeFsPath | None): Path to existing metadata file (currently not supported)</p> <p>Raises: - <code>StructuredError</code>: If <code>metafilepath</code> is provided (loading existing metadata not supported)</p>"},{"location":"rdetoolkit/rde2util/#attributes","title":"Attributes","text":"<ul> <li><code>metaConst</code> (dict[str, MetaItem]): Dictionary for constant metadata</li> <li><code>metaVar</code> (list[dict[str, MetaItem]]): List of dictionaries for variable metadata</li> <li><code>actions</code> (list[str]): List of metadata actions</li> <li><code>referedmap</code> (dict[str, str | list | None]): Dictionary mapping references</li> <li><code>metaDef</code> (dict[str, MetadataDefJson]): Metadata definition loaded from file</li> </ul>"},{"location":"rdetoolkit/rde2util/#methods","title":"Methods","text":""},{"location":"rdetoolkit/rde2util/#assign_vals","title":"assign_vals","text":"<p>Register and validate metadata values according to the metadata definition.</p> <pre><code>def assign_vals(\n    self,\n    entry_dict_meta: MetaType | RepeatedMetaType,\n    *,\n    ignore_empty_strvalue: bool = True,\n) -&gt; dict[str, set]\n</code></pre> <p>Parameters: - <code>entry_dict_meta</code> (MetaType | RepeatedMetaType): Metadata key-value pairs to register - <code>ignore_empty_strvalue</code> (bool): Whether to ignore empty string values (default: True)</p> <p>Returns: - <code>dict[str, set]</code>: Dictionary with 'assigned' and 'unknown' keys containing sets of processed keys</p> <p>Raises: - <code>StructuredError</code>: If 'action' is included in the metadata definition</p> <p>Functionality: - Validates and casts input metadata values according to metadata-def.json specifications - Handles both constant and variable metadata types - Processes unit references and action definitions - Excludes None values from assignment</p>"},{"location":"rdetoolkit/rde2util/#writefile","title":"writefile","text":"<p>Write processed metadata to a file after handling units and actions.</p> <pre><code>def writefile(self, meta_filepath: str, enc: str = \"utf_8\") -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>meta_filepath</code> (str): Output file path for metadata - <code>enc</code> (str): File encoding (default: \"utf_8\")</p> <p>Returns: - <code>dict[str, Any]</code>: Dictionary with 'assigned' and 'unknown' keys showing processing results</p> <p>Functionality: - Processes units and actions for each metadata entry - Sorts items according to metadata definition order - Outputs structured JSON with constant and variable sections - Returns summary of assigned vs. unassigned metadata keys</p>"},{"location":"rdetoolkit/rde2util/#metadata_validation","title":"metadata_validation","text":"<p>Cast and validate metadata values according to specified formats.</p> <pre><code>def metadata_validation(\n    self,\n    vsrc: str,\n    outtype: str | None,\n    outfmt: str | None,\n    orgtype: str | None,\n    outunit: str | None,\n) -&gt; dict[str, bool | int | float | str]\n</code></pre> <p>Parameters: - <code>vsrc</code> (str): Input metadata value - <code>outtype</code> (str | None): Target data type - <code>outfmt</code> (str | None): Target format (for dates) - <code>orgtype</code> (str | None): Original data type - <code>outunit</code> (str | None): Unit specification</p> <p>Returns: - <code>dict[str, bool | int | float | str]</code>: Validated metadata with value and optional unit</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import Meta\nfrom pathlib import Path\nimport json\n\n# Create metadata processor\nmeta = Meta(\"data/tasksupport/metadata-def.json\")\n\n# Prepare metadata to register\nconst_metadata = {\n    \"title\": \"Sample Dataset\",\n    \"description\": \"A comprehensive research dataset\",\n    \"creator\": \"Research Team\",\n    \"created\": \"2024-01-01T00:00:00Z\",\n    \"keywords\": [\"research\", \"data\", \"analysis\"]\n}\n\nvariable_metadata = {\n    \"temperature\": [\"25.5\", \"26.1\", \"24.8\"],\n    \"pressure\": [\"1013.25\", \"1012.8\", \"1014.1\"],\n    \"humidity\": [\"65\", \"67\", \"63\"]\n}\n\n# Register metadata\nconst_result = meta.assign_vals(const_metadata)\nvar_result = meta.assign_vals(variable_metadata)\n\nprint(f\"Constant metadata assigned: {const_result['assigned']}\")\nprint(f\"Variable metadata assigned: {var_result['assigned']}\")\n\n# Write metadata to file\noutput_result = meta.writefile(\"data/meta/metadata.json\")\nprint(f\"Total assigned keys: {len(output_result['assigned'])}\")\nprint(f\"Unassigned keys: {output_result['unknown']}\")\n\n# Example with unit handling\nmeasurement_data = {\n    \"temperature\": \"25.5\u00b0C\",\n    \"pressure\": \"1013.25hPa\",\n    \"distance\": \"150.5m\"\n}\n\nmeasurement_result = meta.assign_vals(measurement_data)\nmeta.writefile(\"data/meta/measurements.json\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#storagedir","title":"StorageDir","text":"<p>A class for handling storage directory operations with support for indexed data organization.</p> <p>Note: This class is deprecated. Use <code>rdetoolkit.core.DirectoryOps</code> instead.</p>"},{"location":"rdetoolkit/rde2util/#class-attributes","title":"Class Attributes","text":"<ul> <li><code>__nDigit</code> (int): Number of digits for divided data index (fixed value: 4)</li> </ul>"},{"location":"rdetoolkit/rde2util/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/rde2util/#get_datadir","title":"get_datadir","text":"<p>Generate a data directory path based on index and optionally create it.</p> <pre><code>@classmethod\ndef get_datadir(cls, is_mkdir: bool, idx: int = 0) -&gt; str\n</code></pre> <p>Parameters: - <code>is_mkdir</code> (bool): Whether to create the directory - <code>idx</code> (int): Index for divided data (0 for base directory)</p> <p>Returns: - <code>str</code>: Path of the generated data directory</p> <p>Deprecation Warning: Use <code>rdetoolkit.core.DirectoryOps</code> instead.</p>"},{"location":"rdetoolkit/rde2util/#get_specific_outputdir","title":"get_specific_outputdir","text":"<p>Generate and optionally create specific output directories.</p> <pre><code>@classmethod\ndef get_specific_outputdir(cls, is_mkdir: bool, dir_basename: str, idx: int = 0) -&gt; pathlib.Path\n</code></pre> <p>Parameters: - <code>is_mkdir</code> (bool): Whether to create the directory - <code>dir_basename</code> (str): Base name of the specific output directory - <code>idx</code> (int): Index for divided data</p> <p>Returns: - <code>pathlib.Path</code>: Path of the specific output directory</p> <p>Supported Directory Types: - invoice, invoice_patch, inputdata, structured, temp, logs - meta, thumbnail, main_image, other_image, attachment - nonshared_raw, raw, tasksupport</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import StorageDir\nimport warnings\n\n# Suppress deprecation warnings for demonstration\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Create base data directory\nbase_dir = StorageDir.get_datadir(is_mkdir=True, idx=0)\nprint(f\"Base directory: {base_dir}\")  # \"data\"\n\n# Create indexed data directory\nindexed_dir = StorageDir.get_datadir(is_mkdir=True, idx=1)\nprint(f\"Indexed directory: {indexed_dir}\")  # \"data/divided/0001\"\n\n# Create specific output directories\ninvoice_dir = StorageDir.get_specific_outputdir(True, \"invoice\", idx=0)\nlogs_dir = StorageDir.get_specific_outputdir(True, \"logs\", idx=1)\nmeta_dir = StorageDir.get_specific_outputdir(True, \"meta\", idx=2)\n\nprint(f\"Invoice directory: {invoice_dir}\")\nprint(f\"Logs directory: {logs_dir}\")\nprint(f\"Meta directory: {meta_dir}\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#chardecencoding","title":"CharDecEncoding","text":"<p>A utility class for character encoding detection and conversion, with special support for Japanese text files.</p>"},{"location":"rdetoolkit/rde2util/#class-attributes_1","title":"Class Attributes","text":"<ul> <li><code>USUAL_ENCs</code> (tuple): Common encodings (\"ascii\", \"shift_jis\", \"utf_8\", \"utf_8_sig\", \"euc_jp\")</li> </ul>"},{"location":"rdetoolkit/rde2util/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/rde2util/#detect_text_file_encoding","title":"detect_text_file_encoding","text":"<p>Detect the encoding of a text file with enhanced Japanese support.</p> <pre><code>@classmethod\ndef detect_text_file_encoding(cls, text_filepath: RdeFsPath) -&gt; str\n</code></pre> <p>Parameters: - <code>text_filepath</code> (RdeFsPath): Path to the text file to analyze</p> <p>Returns: - <code>str</code>: Detected encoding of the text file</p> <p>Raises: - <code>FileNotFoundError</code>: If the file path does not exist</p> <p>Features: - Uses charset_normalizer for initial detection - Falls back to chardet for thorough analysis - Handles Japanese encodings (Shift JIS \u2192 cp932 conversion) - Normalizes encoding names for consistency</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import CharDecEncoding\nfrom pathlib import Path\n\n# Detect encoding of various text files\nfiles_to_check = [\n    \"data/input/japanese_text.txt\",\n    \"data/input/english_text.txt\",\n    \"data/input/mixed_encoding.csv\"\n]\n\nfor file_path in files_to_check:\n    if Path(file_path).exists():\n        try:\n            encoding = CharDecEncoding.detect_text_file_encoding(file_path)\n            print(f\"{file_path}: {encoding}\")\n\n            # Read file with detected encoding\n            with open(file_path, 'r', encoding=encoding) as f:\n                content = f.read()\n                print(f\"  Successfully read {len(content)} characters\")\n\n        except FileNotFoundError:\n            print(f\"{file_path}: File not found\")\n        except Exception as e:\n            print(f\"{file_path}: Error - {e}\")\n\n# Batch encoding detection\ndef detect_encodings_in_directory(directory_path: Path) -&gt; dict[str, str]:\n    \"\"\"Detect encodings for all text files in a directory.\"\"\"\n    results = {}\n\n    for file_path in directory_path.glob(\"*.txt\"):\n        try:\n            encoding = CharDecEncoding.detect_text_file_encoding(file_path)\n            results[str(file_path)] = encoding\n        except Exception as e:\n            results[str(file_path)] = f\"Error: {e}\"\n\n    return results\n\n# Usage\ninput_dir = Path(\"data/input\")\nif input_dir.exists():\n    encoding_results = detect_encodings_in_directory(input_dir)\n    for file_path, encoding in encoding_results.items():\n        print(f\"{file_path}: {encoding}\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#valuecaster","title":"ValueCaster","text":"<p>A utility class for value casting and date format conversion.</p>"},{"location":"rdetoolkit/rde2util/#methods_3","title":"Methods","text":""},{"location":"rdetoolkit/rde2util/#trycast","title":"trycast","text":"<p>Safely attempt to cast a value string to a specified type.</p> <pre><code>@staticmethod\ndef trycast(valstr: str, tp: Callable[[str], Any]) -&gt; Any\n</code></pre> <p>Parameters: - <code>valstr</code> (str): Value string to cast - <code>tp</code> (Callable[[str], Any]): Type function to cast to</p> <p>Returns: - <code>Any</code>: Casted value if successful, None otherwise</p>"},{"location":"rdetoolkit/rde2util/#convert_to_date_format","title":"convert_to_date_format","text":"<p>Convert a date string to a specified format.</p> <pre><code>@staticmethod\ndef convert_to_date_format(value: str, fmt: str) -&gt; str\n</code></pre> <p>Parameters: - <code>value</code> (str): Date string to convert - <code>fmt</code> (str): Target date format (\"date-time\", \"date\", \"time\")</p> <p>Returns: - <code>str</code>: Converted date string</p> <p>Raises: - <code>StructuredError</code>: If the format is unknown</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import ValueCaster\n\n# Test value casting\ntest_values = [\"123\", \"45.67\", \"true\", \"invalid\"]\ntest_types = [int, float, bool, str]\n\nfor value in test_values:\n    for cast_type in test_types:\n        result = ValueCaster.trycast(value, cast_type)\n        if result is not None:\n            print(f\"'{value}' \u2192 {cast_type.__name__}: {result}\")\n        else:\n            print(f\"'{value}' \u2192 {cast_type.__name__}: Failed\")\n\n# Date format conversion\ndate_strings = [\n    \"2024-01-15T14:30:00Z\",\n    \"2024-01-15 14:30:00\",\n    \"Jan 15, 2024 2:30 PM\"\n]\n\nformats = [\"date-time\", \"date\", \"time\"]\n\nfor date_str in date_strings:\n    print(f\"\\nOriginal: {date_str}\")\n    for fmt in formats:\n        try:\n            converted = ValueCaster.convert_to_date_format(date_str, fmt)\n            print(f\"  {fmt}: {converted}\")\n        except Exception as e:\n            print(f\"  {fmt}: Error - {e}\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#functions","title":"Functions","text":""},{"location":"rdetoolkit/rde2util/#get_default_values","title":"get_default_values","text":"<p>Read default values from a CSV file and return them as a dictionary.</p> <pre><code>def get_default_values(default_values_filepath: RdeFsPath) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>default_values_filepath</code> (RdeFsPath): Path to the CSV file containing default values</p> <p>Returns: - <code>dict[str, Any]</code>: Dictionary mapping keys to their corresponding default values</p> <p>CSV Format: - Must contain 'key' and 'value' columns - Encoding is automatically detected</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import get_default_values\nfrom pathlib import Path\nimport csv\n\n# Create sample default values CSV\ndefault_csv_path = Path(\"data/config/default_values.csv\")\ndefault_csv_path.parent.mkdir(parents=True, exist_ok=True)\n\n# Sample data\nsample_defaults = [\n    {\"key\": \"default_temperature_unit\", \"value\": \"\u00b0C\"},\n    {\"key\": \"default_pressure_unit\", \"value\": \"hPa\"},\n    {\"key\": \"default_author\", \"value\": \"Research Team\"},\n    {\"key\": \"default_language\", \"value\": \"en\"},\n    {\"key\": \"max_file_size\", \"value\": \"100MB\"}\n]\n\n# Write sample CSV\nwith open(default_csv_path, 'w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=['key', 'value'])\n    writer.writeheader()\n    writer.writerows(sample_defaults)\n\n# Read default values\ntry:\n    defaults = get_default_values(default_csv_path)\n    print(\"Default values loaded:\")\n    for key, value in defaults.items():\n        print(f\"  {key}: {value}\")\n\n    # Use defaults in processing\n    temperature_unit = defaults.get(\"default_temperature_unit\", \"K\")\n    author = defaults.get(\"default_author\", \"Unknown\")\n\n    print(f\"\\nUsing defaults:\")\n    print(f\"Temperature unit: {temperature_unit}\")\n    print(f\"Author: {author}\")\n\nexcept Exception as e:\n    print(f\"Error reading defaults: {e}\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#unzip_japanese_zip","title":"unzip_japanese_zip","text":"<p>Extract files from ZIP archives with Japanese filename encoding support.</p> <pre><code>def unzip_japanese_zip(src_zipfilepath: str, dst_dirpath: str) -&gt; None\n</code></pre> <p>Parameters: - <code>src_zipfilepath</code> (str): Path to the source ZIP file - <code>dst_dirpath</code> (str): Destination directory for extraction</p> <p>Returns: - <code>None</code></p> <p>Features: - Handles Japanese-specific encodings (Shift JIS) - Automatically decodes file names appropriately - Creates destination directory structure</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import unzip_japanese_zip\nfrom pathlib import Path\nimport zipfile\n\n# Create a sample ZIP file with Japanese filenames\ndef create_sample_japanese_zip():\n    \"\"\"Create a sample ZIP file for demonstration.\"\"\"\n    zip_path = Path(\"data/input/japanese_sample.zip\")\n    zip_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Create some sample files\n    sample_files = {\n        \"english_file.txt\": \"This is an English file.\",\n        \"\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb.txt\": \"\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002\",\n        \"\u6e2c\u5b9a\u7d50\u679c.csv\": \"time,temperature,humidity\\n2024-01-01,25.5,65\\n\"\n    }\n\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n        for filename, content in sample_files.items():\n            zf.writestr(filename, content.encode('utf-8'))\n\n    return zip_path\n\n# Extract ZIP with Japanese filename support\nzip_file = create_sample_japanese_zip()\nextract_dir = Path(\"data/extracted\")\n\ntry:\n    unzip_japanese_zip(str(zip_file), str(extract_dir))\n    print(f\"Successfully extracted to: {extract_dir}\")\n\n    # List extracted files\n    if extract_dir.exists():\n        print(\"Extracted files:\")\n        for file_path in extract_dir.rglob(\"*\"):\n            if file_path.is_file():\n                print(f\"  {file_path.name}\")\n                # Read and display content\n                try:\n                    content = file_path.read_text(encoding='utf-8')\n                    print(f\"    Content preview: {content[:50]}...\")\n                except Exception as e:\n                    print(f\"    Error reading content: {e}\")\n\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#castval","title":"castval","text":"<p>Format and cast string values based on specified type and format specifications.</p> <pre><code>def castval(valstr: Any, outtype: str | None, outfmt: str | None) -&gt; bool | int | float | str\n</code></pre> <p>Parameters: - <code>valstr</code> (Any): String to be converted - <code>outtype</code> (str | None): Target output type (\"boolean\", \"integer\", \"number\", \"string\") - <code>outfmt</code> (str | None): Output format (for date formatting)</p> <p>Returns: - <code>bool | int | float | str</code>: Converted value</p> <p>Raises: - <code>StructuredError</code>: If type is unknown or casting fails</p> <p>Supported Types: - boolean: Converts to boolean - integer: Converts to integer (handles unit-value pairs) - number: Converts to float (handles unit-value pairs) - string: Returns string, applies date formatting if specified</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import castval\nfrom rdetoolkit.exceptions import StructuredError\n\n# Test various value casting scenarios\ntest_cases = [\n    # Boolean casting\n    (\"true\", \"boolean\", None),\n    (\"false\", \"boolean\", None),\n    (\"1\", \"boolean\", None),\n\n    # Integer casting\n    (\"42\", \"integer\", None),\n    (\"42.5m\", \"integer\", None),  # With units\n    (\"-123\", \"integer\", None),\n\n    # Number casting\n    (\"3.14159\", \"number\", None),\n    (\"25.5\u00b0C\", \"number\", None),  # With units\n    (\"1.23e-4\", \"number\", None),\n\n    # String casting\n    (\"Hello World\", \"string\", None),\n    (\"2024-01-15T14:30:00\", \"string\", \"date-time\"),\n    (\"2024-01-15T14:30:00\", \"string\", \"date\"),\n    (\"2024-01-15T14:30:00\", \"string\", \"time\"),\n]\n\nprint(\"Value casting examples:\")\nfor valstr, outtype, outfmt in test_cases:\n    try:\n        result = castval(valstr, outtype, outfmt)\n        result_type = type(result).__name__\n        print(f\"'{valstr}' \u2192 {outtype}/{outfmt} \u2192 {result} ({result_type})\")\n    except StructuredError as e:\n        print(f\"'{valstr}' \u2192 {outtype}/{outfmt} \u2192 Error: {e}\")\n    except Exception as e:\n        print(f\"'{valstr}' \u2192 {outtype}/{outfmt} \u2192 Unexpected error: {e}\")\n\n# Practical usage in data processing\ndef process_measurement_data(raw_data: dict[str, str]) -&gt; dict[str, any]:\n    \"\"\"Process raw measurement data with type casting.\"\"\"\n\n    # Define expected types for each field\n    field_types = {\n        \"temperature\": (\"number\", None),\n        \"pressure\": (\"number\", None),\n        \"humidity\": (\"integer\", None),\n        \"is_valid\": (\"boolean\", None),\n        \"timestamp\": (\"string\", \"date-time\"),\n        \"location\": (\"string\", None)\n    }\n\n    processed_data = {}\n\n    for field, value in raw_data.items():\n        if field in field_types:\n            outtype, outfmt = field_types[field]\n            try:\n                processed_data[field] = castval(value, outtype, outfmt)\n            except Exception as e:\n                print(f\"Warning: Failed to cast {field}='{value}': {e}\")\n                processed_data[field] = value  # Keep original value\n        else:\n            processed_data[field] = value\n\n    return processed_data\n\n# Example usage\nraw_measurements = {\n    \"temperature\": \"25.5\u00b0C\",\n    \"pressure\": \"1013.25hPa\",\n    \"humidity\": \"65\",\n    \"is_valid\": \"true\",\n    \"timestamp\": \"2024-01-15T14:30:00Z\",\n    \"location\": \"Laboratory A\",\n    \"notes\": \"Clear weather\"\n}\n\nprocessed = process_measurement_data(raw_measurements)\nprint(\"\\nProcessed measurement data:\")\nfor field, value in processed.items():\n    print(f\"  {field}: {value} ({type(value).__name__})\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#dict2meta","title":"dict2meta","text":"<p>Convert dictionary data into structured metadata and write to a file.</p> <pre><code>def dict2meta(\n    metadef_filepath: pathlib.Path,\n    metaout_filepath: pathlib.Path,\n    const_info: MetaType,\n    val_info: MetaType\n) -&gt; dict[str, set[Any]]\n</code></pre> <p>Parameters: - <code>metadef_filepath</code> (pathlib.Path): Path to metadata definition file - <code>metaout_filepath</code> (pathlib.Path): Output path for processed metadata - <code>const_info</code> (MetaType): Dictionary with constant metadata - <code>val_info</code> (MetaType): Dictionary with variable metadata</p> <p>Returns: - <code>dict[str, set[Any]]</code>: Dictionary with 'assigned' and 'unknown' metadata fields</p> <p>Example:</p> <pre><code>from rdetoolkit.rde2util import dict2meta\nfrom pathlib import Path\nimport json\n\n# Setup file paths\nmetadef_path = Path(\"data/tasksupport/metadata-def.json\")\noutput_path = Path(\"data/meta/processed_metadata.json\")\n\n# Prepare constant metadata (applies to entire dataset)\nconstant_metadata = {\n    \"title\": \"Environmental Monitoring Dataset\",\n    \"description\": \"Temperature and humidity measurements from IoT sensors\",\n    \"creator\": \"Environmental Research Lab\",\n    \"created\": \"2024-01-15T00:00:00Z\",\n    \"keywords\": [\"environment\", \"IoT\", \"monitoring\", \"sensors\"],\n    \"license\": \"CC BY 4.0\",\n    \"language\": \"en\"\n}\n\n# Prepare variable metadata (varies by measurement)\nvariable_metadata = {\n    \"sensor_id\": [\"TEMP001\", \"TEMP002\", \"TEMP003\"],\n    \"location\": [\"Building A\", \"Building B\", \"Building C\"],\n    \"temperature\": [\"23.5\", \"24.1\", \"22.8\"],\n    \"humidity\": [\"65\", \"68\", \"62\"],\n    \"battery_level\": [\"85\", \"92\", \"78\"],\n    \"last_calibration\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-01\"]\n}\n\ntry:\n    # Convert dictionaries to structured metadata\n    result = dict2meta(\n        metadef_filepath=metadef_path,\n        metaout_filepath=output_path,\n        const_info=constant_metadata,\n        val_info=variable_metadata\n    )\n\n    print(\"Metadata processing completed successfully!\")\n    print(f\"Assigned fields: {result['assigned']}\")\n    print(f\"Unknown fields: {result['unknown']}\")\n\n    # Verify output file\n    if output_path.exists():\n        with open(output_path, 'r', encoding='utf-8') as f:\n            metadata = json.load(f)\n\n        print(f\"\\nGenerated metadata structure:\")\n        print(f\"  Constant fields: {list(metadata.get('constant', {}).keys())}\")\n        print(f\"  Variable entries: {len(metadata.get('variable', []))}\")\n\n        # Display sample variable entry\n        if metadata.get('variable'):\n            sample_entry = metadata['variable'][0]\n            print(f\"  Sample variable entry: {list(sample_entry.keys())}\")\n\nexcept Exception as e:\n    print(f\"Error processing metadata: {e}\")\n\n# Advanced usage with error handling and validation\ndef create_metadata_with_validation(\n    metadef_path: Path,\n    output_path: Path,\n    const_data: dict,\n    var_data: dict\n) -&gt; bool:\n    \"\"\"Create metadata with comprehensive validation.\"\"\"\n\n    try:\n        # Validate input data\n        if not metadef_path.exists():\n            raise FileNotFoundError(f\"Metadata definition not found: {metadef_path}\")\n\n        # Ensure output directory exists\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Process metadata\n        result = dict2meta(metadef_path, output_path, const_data, var_data)\n\n        # Check for unassigned fields\n        if result['unknown']:\n            print(f\"Warning: Some fields were not assigned: {result['unknown']}\")\n\n        # Validate output\n        if not output_path.exists():\n            raise RuntimeError(\"Output file was not created\")\n\n        print(f\"Metadata successfully created: {output_path}\")\n        return True\n\n    except Exception as e:\n        print(f\"Metadata creation failed: {e}\")\n        return False\n\n# Usage with validation\nsuccess = create_metadata_with_validation(\n    metadef_path, output_path, constant_metadata, variable_metadata\n)\n</code></pre>"},{"location":"rdetoolkit/rde2util/#deprecated-functions","title":"Deprecated Functions","text":""},{"location":"rdetoolkit/rde2util/#read_from_json_file","title":"read_from_json_file","text":"<p>Deprecated: Use <code>rdetoolkit.fileops.readf_json</code> instead.</p> <pre><code>def read_from_json_file(invoice_file_path: RdeFsPath) -&gt; dict[str, Any]\n</code></pre>"},{"location":"rdetoolkit/rde2util/#write_to_json_file","title":"write_to_json_file","text":"<p>Deprecated: Use <code>rdetoolkit.fileops.writef_json</code> instead.</p> <pre><code>def write_to_json_file(invoicefile_path: RdeFsPath, invoiceobj: dict[str, Any], enc: str = \"utf_8\") -&gt; None\n</code></pre>"},{"location":"rdetoolkit/rde2util/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/rde2util/#comprehensive-metadata-processing-pipeline","title":"Comprehensive Metadata Processing Pipeline","text":"<pre><code>from rdetoolkit.rde2util import Meta, get_default_values, CharDecEncoding, dict2meta\nfrom pathlib import Path\nimport json\nimport csv\nfrom typing import Dict, List, Any\n\nclass MetadataProcessor:\n    \"\"\"Comprehensive metadata processing pipeline.\"\"\"\n\n    def __init__(self, config_dir: Path, output_dir: Path):\n        self.config_dir = config_dir\n        self.output_dir = output_dir\n        self.defaults = {}\n        self.encoding_cache = {}\n\n        # Load default values\n        defaults_file = config_dir / \"default_values.csv\"\n        if defaults_file.exists():\n            self.defaults = get_default_values(defaults_file)\n\n    def detect_file_encodings(self, file_paths: List[Path]) -&gt; Dict[str, str]:\n        \"\"\"Detect encodings for multiple files with caching.\"\"\"\n\n        for file_path in file_paths:\n            if str(file_path) not in self.encoding_cache:\n                try:\n                    encoding = CharDecEncoding.detect_text_file_encoding(file_path)\n                    self.encoding_cache[str(file_path)] = encoding\n                    print(f\"Detected encoding for {file_path.name}: {encoding}\")\n                except Exception as e:\n                    print(f\"Encoding detection failed for {file_path.name}: {e}\")\n                    self.encoding_cache[str(file_path)] = \"utf-8\"  # Fallback\n\n        return {path: self.encoding_cache[str(Path(path))] for path in file_paths}\n\n    def process_csv_metadata(self, csv_path: Path) -&gt; Dict[str, List[str]]:\n        \"\"\"Process CSV file and extract metadata with proper encoding.\"\"\"\n\n        # Detect encoding\n        encoding = CharDecEncoding.detect_text_file_encoding(csv_path)\n\n        metadata = {}\n\n        try:\n            with open(csv_path, 'r', encoding=encoding) as f:\n                reader = csv.DictReader(f)\n\n                # Initialize metadata structure\n                for fieldname in reader.fieldnames:\n                    metadata[fieldname] = []\n\n                # Read data\n                for row in reader:\n                    for fieldname, value in row.items():\n                        metadata[fieldname].append(value or \"\")\n\n            print(f\"Processed CSV: {csv_path.name} ({len(metadata)} fields)\")\n            return metadata\n\n        except Exception as e:\n            print(f\"Error processing CSV {csv_path.name}: {e}\")\n            return {}\n\n    def create_comprehensive_metadata(\n        self,\n        metadef_path: Path,\n        csv_files: List[Path],\n        additional_const: Dict[str, Any] = None\n    ) -&gt; Path:\n        \"\"\"Create comprehensive metadata from multiple sources.\"\"\"\n\n        # Setup output path\n        output_path = self.output_dir / \"comprehensive_metadata.json\"\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Start with defaults\n        constant_metadata = self.defaults.copy()\n\n        # Add additional constants\n        if additional_const:\n            constant_metadata.update(additional_const)\n\n        # Process CSV files for variable metadata\n        variable_metadata = {}\n\n        for csv_file in csv_files:\n            if csv_file.exists():\n                csv_data = self.process_csv_metadata(csv_file)\n                variable_metadata.update(csv_data)\n\n        # Apply defaults to missing variable data\n        max_length = max(len(values) for values in variable_metadata.values()) if variable_metadata else 0\n\n        for key, default_value in self.defaults.items():\n            if key not in constant_metadata and key not in variable_metadata:\n                variable_metadata[key] = [default_value] * max_length\n\n        # Create metadata using dict2meta\n        try:\n            result = dict2meta(\n                metadef_filepath=metadef_path,\n                metaout_filepath=output_path,\n                const_info=constant_metadata,\n                val_info=variable_metadata\n            )\n\n            print(f\"Metadata creation completed:\")\n            print(f\"  Output: {output_path}\")\n            print(f\"  Assigned fields: {len(result['assigned'])}\")\n            print(f\"  Unknown fields: {len(result['unknown'])}\")\n\n            if result['unknown']:\n                print(f\"  Unknown fields: {result['unknown']}\")\n\n            return output_path\n\n        except Exception as e:\n            print(f\"Metadata creation failed: {e}\")\n            raise\n\n    def validate_metadata_output(self, metadata_path: Path) -&gt; bool:\n        \"\"\"Validate the generated metadata file.\"\"\"\n\n        if not metadata_path.exists():\n            print(\"Metadata file does not exist\")\n            return False\n\n        try:\n            with open(metadata_path, 'r', encoding='utf-8') as f:\n                metadata = json.load(f)\n\n            # Check structure\n            required_sections = ['constant', 'variable']\n            for section in required_sections:\n                if section not in metadata:\n                    print(f\"Missing section: {section}\")\n                    return False\n\n            # Validate constant metadata\n            const_count = len(metadata['constant'])\n            var_count = len(metadata['variable'])\n\n            print(f\"Validation results:\")\n            print(f\"  Constant metadata entries: {const_count}\")\n            print(f\"  Variable metadata entries: {var_count}\")\n\n            # Check for required fields\n            required_const_fields = ['title', 'description', 'creator']\n            missing_fields = [field for field in required_const_fields\n                            if field not in metadata['constant']]\n\n            if missing_fields:\n                print(f\"  Missing required constant fields: {missing_fields}\")\n                return False\n\n            print(\"  Metadata validation passed\")\n            return True\n\n        except json.JSONDecodeError as e:\n            print(f\"Invalid JSON format: {e}\")\n            return False\n        except Exception as e:\n            print(f\"Validation error: {e}\")\n            return False\n\n# Usage example\ndef main():\n    \"\"\"Main processing example.\"\"\"\n\n    # Setup paths\n    config_dir = Path(\"data/config\")\n    output_dir = Path(\"data/meta\")\n    metadef_path = Path(\"data/tasksupport/metadata-def.json\")\n\n    # Sample CSV files\n    csv_files = [\n        Path(\"data/measurements/temperature.csv\"),\n        Path(\"data/measurements/humidity.csv\"),\n        Path(\"data/measurements/pressure.csv\")\n    ]\n\n    # Create sample data for demonstration\n    config_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create default values CSV\n    defaults_csv = config_dir / \"default_values.csv\"\n    with open(defaults_csv, 'w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['key', 'value'])\n        writer.writerows([\n            ['default_temperature_unit', '\u00b0C'],\n            ['default_pressure_unit', 'hPa'],\n            ['default_humidity_unit', '%'],\n            ['creator', 'Automated Processing System'],\n            ['language', 'en']\n        ])\n\n    # Initialize processor\n    processor = MetadataProcessor(config_dir, output_dir)\n\n    # Additional constant metadata\n    additional_constants = {\n        \"title\": \"Multi-Sensor Environmental Data\",\n        \"description\": \"Comprehensive environmental monitoring dataset\",\n        \"created\": \"2024-01-15T00:00:00Z\",\n        \"keywords\": [\"environment\", \"sensors\", \"monitoring\"]\n    }\n\n    try:\n        # Process metadata\n        output_path = processor.create_comprehensive_metadata(\n            metadef_path=metadef_path,\n            csv_files=csv_files,\n            additional_const=additional_constants\n        )\n\n        # Validate output\n        if processor.validate_metadata_output(output_path):\n            print(\"\u2705 Metadata processing completed successfully\")\n        else:\n            print(\"\u274c Metadata validation failed\")\n\n    except Exception as e:\n        print(f\"\u274c Processing failed: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"rdetoolkit/rde2util/#advanced-value-processing-and-unit-handling","title":"Advanced Value Processing and Unit Handling","text":"<pre><code>from rdetoolkit.rde2util import ValueCaster, castval, Meta\nfrom rdetoolkit.exceptions import StructuredError\nimport re\nfrom typing import Dict, Any, Tuple, List\nfrom pathlib import Path\n\nclass AdvancedValueProcessor:\n    \"\"\"Advanced value processing with unit handling and validation.\"\"\"\n\n    def __init__(self):\n        self.unit_conversions = {\n            # Temperature\n            '\u00b0F': lambda x: (x - 32) * 5/9,  # Fahrenheit to Celsius\n            'K': lambda x: x - 273.15,       # Kelvin to Celsius\n\n            # Pressure\n            'psi': lambda x: x * 6894.76,    # PSI to Pa\n            'bar': lambda x: x * 100000,     # Bar to Pa\n            'atm': lambda x: x * 101325,     # Atmosphere to Pa\n\n            # Distance\n            'ft': lambda x: x * 0.3048,      # Feet to meters\n            'in': lambda x: x * 0.0254,      # Inches to meters\n            'km': lambda x: x * 1000,        # Kilometers to meters\n        }\n\n        self.target_units = {\n            'temperature': '\u00b0C',\n            'pressure': 'Pa',\n            'distance': 'm'\n        }\n\n    def parse_value_with_unit(self, value_str: str) -&gt; Tuple[float, str]:\n        \"\"\"Parse a string containing a value and unit.\"\"\"\n\n        # Regular expression to extract numeric value and unit\n        pattern = r'^([+-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+-]?\\d+)?)\\s*(.*)$'\n        match = re.match(pattern, value_str.strip())\n\n        if match:\n            value_part = match.group(1)\n            unit_part = match.group(2).strip()\n\n            try:\n                numeric_value = float(value_part)\n                return numeric_value, unit_part\n            except ValueError:\n                raise ValueError(f\"Cannot parse numeric value: {value_part}\")\n        else:\n            raise ValueError(f\"Cannot parse value-unit pair: {value_str}\")\n\n    def convert_unit(self, value: float, from_unit: str, measurement_type: str) -&gt; Tuple[float, str]:\n        \"\"\"Convert a value from one unit to the target unit for the measurement type.\"\"\"\n\n        target_unit = self.target_units.get(measurement_type)\n        if not target_unit:\n            return value, from_unit  # No conversion available\n\n        if from_unit == target_unit:\n            return value, target_unit  # Already in target unit\n\n        if from_unit in self.unit_conversions:\n            converted_value = self.unit_conversions[from_unit](value)\n            return converted_value, target_unit\n        else:\n            # Unknown unit, keep original\n            return value, from_unit\n\n    def process_measurement_values(\n        self,\n        measurements: Dict[str, List[str]],\n        measurement_types: Dict[str, str]\n    ) -&gt; Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Process measurement values with unit conversion and validation.\"\"\"\n\n        processed_data = {}\n\n        for measurement_name, values in measurements.items():\n            measurement_type = measurement_types.get(measurement_name, 'unknown')\n            processed_values = []\n\n            for value_str in values:\n                try:\n                    # Parse value and unit\n                    numeric_value, unit = self.parse_value_with_unit(value_str)\n\n                    # Convert unit if possible\n                    converted_value, target_unit = self.convert_unit(\n                        numeric_value, unit, measurement_type\n                    )\n\n                    # Create processed entry\n                    processed_entry = {\n                        'original_value': value_str,\n                        'numeric_value': converted_value,\n                        'unit': target_unit,\n                        'original_unit': unit,\n                        'converted': unit != target_unit\n                    }\n\n                    processed_values.append(processed_entry)\n\n                except Exception as e:\n                    # Handle parsing errors\n                    processed_entry = {\n                        'original_value': value_str,\n                        'error': str(e),\n                        'numeric_value': None,\n                        'unit': None\n                    }\n                    processed_values.append(processed_entry)\n\n            processed_data[measurement_name] = processed_values\n\n        return processed_data\n\n    def create_metadata_with_processing(\n        self,\n        metadef_path: Path,\n        output_path: Path,\n        raw_measurements: Dict[str, List[str]],\n        measurement_types: Dict[str, str],\n        constant_metadata: Dict[str, Any]\n    ) -&gt; bool:\n        \"\"\"Create metadata with advanced value processing.\"\"\"\n\n        try:\n            # Process measurements\n            processed_measurements = self.process_measurement_values(\n                raw_measurements, measurement_types\n            )\n\n            # Prepare variable metadata for Meta class\n            variable_metadata = {}\n\n            for measurement_name, processed_values in processed_measurements.items():\n                # Extract numeric values and units for metadata\n                numeric_values = []\n                units = []\n\n                for entry in processed_values:\n                    if entry.get('numeric_value') is not None:\n                        numeric_values.append(str(entry['numeric_value']))\n                        units.append(entry.get('unit', ''))\n                    else:\n                        numeric_values.append('')\n                        units.append('')\n\n                variable_metadata[measurement_name] = numeric_values\n                if any(units):  # Add unit information if available\n                    variable_metadata[f\"{measurement_name}_unit\"] = units\n\n            # Create metadata using Meta class\n            meta = Meta(metadef_path)\n\n            # Assign constant metadata\n            const_result = meta.assign_vals(constant_metadata)\n            print(f\"Assigned constant metadata: {const_result['assigned']}\")\n\n            # Assign variable metadata\n            var_result = meta.assign_vals(variable_metadata)\n            print(f\"Assigned variable metadata: {var_result['assigned']}\")\n\n            # Write metadata file\n            output_result = meta.writefile(str(output_path))\n            print(f\"Metadata written to: {output_path}\")\n            print(f\"Total assigned: {len(output_result['assigned'])}\")\n            print(f\"Unknown fields: {output_result['unknown']}\")\n\n            # Create processing report\n            report_path = output_path.parent / f\"{output_path.stem}_processing_report.json\"\n            processing_report = {\n                'processing_summary': {\n                    'total_measurements': len(raw_measurements),\n                    'processed_measurements': len(processed_measurements),\n                    'conversion_summary': {}\n                },\n                'detailed_processing': processed_measurements\n            }\n\n            # Add conversion summary\n            for measurement_name, processed_values in processed_measurements.items():\n                converted_count = sum(1 for entry in processed_values if entry.get('converted', False))\n                error_count = sum(1 for entry in processed_values if 'error' in entry)\n\n                processing_report['processing_summary']['conversion_summary'][measurement_name] = {\n                    'total_values': len(processed_values),\n                    'converted_values': converted_count,\n                    'error_values': error_count\n                }\n\n            with open(report_path, 'w', encoding='utf-8') as f:\n                import json\n                json.dump(processing_report, f, indent=2, ensure_ascii=False)\n\n            print(f\"Processing report saved to: {report_path}\")\n            return True\n\n        except Exception as e:\n            print(f\"Advanced processing failed: {e}\")\n            return False\n\n# Usage example\ndef demonstrate_advanced_processing():\n    \"\"\"Demonstrate advanced value processing capabilities.\"\"\"\n\n    # Setup processor\n    processor = AdvancedValueProcessor()\n\n    # Sample raw measurements with various units\n    raw_measurements = {\n        'temperature': ['25.5\u00b0C', '78.2\u00b0F', '298.15K', '22.1\u00b0C'],\n        'pressure': ['1013.25hPa', '14.7psi', '1.01bar', '101325Pa'],\n        'distance': ['150.5m', '5.2ft', '30.5in', '0.1km'],\n        'humidity': ['65%', '70%', '58%', '72%']\n    }\n\n    # Define measurement types for unit conversion\n    measurement_types = {\n        'temperature': 'temperature',\n        'pressure': 'pressure',\n        'distance': 'distance',\n        'humidity': 'humidity'  # No conversion defined\n    }\n\n    # Process measurements\n    processed = processor.process_measurement_values(raw_measurements, measurement_types)\n\n    # Display processing results\n    print(\"Advanced Processing Results:\")\n    print(\"=\" * 50)\n\n    for measurement, values in processed.items():\n        print(f\"\\n{measurement.upper()}:\")\n        for i, entry in enumerate(values):\n            print(f\"  {i+1}. {entry['original_value']}\")\n            if entry.get('error'):\n                print(f\"     Error: {entry['error']}\")\n            else:\n                print(f\"     \u2192 {entry['numeric_value']:.2f} {entry['unit']}\")\n                if entry.get('converted'):\n                    print(f\"     (converted from {entry['original_unit']})\")\n\n    # Create metadata with processing\n    metadef_path = Path(\"data/tasksupport/metadata-def.json\")\n    output_path = Path(\"data/meta/advanced_metadata.json\")\n\n    constant_metadata = {\n        'title': 'Advanced Processed Measurements',\n        'description': 'Measurements with unit conversion and validation',\n        'creator': 'Advanced Processing System',\n        'processing_method': 'unit_conversion_and_validation'\n    }\n\n    success = processor.create_metadata_with_processing(\n        metadef_path=metadef_path,\n        output_path=output_path,\n        raw_measurements=raw_measurements,\n        measurement_types=measurement_types,\n        constant_metadata=constant_metadata\n    )\n\n    if success:\n        print(\"\\n\u2705 Advanced processing completed successfully\")\n    else:\n        print(\"\\n\u274c Advanced processing failed\")\n\nif __name__ == \"__main__\":\n    demonstrate_advanced_processing()\n</code></pre>"},{"location":"rdetoolkit/rde2util/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/rde2util/#exception-types","title":"Exception Types","text":"<p>The rde2util module raises <code>StructuredError</code> for various error conditions:</p> <pre><code>from rdetoolkit.exceptions import StructuredError\nfrom rdetoolkit.rde2util import Meta, castval\n\n# Handle metadata processing errors\ntry:\n    meta = Meta(\"invalid_path.json\")\nexcept StructuredError as e:\n    print(f\"Metadata initialization failed: {e}\")\n\n# Handle value casting errors\ntry:\n    result = castval(\"invalid_number\", \"integer\", None)\nexcept StructuredError as e:\n    print(f\"Value casting failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/rde2util/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Graceful Degradation:    <pre><code>def safe_metadata_processing(metadef_path, output_path, data):\n    \"\"\"Process metadata with graceful error handling.\"\"\"\n    try:\n        meta = Meta(metadef_path)\n        result = meta.assign_vals(data)\n        meta.writefile(output_path)\n        return True, result\n    except StructuredError as e:\n        print(f\"Structured error: {e}\")\n        return False, None\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return False, None\n</code></pre></p> </li> <li> <p>Validation Before Processing:    <pre><code>def validate_before_processing(metadef_path, data):\n    \"\"\"Validate inputs before processing.\"\"\"\n    if not Path(metadef_path).exists():\n        raise FileNotFoundError(f\"Metadata definition not found: {metadef_path}\")\n\n    if not data:\n        raise ValueError(\"No data provided for processing\")\n\n    # Additional validation...\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/rde2util/#performance-notes","title":"Performance Notes","text":""},{"location":"rdetoolkit/rde2util/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Encoding Detection Caching: Cache encoding detection results for frequently accessed files</li> <li>Batch Processing: Process multiple files in batches to reduce I/O overhead</li> <li>Memory Management: Use generators for large datasets to reduce memory usage</li> <li>Unit Conversion Caching: Cache unit conversion functions for repeated operations</li> </ol>"},{"location":"rdetoolkit/rde2util/#performance-best-practices","title":"Performance Best Practices","text":"<pre><code># Efficient batch processing\ndef efficient_file_processing(file_paths: List[Path]) -&gt; Dict[str, str]:\n    \"\"\"Efficiently process multiple files with caching.\"\"\"\n    encoding_cache = {}\n\n    for file_path in file_paths:\n        if str(file_path) not in encoding_cache:\n            encoding = CharDecEncoding.detect_text_file_encoding(file_path)\n            encoding_cache[str(file_path)] = encoding\n\n    return encoding_cache\n</code></pre>"},{"location":"rdetoolkit/rde2util/#see-also","title":"See Also","text":"<ul> <li>Core Module - For directory operations and file handling</li> <li>File Operations - For JSON file reading and writing utilities</li> <li>Models - RDE2 Types - For type definitions used in this module</li> <li>Models - Metadata - For metadata structure definitions</li> <li>Validation - For metadata validation functionality</li> <li>Exceptions - For StructuredError and other exception types</li> <li>Usage - Structured Process - For metadata processing in workflows</li> <li>Usage - Metadata Definition - For metadata definition file usage</li> </ul>"},{"location":"rdetoolkit/rdelogger/","title":"RDE Logger Module","text":"<p>The <code>rdetoolkit.rdelogger</code> module provides specialized logging functionality for RDE (Research Data Exchange) structuring processes. This module offers lazy file handling, custom logging configurations, and decorators for comprehensive execution tracking.</p>"},{"location":"rdetoolkit/rdelogger/#overview","title":"Overview","text":"<p>The rdelogger module provides robust logging capabilities tailored for RDE workflows:</p> <ul> <li>Lazy File Handling: Efficient file handler that creates log files only when needed</li> <li>Custom Logger Configuration: Specialized logger setup for RDE processes</li> <li>Flexible Log Output: Support for both file and console logging</li> <li>Function Decorators: Automatic logging of function execution start, end, and errors</li> <li>Directory Management: Automatic creation of log directories</li> <li>Duplicate Prevention: Smart handler management to avoid duplicate log entries</li> </ul>"},{"location":"rdetoolkit/rdelogger/#classes","title":"Classes","text":""},{"location":"rdetoolkit/rdelogger/#lazyfilehandler","title":"LazyFileHandler","text":"<p>A logging handler that lazily creates the actual FileHandler when needed, preventing unnecessary file creation when logging is configured but not used.</p>"},{"location":"rdetoolkit/rdelogger/#constructor","title":"Constructor","text":"<pre><code>LazyFileHandler(filename: str, mode: str = \"a\", encoding: str = 'utf-8')\n</code></pre> <p>Parameters: - <code>filename</code> (str): The path to the log file - <code>mode</code> (str): The file opening mode (default: 'a' for append) - <code>encoding</code> (str): The encoding to use for the file (default: 'utf-8')</p>"},{"location":"rdetoolkit/rdelogger/#attributes","title":"Attributes","text":"<ul> <li><code>filename</code> (str): The path where the log file will be created</li> <li><code>mode</code> (str): The file opening mode</li> <li><code>encoding</code> (str): The file encoding</li> <li><code>_handler</code> (logging.FileHandler | None): The underlying FileHandler instance, created on first use</li> </ul>"},{"location":"rdetoolkit/rdelogger/#methods","title":"Methods","text":""},{"location":"rdetoolkit/rdelogger/#_ensure_handler","title":"_ensure_handler","text":"<p>Create the actual FileHandler if it hasn't been created yet.</p> <pre><code>def _ensure_handler(self) -&gt; None\n</code></pre> <p>Returns: - <code>None</code></p> <p>Functionality: - Creates necessary directories if they don't exist - Initializes the FileHandler with specified filename, mode, and encoding - Configures the handler with formatter and level settings from the parent handler</p>"},{"location":"rdetoolkit/rdelogger/#emit","title":"emit","text":"<p>Lazily create the actual FileHandler and delegate the emission of the log record.</p> <pre><code>def emit(self, record: logging.LogRecord) -&gt; None\n</code></pre> <p>Parameters: - <code>record</code> (logging.LogRecord): The LogRecord instance containing all logging event information</p> <p>Returns: - <code>None</code></p> <p>Example:</p> <pre><code>from rdetoolkit.rdelogger import LazyFileHandler\nimport logging\n\n# Create a lazy file handler\nhandler = LazyFileHandler(\"data/logs/application.log\", mode=\"a\", encoding=\"utf-8\")\n\n# Setup logger with lazy handler\nlogger = logging.getLogger(\"example\")\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(handler)\n\n# Set formatter\nformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nhandler.setFormatter(formatter)\n\n# The log file won't be created until the first log message\nlogger.info(\"This message will create the log file\")  # File created here\nlogger.debug(\"Subsequent messages use the existing handler\")\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#customlog","title":"CustomLog","text":"<p>A specialized class for creating custom loggers with user-defined log files and flexible output options.</p>"},{"location":"rdetoolkit/rdelogger/#constructor_1","title":"Constructor","text":"<pre><code>CustomLog(name: str = \"rdeuser\")\n</code></pre> <p>Parameters: - <code>name</code> (str): Logger name (default: \"rdeuser\")</p>"},{"location":"rdetoolkit/rdelogger/#attributes_1","title":"Attributes","text":"<ul> <li><code>logger</code> (logging.Logger): The underlying logger instance</li> </ul>"},{"location":"rdetoolkit/rdelogger/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/rdelogger/#get_logger","title":"get_logger","text":"<p>Retrieve the configured logger instance with optional log output control.</p> <pre><code>def get_logger(self, needlogs: bool = True) -&gt; logging.Logger\n</code></pre> <p>Parameters: - <code>needlogs</code> (bool): Whether logs should be written (default: True)</p> <p>Returns: - <code>logging.Logger</code>: The configured logger instance</p> <p>Functionality: - Creates log directory (<code>data/logs</code>) if it doesn't exist - Sets up both console and file handlers when <code>needlogs=True</code> - Uses NullHandler when <code>needlogs=False</code> to suppress output - Prevents duplicate handler registration</p>"},{"location":"rdetoolkit/rdelogger/#_set_handler","title":"_set_handler","text":"<p>Internal method to configure and add handlers to the logger.</p> <pre><code>def _set_handler(self, handler: logging.Handler, verbose: bool) -&gt; None\n</code></pre> <p>Parameters: - <code>handler</code> (logging.Handler): The handler to configure and add - <code>verbose</code> (bool): Whether to use verbose logging (DEBUG level vs INFO level)</p> <p>Returns: - <code>None</code></p> <p>Example:</p> <pre><code>from rdetoolkit.rdelogger import CustomLog\n\n# Create custom logger with logging enabled\ncustom_logger = CustomLog(\"my_module\").get_logger(needlogs=True)\ncustom_logger.info(\"This will be logged to both console and file\")\ncustom_logger.debug(\"Debug message will also be logged\")\n\n# Create custom logger with logging disabled\nsilent_logger = CustomLog(\"silent_module\").get_logger(needlogs=False)\nsilent_logger.info(\"This message will be suppressed\")\n\n# Custom logger for specific module\nclass DataProcessor:\n    def __init__(self):\n        self.logger = CustomLog(f\"{__name__}.DataProcessor\").get_logger()\n\n    def process_data(self, data):\n        self.logger.info(\"Starting data processing\")\n        try:\n            # Process data\n            result = self._transform_data(data)\n            self.logger.info(f\"Successfully processed {len(result)} items\")\n            return result\n        except Exception as e:\n            self.logger.error(f\"Data processing failed: {e}\")\n            raise\n\n    def _transform_data(self, data):\n        # Data transformation logic\n        return [item.upper() for item in data if isinstance(item, str)]\n\n# Usage\nprocessor = DataProcessor()\nresult = processor.process_data([\"hello\", \"world\", 123, \"test\"])\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#functions","title":"Functions","text":""},{"location":"rdetoolkit/rdelogger/#get_logger_1","title":"get_logger","text":"<p>Create and configure a logger using Python's built-in logging module with RDE-specific optimizations.</p> <pre><code>def get_logger(name: str, *, file_path: RdeFsPath | None = None, level: int = logging.DEBUG) -&gt; logging.Logger\n</code></pre> <p>Parameters: - <code>name</code> (str): The name of the logger (typically the module name, e.g., <code>__name__</code>) - <code>file_path</code> (RdeFsPath | None): The file path where log messages will be written (optional) - <code>level</code> (int): The logging level (default: <code>logging.DEBUG</code>)</p> <p>Returns: - <code>logging.Logger</code>: A configured logger instance</p> <p>Features: - Lazy file handler creation using <code>LazyFileHandler</code> - Automatic directory creation for log files - Duplicate handler prevention - Standardized log formatting - Flexible log level configuration</p> <p>Example:</p> <pre><code>from rdetoolkit.rdelogger import get_logger\nimport logging\n\n# Create a logger with default DEBUG level\nlogger = get_logger(__name__, file_path=\"data/logs/rdesys.log\")\nlogger.debug('This is a debug message.')\nlogger.info('This is an info message.')\nlogger.warning('This is a warning message.')\nlogger.error('This is an error message.')\n\n# Create a logger with custom logging level (INFO)\nlogger_info = get_logger(__name__, file_path=\"data/logs/rdesys.log\", level=logging.INFO)\nlogger_info.debug('This debug message will not appear')  # Below INFO level\nlogger_info.info('This info message will appear')\n\n# Create a console-only logger (no file output)\nconsole_logger = get_logger(__name__, file_path=None, level=logging.WARNING)\nconsole_logger.warning('This will only appear on console')\n\n# Module-specific logger\nclass WorkflowManager:\n    def __init__(self):\n        self.logger = get_logger(f\"{__name__}.WorkflowManager\",\n                                file_path=\"data/logs/workflow.log\")\n\n    def execute_workflow(self, workflow_config):\n        self.logger.info(f\"Starting workflow: {workflow_config.get('name', 'unnamed')}\")\n\n        try:\n            # Execute workflow steps\n            for step in workflow_config.get('steps', []):\n                self.logger.debug(f\"Executing step: {step}\")\n                self._execute_step(step)\n\n            self.logger.info(\"Workflow completed successfully\")\n            return True\n\n        except Exception as e:\n            self.logger.error(f\"Workflow failed: {e}\")\n            return False\n\n    def _execute_step(self, step):\n        # Step execution logic\n        pass\n\n# Usage\nmanager = WorkflowManager()\nconfig = {\"name\": \"data_processing\", \"steps\": [\"validate\", \"transform\", \"save\"]}\nsuccess = manager.execute_workflow(config)\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#log_decorator","title":"log_decorator","text":"<p>A decorator function that automatically logs the start and end of decorated functions, including error handling.</p> <pre><code>def log_decorator() -&gt; Callable\n</code></pre> <p>Returns: - <code>Callable</code>: The decorator function</p> <p>Functionality: - Logs function start with function name - Logs function completion - Logs errors if exceptions occur - Re-raises exceptions after logging - Uses CustomLog for consistent formatting</p> <p>Example:</p> <pre><code>from rdetoolkit.rdelogger import log_decorator\n\n@log_decorator()\ndef process_dataset(dataset_path: str) -&gt; dict:\n    \"\"\"Process a dataset and return statistics.\"\"\"\n    print(f\"Processing dataset: {dataset_path}\")\n\n    # Simulate processing\n    import time\n    time.sleep(1)\n\n    # Return results\n    return {\"processed\": True, \"records\": 1000}\n\n@log_decorator()\ndef risky_operation() -&gt; str:\n    \"\"\"Function that might raise an exception.\"\"\"\n    import random\n    if random.random() &lt; 0.5:\n        raise ValueError(\"Random error occurred\")\n    return \"Success\"\n\n# Usage examples\ntry:\n    result = process_dataset(\"/data/sample.csv\")\n    print(f\"Processing result: {result}\")\nexcept Exception as e:\n    print(f\"Processing failed: {e}\")\n\n# Example with error handling\nfor i in range(3):\n    try:\n        result = risky_operation()\n        print(f\"Attempt {i+1}: {result}\")\n        break\n    except ValueError as e:\n        print(f\"Attempt {i+1} failed: {e}\")\n\n# Advanced usage with class methods\nclass DataAnalyzer:\n    def __init__(self, name: str):\n        self.name = name\n\n    @log_decorator()\n    def analyze_data(self, data: list) -&gt; dict:\n        \"\"\"Analyze data and return statistics.\"\"\"\n        if not data:\n            raise ValueError(\"Empty data provided\")\n\n        return {\n            \"count\": len(data),\n            \"mean\": sum(data) / len(data),\n            \"min\": min(data),\n            \"max\": max(data)\n        }\n\n    @log_decorator()\n    def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a formatted report.\"\"\"\n        return f\"Analysis Report for {self.name}:\\n\" + \\\n               f\"Count: {analysis['count']}\\n\" + \\\n               f\"Mean: {analysis['mean']:.2f}\\n\" + \\\n               f\"Range: {analysis['min']} - {analysis['max']}\"\n\n# Usage\nanalyzer = DataAnalyzer(\"Sample Dataset\")\ntry:\n    data = [1, 2, 3, 4, 5, 10, 15, 20]\n    analysis = analyzer.analyze_data(data)\n    report = analyzer.generate_report(analysis)\n    print(report)\nexcept Exception as e:\n    print(f\"Analysis failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/rdelogger/#comprehensive-logging-setup","title":"Comprehensive Logging Setup","text":"<pre><code>from rdetoolkit.rdelogger import get_logger, CustomLog, log_decorator\nfrom pathlib import Path\nimport logging\nfrom typing import Dict, List, Any\n\nclass RDEProcessingPipeline:\n    \"\"\"Example pipeline with comprehensive logging setup.\"\"\"\n\n    def __init__(self, pipeline_name: str, log_level: int = logging.INFO):\n        self.pipeline_name = pipeline_name\n\n        # Setup multiple loggers for different purposes\n        self.main_logger = get_logger(\n            f\"{__name__}.{self.__class__.__name__}\",\n            file_path=f\"data/logs/{pipeline_name}_main.log\",\n            level=log_level\n        )\n\n        self.error_logger = get_logger(\n            f\"{__name__}.{self.__class__.__name__}.errors\",\n            file_path=f\"data/logs/{pipeline_name}_errors.log\",\n            level=logging.ERROR\n        )\n\n        self.debug_logger = get_logger(\n            f\"{__name__}.{self.__class__.__name__}.debug\",\n            file_path=f\"data/logs/{pipeline_name}_debug.log\",\n            level=logging.DEBUG\n        )\n\n        # Custom user logger for operational messages\n        self.user_logger = CustomLog(f\"{pipeline_name}_user\").get_logger()\n\n    @log_decorator()\n    def initialize_pipeline(self) -&gt; bool:\n        \"\"\"Initialize the processing pipeline.\"\"\"\n        self.main_logger.info(f\"Initializing pipeline: {self.pipeline_name}\")\n\n        try:\n            # Initialize components\n            self._setup_directories()\n            self._validate_configuration()\n\n            self.main_logger.info(\"Pipeline initialization completed successfully\")\n            self.user_logger.info(f\"Pipeline '{self.pipeline_name}' is ready for processing\")\n            return True\n\n        except Exception as e:\n            self.error_logger.error(f\"Pipeline initialization failed: {e}\")\n            self.user_logger.error(f\"Failed to initialize pipeline '{self.pipeline_name}': {e}\")\n            return False\n\n    def _setup_directories(self):\n        \"\"\"Setup required directories.\"\"\"\n        directories = [\"data/input\", \"data/output\", \"data/temp\", \"data/logs\"]\n        for directory in directories:\n            Path(directory).mkdir(parents=True, exist_ok=True)\n            self.debug_logger.debug(f\"Created/verified directory: {directory}\")\n\n    def _validate_configuration(self):\n        \"\"\"Validate pipeline configuration.\"\"\"\n        # Configuration validation logic\n        self.debug_logger.debug(\"Configuration validation completed\")\n\n    @log_decorator()\n    def process_batch(self, input_files: List[str]) -&gt; Dict[str, Any]:\n        \"\"\"Process a batch of input files.\"\"\"\n        self.main_logger.info(f\"Starting batch processing of {len(input_files)} files\")\n\n        results = {\n            \"processed\": 0,\n            \"failed\": 0,\n            \"details\": []\n        }\n\n        for file_path in input_files:\n            try:\n                self.debug_logger.debug(f\"Processing file: {file_path}\")\n                result = self._process_single_file(file_path)\n\n                results[\"processed\"] += 1\n                results[\"details\"].append({\n                    \"file\": file_path,\n                    \"status\": \"success\",\n                    \"result\": result\n                })\n\n                self.main_logger.info(f\"Successfully processed: {file_path}\")\n\n            except Exception as e:\n                results[\"failed\"] += 1\n                results[\"details\"].append({\n                    \"file\": file_path,\n                    \"status\": \"failed\",\n                    \"error\": str(e)\n                })\n\n                self.error_logger.error(f\"Failed to process {file_path}: {e}\")\n                self.main_logger.warning(f\"Skipping failed file: {file_path}\")\n\n        # Summary logging\n        self.main_logger.info(f\"Batch processing completed: {results['processed']} successful, {results['failed']} failed\")\n        self.user_logger.info(f\"Batch processing summary: {results['processed']}/{len(input_files)} files processed successfully\")\n\n        return results\n\n    def _process_single_file(self, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Process a single file.\"\"\"\n        # File processing logic\n        import time\n        time.sleep(0.1)  # Simulate processing time\n\n        return {\n            \"processed_at\": \"2024-01-01T00:00:00Z\",\n            \"size\": 1024,\n            \"records\": 100\n        }\n\n    def get_processing_statistics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get processing statistics from log files.\"\"\"\n        self.debug_logger.debug(\"Collecting processing statistics\")\n\n        stats = {\n            \"pipeline_name\": self.pipeline_name,\n            \"log_files\": [],\n            \"total_size\": 0\n        }\n\n        # Collect log file information\n        log_dir = Path(\"data/logs\")\n        for log_file in log_dir.glob(f\"{self.pipeline_name}_*.log\"):\n            if log_file.exists():\n                size = log_file.stat().st_size\n                stats[\"log_files\"].append({\n                    \"name\": log_file.name,\n                    \"size\": size,\n                    \"path\": str(log_file)\n                })\n                stats[\"total_size\"] += size\n\n        self.main_logger.info(f\"Statistics collected: {len(stats['log_files'])} log files, total size: {stats['total_size']} bytes\")\n        return stats\n\n# Usage example\ndef main():\n    \"\"\"Example usage of comprehensive logging setup.\"\"\"\n\n    # Create pipeline with INFO level logging\n    pipeline = RDEProcessingPipeline(\"sample_processing\", logging.INFO)\n\n    # Initialize pipeline\n    if not pipeline.initialize_pipeline():\n        print(\"Failed to initialize pipeline\")\n        return False\n\n    # Process sample files\n    sample_files = [\n        \"data/input/file1.csv\",\n        \"data/input/file2.json\",\n        \"data/input/file3.txt\"\n    ]\n\n    # Execute batch processing\n    results = pipeline.process_batch(sample_files)\n\n    # Get and display statistics\n    stats = pipeline.get_processing_statistics()\n    print(f\"Processing completed. Statistics: {stats}\")\n\n    return results[\"failed\"] == 0\n\nif __name__ == \"__main__\":\n    success = main()\n    print(f\"Pipeline execution {'succeeded' if success else 'failed'}\")\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#advanced-logging-with-context-management","title":"Advanced Logging with Context Management","text":"<pre><code>from rdetoolkit.rdelogger import get_logger, CustomLog\nimport logging\nimport contextlib\nfrom typing import Generator, Any\nimport time\nimport json\n\nclass LoggingContext:\n    \"\"\"Context manager for structured logging with automatic cleanup.\"\"\"\n\n    def __init__(self, operation_name: str, log_level: int = logging.INFO):\n        self.operation_name = operation_name\n        self.log_level = log_level\n        self.start_time = None\n        self.logger = None\n        self.context_data = {}\n\n    def __enter__(self):\n        self.start_time = time.time()\n        self.logger = get_logger(\n            f\"context.{self.operation_name}\",\n            file_path=f\"data/logs/context_{self.operation_name}.log\",\n            level=self.log_level\n        )\n\n        self.logger.info(f\"=== Starting operation: {self.operation_name} ===\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        duration = time.time() - self.start_time\n\n        if exc_type is None:\n            self.logger.info(f\"=== Operation completed successfully: {self.operation_name} (Duration: {duration:.2f}s) ===\")\n        else:\n            self.logger.error(f\"=== Operation failed: {self.operation_name} (Duration: {duration:.2f}s) ===\")\n            self.logger.error(f\"Exception: {exc_type.__name__}: {exc_val}\")\n\n        # Log context data if available\n        if self.context_data:\n            self.logger.info(f\"Context data: {json.dumps(self.context_data, default=str)}\")\n\n    def add_context(self, key: str, value: Any):\n        \"\"\"Add context information to be logged.\"\"\"\n        self.context_data[key] = value\n        self.logger.debug(f\"Added context: {key} = {value}\")\n\n    def log_progress(self, message: str, **kwargs):\n        \"\"\"Log progress information with optional context.\"\"\"\n        if kwargs:\n            self.context_data.update(kwargs)\n        self.logger.info(f\"Progress: {message}\")\n\n@contextlib.contextmanager\ndef logging_operation(operation_name: str, log_level: int = logging.INFO) -&gt; Generator[LoggingContext, None, None]:\n    \"\"\"Context manager factory for logging operations.\"\"\"\n    context = LoggingContext(operation_name, log_level)\n    with context:\n        yield context\n\n# Example usage with context managers\ndef process_data_with_context():\n    \"\"\"Example of using logging context managers.\"\"\"\n\n    with logging_operation(\"data_validation\", logging.DEBUG) as ctx:\n        ctx.add_context(\"input_files\", 5)\n        ctx.add_context(\"validation_rules\", [\"format\", \"schema\", \"completeness\"])\n\n        # Simulate validation process\n        for i in range(5):\n            ctx.log_progress(f\"Validating file {i+1}/5\", current_file=f\"file_{i+1}.csv\")\n            time.sleep(0.1)\n\n        ctx.add_context(\"validation_result\", \"all_passed\")\n\n    with logging_operation(\"data_transformation\") as ctx:\n        ctx.add_context(\"transformation_type\", \"normalize_and_aggregate\")\n\n        # Simulate transformation\n        ctx.log_progress(\"Starting data normalization\")\n        time.sleep(0.2)\n\n        ctx.log_progress(\"Performing aggregation\")\n        time.sleep(0.2)\n\n        ctx.add_context(\"output_records\", 1500)\n\n    # Example with error handling\n    with logging_operation(\"error_prone_operation\") as ctx:\n        ctx.add_context(\"operation_type\", \"risky_calculation\")\n\n        try:\n            # Simulate an operation that might fail\n            import random\n            if random.random() &lt; 0.3:\n                raise ValueError(\"Simulated calculation error\")\n\n            ctx.log_progress(\"Calculation completed successfully\")\n            ctx.add_context(\"calculation_result\", 42.7)\n\n        except ValueError as e:\n            ctx.add_context(\"error_details\", str(e))\n            # Error will be automatically logged by context manager\n            raise\n\n# Function-level logging with multiple loggers\nclass MultiLoggerProcessor:\n    \"\"\"Processor with multiple specialized loggers.\"\"\"\n\n    def __init__(self):\n        # Different loggers for different purposes\n        self.audit_logger = get_logger(\n            \"audit\",\n            file_path=\"data/logs/audit.log\",\n            level=logging.INFO\n        )\n\n        self.performance_logger = get_logger(\n            \"performance\",\n            file_path=\"data/logs/performance.log\",\n            level=logging.DEBUG\n        )\n\n        self.business_logger = CustomLog(\"business_events\").get_logger()\n\n    def process_transaction(self, transaction_id: str, amount: float):\n        \"\"\"Process a transaction with multi-level logging.\"\"\"\n        start_time = time.time()\n\n        # Audit logging\n        self.audit_logger.info(f\"Transaction started: {transaction_id}, amount: {amount}\")\n\n        try:\n            # Business logic simulation\n            self.business_logger.info(f\"Processing payment transaction: ${amount:.2f}\")\n\n            # Performance monitoring\n            processing_start = time.time()\n            time.sleep(0.1)  # Simulate processing\n            processing_time = time.time() - processing_start\n\n            self.performance_logger.debug(f\"Transaction {transaction_id} processing time: {processing_time:.3f}s\")\n\n            # Success logging\n            total_time = time.time() - start_time\n            self.audit_logger.info(f\"Transaction completed: {transaction_id}, total_time: {total_time:.3f}s\")\n            self.business_logger.info(f\"Payment processed successfully: ${amount:.2f}\")\n\n            return {\"status\": \"success\", \"transaction_id\": transaction_id, \"processing_time\": total_time}\n\n        except Exception as e:\n            # Error logging across all loggers\n            self.audit_logger.error(f\"Transaction failed: {transaction_id}, error: {e}\")\n            self.business_logger.error(f\"Payment processing failed: ${amount:.2f} - {e}\")\n            raise\n\n# Usage examples\ndef demonstrate_advanced_logging():\n    \"\"\"Demonstrate advanced logging features.\"\"\"\n\n    print(\"=== Context Manager Logging ===\")\n    process_data_with_context()\n\n    print(\"\\n=== Multi-Logger Processing ===\")\n    processor = MultiLoggerProcessor()\n\n    transactions = [\n        (\"TXN001\", 99.99),\n        (\"TXN002\", 1500.00),\n        (\"TXN003\", 25.50)\n    ]\n\n    for txn_id, amount in transactions:\n        try:\n            result = processor.process_transaction(txn_id, amount)\n            print(f\"Transaction result: {result}\")\n        except Exception as e:\n            print(f\"Transaction {txn_id} failed: {e}\")\n\nif __name__ == \"__main__\":\n    demonstrate_advanced_logging()\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/rdelogger/#exception-handling-in-logging","title":"Exception Handling in Logging","text":"<p>The rdelogger module handles various error conditions gracefully:</p> <pre><code>from rdetoolkit.rdelogger import get_logger, LazyFileHandler\nimport logging\nimport os\n\ndef robust_logging_setup():\n    \"\"\"Example of robust logging setup with error handling.\"\"\"\n\n    try:\n        # Attempt to create logger with file output\n        logger = get_logger(__name__, file_path=\"data/logs/application.log\")\n        logger.info(\"Logger created successfully\")\n        return logger\n\n    except PermissionError:\n        # Fall back to console-only logging\n        print(\"Warning: Cannot write to log file, using console logging\")\n        logger = get_logger(__name__, file_path=None)\n        logger.warning(\"Log file creation failed, using console output only\")\n        return logger\n\n    except Exception as e:\n        # Ultimate fallback\n        print(f\"Error setting up logging: {e}\")\n        logger = logging.getLogger(__name__)\n        logger.addHandler(logging.StreamHandler())\n        return logger\n\n# Handling logging errors in decorators\nfrom rdetoolkit.rdelogger import log_decorator\n\ndef safe_log_decorator():\n    \"\"\"A safer version of log_decorator with error handling.\"\"\"\n\n    def _safe_log_decorator(func):\n        def wrapper(*args, **kwargs):\n            try:\n                logger = get_logger(func.__module__)\n                logger.info(f\"{func.__name__:15} --&gt; Start\")\n\n                try:\n                    result = func(*args, **kwargs)\n                    logger.info(f\"{func.__name__:15} &lt;-- End (Success)\")\n                    return result\n                except Exception as func_error:\n                    logger.error(f\"{func.__name__:15} !!! Error: {func_error}\")\n                    raise\n\n            except Exception as log_error:\n                # If logging fails, still execute the function\n                print(f\"Logging error for {func.__name__}: {log_error}\")\n                return func(*args, **kwargs)\n\n        return wrapper\n    return _safe_log_decorator\n\n# Usage with error handling\n@safe_log_decorator()\ndef potentially_failing_function():\n    \"\"\"Function that might fail.\"\"\"\n    import random\n    if random.random() &lt; 0.5:\n        raise ValueError(\"Random failure\")\n    return \"Success\"\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Graceful Degradation:    <pre><code>def setup_logging_with_fallback(preferred_path: str):\n    \"\"\"Setup logging with multiple fallback options.\"\"\"\n    fallback_paths = [\n        preferred_path,\n        \"logs/fallback.log\",\n        \"/tmp/application.log\",\n        None  # Console only\n    ]\n\n    for path in fallback_paths:\n        try:\n            logger = get_logger(__name__, file_path=path)\n            if path:\n                logger.info(f\"Logging initialized with file: {path}\")\n            else:\n                logger.info(\"Logging initialized (console only)\")\n            return logger\n        except Exception as e:\n            if path is None:\n                # Last resort - basic console logging\n                logger = logging.getLogger(__name__)\n                logger.addHandler(logging.StreamHandler())\n                logger.error(f\"All logging setup attempts failed: {e}\")\n                return logger\n            continue\n</code></pre></p> </li> <li> <p>Resource Cleanup:    <pre><code>import atexit\n\ndef setup_logging_with_cleanup():\n    \"\"\"Setup logging with proper cleanup registration.\"\"\"\n    logger = get_logger(__name__, file_path=\"data/logs/app.log\")\n\n    def cleanup_logging():\n        \"\"\"Cleanup function for logging resources.\"\"\"\n        for handler in logger.handlers:\n            if hasattr(handler, 'close'):\n                handler.close()\n\n    # Register cleanup function\n    atexit.register(cleanup_logging)\n    return logger\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/rdelogger/#performance-notes","title":"Performance Notes","text":""},{"location":"rdetoolkit/rdelogger/#optimization-features","title":"Optimization Features","text":"<ol> <li>Lazy File Creation: LazyFileHandler only creates files when log messages are actually emitted</li> <li>Handler Deduplication: Automatic prevention of duplicate handlers to avoid redundant processing</li> <li>Efficient Formatting: Standardized formatters reduce overhead</li> <li>Level-based Filtering: Messages below the configured level are filtered early</li> </ol>"},{"location":"rdetoolkit/rdelogger/#performance-best-practices","title":"Performance Best Practices","text":"<pre><code># Efficient logging patterns\ndef efficient_logging_example():\n    \"\"\"Demonstrate efficient logging patterns.\"\"\"\n\n    logger = get_logger(__name__, file_path=\"data/logs/efficient.log\", level=logging.INFO)\n\n    # Good: Use appropriate log levels\n    logger.debug(\"Detailed debug info\")  # Won't be processed if level is INFO\n    logger.info(\"Important information\")  # Will be processed\n\n    # Good: Use lazy evaluation for expensive operations\n    def expensive_calculation():\n        import time\n        time.sleep(1)\n        return \"expensive_result\"\n\n    # Only call expensive_calculation if DEBUG level is enabled\n    if logger.isEnabledFor(logging.DEBUG):\n        logger.debug(f\"Debug info: {expensive_calculation()}\")\n\n    # Good: Use string formatting in log calls\n    user_id = 12345\n    action = \"login\"\n    logger.info(\"User %s performed action: %s\", user_id, action)\n\n    # Avoid: String concatenation before log call\n    # logger.info(\"User \" + str(user_id) + \" performed action: \" + action)  # Always executes\n\n# Batch logging for high-volume scenarios\nclass BatchLogger:\n    \"\"\"Logger that batches messages for high-volume scenarios.\"\"\"\n\n    def __init__(self, batch_size: int = 100):\n        self.logger = get_logger(__name__, file_path=\"data/logs/batch.log\")\n        self.batch_size = batch_size\n        self.message_buffer = []\n\n    def add_message(self, level: int, message: str):\n        \"\"\"Add message to buffer.\"\"\"\n        self.message_buffer.append((level, message))\n\n        if len(self.message_buffer) &gt;= self.batch_size:\n            self.flush()\n\n    def flush(self):\n        \"\"\"Flush all buffered messages.\"\"\"\n        for level, message in self.message_buffer:\n            self.logger.log(level, message)\n        self.message_buffer.clear()\n\n    def __del__(self):\n        \"\"\"Ensure messages are flushed on destruction.\"\"\"\n        if self.message_buffer:\n            self.flush()\n</code></pre>"},{"location":"rdetoolkit/rdelogger/#see-also","title":"See Also","text":"<ul> <li>Core Module - For directory management and file operations</li> <li>Workflows Module - For workflow execution logging</li> <li>Mode Processing - For processing mode logging</li> <li>Models - RDE2 Types - For RdeFsPath type definitions</li> <li>Error Handling - For error management utilities</li> <li>Usage - CLI - For command-line logging examples</li> <li>Usage - Structured Process - For process logging</li> <li>Usage - Error Handling - For error logging strategies</li> </ul>"},{"location":"rdetoolkit/validation/","title":"Validation Module","text":"<p>The <code>rdetoolkit.validation</code> module provides comprehensive validation functionality for RDE (Research Data Exchange) template files and data structures. This module ensures data integrity and schema compliance for metadata definitions and invoice files.</p>"},{"location":"rdetoolkit/validation/#overview","title":"Overview","text":"<p>The validation module handles validation for critical RDE components:</p> <ul> <li>Metadata Validation: Validates metadata definition files against Pydantic schemas</li> <li>Invoice Validation: Validates invoice files against JSON Schema specifications</li> <li>Schema Compliance: Ensures data structure compliance with RDE standards</li> <li>Error Reporting: Provides detailed validation error messages and context</li> <li>Flexible Input: Supports validation from file paths or in-memory objects</li> </ul>"},{"location":"rdetoolkit/validation/#classes","title":"Classes","text":""},{"location":"rdetoolkit/validation/#metadatavalidator","title":"MetadataValidator","text":"<p>A validator class for validating metadata definition files against the MetadataItem schema.</p>"},{"location":"rdetoolkit/validation/#constructor","title":"Constructor","text":"<pre><code>MetadataValidator()\n</code></pre> <p>Creates a new MetadataValidator instance with the MetadataItem schema.</p>"},{"location":"rdetoolkit/validation/#attributes","title":"Attributes","text":"<ul> <li><code>schema</code> (type[MetadataItem]): The Pydantic schema used for validation</li> </ul>"},{"location":"rdetoolkit/validation/#methods","title":"Methods","text":""},{"location":"rdetoolkit/validation/#validate","title":"validate","text":"<p>Validate JSON data against the MetadataItem schema.</p> <pre><code>def validate(self, *, path: str | Path | None = None, json_obj: dict[str, Any] | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>path</code> (str | Path | None): Path to the JSON file to validate (optional) - <code>json_obj</code> (dict[str, Any] | None): JSON object to validate (optional)</p> <p>Returns: - <code>dict[str, Any]</code>: The validated JSON data</p> <p>Raises: - <code>ValueError</code>: If neither 'path' nor 'json_obj' is provided - <code>ValueError</code>: If both 'path' and 'json_obj' are provided - <code>ValueError</code>: If an unexpected validation error occurs - <code>ValidationError</code>: If the data fails Pydantic validation</p> <p>Example:</p> <pre><code>from rdetoolkit.validation import MetadataValidator\nfrom pathlib import Path\n\n# Create validator instance\nvalidator = MetadataValidator()\n\n# Validate from file path\ntry:\n    validated_data = validator.validate(path=\"data/metadata.json\")\n    print(\"Metadata validation successful\")\n    print(f\"Validated keys: {list(validated_data.keys())}\")\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\n# Validate from JSON object\nmetadata_obj = {\n    \"title\": \"Sample Dataset\",\n    \"description\": \"A sample dataset for testing\",\n    \"creator\": \"Research Team\",\n    \"created\": \"2024-01-01T00:00:00Z\"\n}\n\ntry:\n    validated_data = validator.validate(json_obj=metadata_obj)\n    print(\"Object validation successful\")\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/validation/#invoicevalidator","title":"InvoiceValidator","text":"<p>A comprehensive validator class for validating invoice files against JSON Schema specifications with RDE-specific enhancements.</p>"},{"location":"rdetoolkit/validation/#constructor_1","title":"Constructor","text":"<pre><code>InvoiceValidator(schema_path: str | Path)\n</code></pre> <p>Parameters: - <code>schema_path</code> (str | Path): Path to the invoice schema JSON file</p> <p>Raises: - <code>ValueError</code>: If the schema file is not a JSON file - <code>InvoiceSchemaValidationError</code>: If the schema file itself is invalid</p>"},{"location":"rdetoolkit/validation/#attributes_1","title":"Attributes","text":"<ul> <li><code>schema_path</code> (str | Path): Path to the schema file</li> <li><code>schema</code> (dict[str, Any]): Loaded and processed schema dictionary</li> <li><code>pre_basic_info_schema</code> (str): Path to the basic information schema file</li> </ul>"},{"location":"rdetoolkit/validation/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/validation/#validate_1","title":"validate","text":"<p>Validate JSON data against the invoice schema.</p> <pre><code>def validate(self, *, path: str | Path | None = None, obj: dict[str, Any] | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Parameters: - <code>path</code> (str | Path | None): Path to the JSON file to validate (optional) - <code>obj</code> (dict[str, Any] | None): JSON object to validate (optional)</p> <p>Returns: - <code>dict[str, Any]</code>: The validated and cleaned JSON data</p> <p>Raises: - <code>ValueError</code>: If neither 'path' nor 'obj' is provided - <code>ValueError</code>: If both 'path' and 'obj' are provided - <code>ValueError</code>: If the data is not a dictionary - <code>InvoiceSchemaValidationError</code>: If validation against the schema fails</p> <p>Processing Steps: 1. Load JSON data from file or object 2. Remove None values to handle system-generated invoices 3. Validate against basic information schema 4. Validate against full invoice schema 5. Provide detailed error reporting</p> <p>Example:</p> <pre><code>from rdetoolkit.validation import InvoiceValidator\nfrom rdetoolkit.exceptions import InvoiceSchemaValidationError\nfrom pathlib import Path\n\n# Create validator with schema\nschema_path = Path(\"data/tasksupport/invoice.schema.json\")\nvalidator = InvoiceValidator(schema_path)\n\n# Validate invoice file\ntry:\n    validated_data = validator.validate(path=\"data/invoice/invoice.json\")\n    print(\"Invoice validation successful\")\n    print(f\"Invoice contains {len(validated_data.get('sample', {}))} samples\")\nexcept InvoiceSchemaValidationError as e:\n    print(f\"Invoice validation failed: {e}\")\n\n# Validate invoice object\ninvoice_obj = {\n    \"basic\": {\n        \"title\": \"Research Dataset\",\n        \"description\": \"Experimental data collection\",\n        \"creator\": \"Research Lab\"\n    },\n    \"sample\": {\n        \"generalAttributes\": [\n            {\"key\": \"experiment_id\", \"value\": \"EXP001\"},\n            {\"key\": \"date\", \"value\": \"2024-01-01\"}\n        ],\n        \"specificAttributes\": [\n            {\"key\": \"temperature\", \"value\": \"25.5\", \"unit\": \"\u00b0C\"},\n            {\"key\": \"pressure\", \"value\": \"1013.25\", \"unit\": \"hPa\"}\n        ]\n    }\n}\n\ntry:\n    validated_data = validator.validate(obj=invoice_obj)\n    print(\"Object validation successful\")\nexcept InvoiceSchemaValidationError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/validation/#functions","title":"Functions","text":""},{"location":"rdetoolkit/validation/#metadata_validate","title":"metadata_validate","text":"<p>Validate a metadata definition file against the MetadataItem schema.</p> <pre><code>def metadata_validate(path: str | Path) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path to the metadata definition file</p> <p>Returns: - <code>None</code></p> <p>Raises: - <code>FileNotFoundError</code>: If the metadata file does not exist - <code>MetadataValidationError</code>: If validation fails with detailed error information</p> <p>Example:</p> <pre><code>from rdetoolkit.validation import metadata_validate\nfrom rdetoolkit.exceptions import MetadataValidationError\nfrom pathlib import Path\n\n# Validate metadata file\ntry:\n    metadata_validate(\"data/meta/metadata.json\")\n    print(\"Metadata file is valid\")\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\nexcept MetadataValidationError as e:\n    print(f\"Validation errors found:\\n{e}\")\n\n# Example of handling validation errors\ndef validate_metadata_with_details(file_path: str) -&gt; bool:\n    \"\"\"Validate metadata and return success status.\"\"\"\n    try:\n        metadata_validate(file_path)\n        return True\n    except MetadataValidationError as e:\n        print(\"Metadata validation failed with the following errors:\")\n        # The error message contains numbered validation errors\n        print(e)\n        return False\n    except FileNotFoundError:\n        print(f\"Metadata file not found: {file_path}\")\n        return False\n\n# Usage\nsuccess = validate_metadata_with_details(\"data/meta/metadata.json\")\nif success:\n    print(\"Metadata validation passed\")\nelse:\n    print(\"Metadata validation failed - please fix errors and retry\")\n</code></pre>"},{"location":"rdetoolkit/validation/#invoice_validate","title":"invoice_validate","text":"<p>Validate an invoice file against its corresponding schema.</p> <pre><code>def invoice_validate(path: str | Path, schema: str | Path) -&gt; None\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path to the invoice.json file - <code>schema</code> (str | Path): Path to the invoice.schema.json file</p> <p>Returns: - <code>None</code></p> <p>Raises: - <code>FileNotFoundError</code>: If either the invoice file or schema file does not exist - <code>InvoiceSchemaValidationError</code>: If validation fails</p> <p>Example:</p> <pre><code>from rdetoolkit.validation import invoice_validate\nfrom rdetoolkit.exceptions import InvoiceSchemaValidationError\nfrom pathlib import Path\n\n# Basic invoice validation\ntry:\n    invoice_validate(\n        path=\"data/invoice/invoice.json\",\n        schema=\"data/tasksupport/invoice.schema.json\"\n    )\n    print(\"Invoice validation successful\")\nexcept FileNotFoundError as e:\n    print(f\"Required file not found: {e}\")\nexcept InvoiceSchemaValidationError as e:\n    print(f\"Invoice validation failed: {e}\")\n\n# Batch validation of multiple invoices\ndef validate_invoice_batch(invoice_dir: Path, schema_path: Path) -&gt; dict[str, bool]:\n    \"\"\"Validate multiple invoice files.\"\"\"\n    results = {}\n\n    for invoice_file in invoice_dir.glob(\"invoice_*.json\"):\n        try:\n            invoice_validate(invoice_file, schema_path)\n            results[invoice_file.name] = True\n            print(f\"\u2713 {invoice_file.name} - Valid\")\n        except (FileNotFoundError, InvoiceSchemaValidationError) as e:\n            results[invoice_file.name] = False\n            print(f\"\u2717 {invoice_file.name} - Invalid: {e}\")\n\n    return results\n\n# Usage\ninvoice_directory = Path(\"data/invoices/\")\nschema_file = Path(\"data/tasksupport/invoice.schema.json\")\nvalidation_results = validate_invoice_batch(invoice_directory, schema_file)\n\n# Summary\ntotal = len(validation_results)\nvalid = sum(validation_results.values())\nprint(f\"Validation Summary: {valid}/{total} invoices are valid\")\n</code></pre>"},{"location":"rdetoolkit/validation/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/validation/#comprehensive-validation-workflow","title":"Comprehensive Validation Workflow","text":"<pre><code>from rdetoolkit.validation import MetadataValidator, InvoiceValidator, metadata_validate, invoice_validate\nfrom rdetoolkit.exceptions import MetadataValidationError, InvoiceSchemaValidationError\nfrom pathlib import Path\nimport json\nfrom typing import Dict, List, Tuple\n\nclass ValidationManager:\n    \"\"\"Comprehensive validation manager for RDE files.\"\"\"\n\n    def __init__(self, base_path: Path):\n        self.base_path = base_path\n        self.metadata_validator = MetadataValidator()\n        self.validation_results = {\n            \"metadata\": [],\n            \"invoices\": [],\n            \"summary\": {\"total\": 0, \"valid\": 0, \"invalid\": 0}\n        }\n\n    def validate_all_metadata(self, metadata_dir: Path) -&gt; List[Dict[str, any]]:\n        \"\"\"Validate all metadata files in a directory.\"\"\"\n        results = []\n\n        for metadata_file in metadata_dir.glob(\"metadata*.json\"):\n            result = {\n                \"file\": str(metadata_file),\n                \"valid\": False,\n                \"errors\": []\n            }\n\n            try:\n                self.metadata_validator.validate(path=metadata_file)\n                result[\"valid\"] = True\n                print(f\"\u2713 Metadata validation passed: {metadata_file.name}\")\n            except MetadataValidationError as e:\n                result[\"errors\"].append(str(e))\n                print(f\"\u2717 Metadata validation failed: {metadata_file.name}\")\n            except Exception as e:\n                result[\"errors\"].append(f\"Unexpected error: {e}\")\n                print(f\"\u2717 Unexpected error in {metadata_file.name}: {e}\")\n\n            results.append(result)\n            self.validation_results[\"metadata\"].append(result)\n\n        return results\n\n    def validate_all_invoices(self, invoice_dir: Path, schema_path: Path) -&gt; List[Dict[str, any]]:\n        \"\"\"Validate all invoice files against schema.\"\"\"\n        results = []\n\n        try:\n            invoice_validator = InvoiceValidator(schema_path)\n        except Exception as e:\n            print(f\"Failed to create invoice validator: {e}\")\n            return results\n\n        for invoice_file in invoice_dir.glob(\"invoice*.json\"):\n            result = {\n                \"file\": str(invoice_file),\n                \"valid\": False,\n                \"errors\": []\n            }\n\n            try:\n                invoice_validator.validate(path=invoice_file)\n                result[\"valid\"] = True\n                print(f\"\u2713 Invoice validation passed: {invoice_file.name}\")\n            except InvoiceSchemaValidationError as e:\n                result[\"errors\"].append(str(e))\n                print(f\"\u2717 Invoice validation failed: {invoice_file.name}\")\n            except Exception as e:\n                result[\"errors\"].append(f\"Unexpected error: {e}\")\n                print(f\"\u2717 Unexpected error in {invoice_file.name}: {e}\")\n\n            results.append(result)\n            self.validation_results[\"invoices\"].append(result)\n\n        return results\n\n    def validate_project_structure(self) -&gt; Dict[str, any]:\n        \"\"\"Validate entire project structure.\"\"\"\n        print(\"Starting comprehensive validation...\")\n\n        # Validate metadata files\n        metadata_dir = self.base_path / \"meta\"\n        if metadata_dir.exists():\n            print(\"\\n=== Metadata Validation ===\")\n            metadata_results = self.validate_all_metadata(metadata_dir)\n        else:\n            print(\"Metadata directory not found, skipping metadata validation\")\n            metadata_results = []\n\n        # Validate invoice files\n        invoice_dir = self.base_path / \"invoice\"\n        schema_path = self.base_path / \"tasksupport\" / \"invoice.schema.json\"\n\n        if invoice_dir.exists() and schema_path.exists():\n            print(\"\\n=== Invoice Validation ===\")\n            invoice_results = self.validate_all_invoices(invoice_dir, schema_path)\n        else:\n            print(\"Invoice directory or schema file not found, skipping invoice validation\")\n            invoice_results = []\n\n        # Calculate summary\n        all_results = metadata_results + invoice_results\n        total = len(all_results)\n        valid = sum(1 for r in all_results if r[\"valid\"])\n        invalid = total - valid\n\n        self.validation_results[\"summary\"] = {\n            \"total\": total,\n            \"valid\": valid,\n            \"invalid\": invalid,\n            \"success_rate\": (valid / total * 100) if total &gt; 0 else 0\n        }\n\n        print(f\"\\n=== Validation Summary ===\")\n        print(f\"Total files validated: {total}\")\n        print(f\"Valid files: {valid}\")\n        print(f\"Invalid files: {invalid}\")\n        print(f\"Success rate: {self.validation_results['summary']['success_rate']:.1f}%\")\n\n        return self.validation_results\n\n    def save_validation_report(self, output_path: Path) -&gt; None:\n        \"\"\"Save detailed validation report to JSON file.\"\"\"\n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)\n        print(f\"Validation report saved to: {output_path}\")\n\n# Usage example\ndef main():\n    # Setup validation manager\n    project_path = Path(\"data\")\n    validator = ValidationManager(project_path)\n\n    # Run comprehensive validation\n    results = validator.validate_project_structure()\n\n    # Save detailed report\n    report_path = project_path / \"validation_report.json\"\n    validator.save_validation_report(report_path)\n\n    # Return success status\n    return results[\"summary\"][\"invalid\"] == 0\n\nif __name__ == \"__main__\":\n    success = main()\n    exit(0 if success else 1)\n</code></pre>"},{"location":"rdetoolkit/validation/#advanced-validation-with-custom-rules","title":"Advanced Validation with Custom Rules","text":"<pre><code>from rdetoolkit.validation import MetadataValidator, InvoiceValidator\nfrom pathlib import Path\nimport json\nfrom typing import Any, Dict, List\n\nclass CustomValidationRules:\n    \"\"\"Custom validation rules for specific business logic.\"\"\"\n\n    @staticmethod\n    def validate_metadata_completeness(metadata: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate that metadata contains all required fields for production.\"\"\"\n        errors = []\n\n        required_fields = [\"title\", \"description\", \"creator\", \"created\", \"keywords\"]\n        for field in required_fields:\n            if field not in metadata or not metadata[field]:\n                errors.append(f\"Required field '{field}' is missing or empty\")\n\n        # Validate date format\n        if \"created\" in metadata:\n            try:\n                from datetime import datetime\n                datetime.fromisoformat(metadata[\"created\"].replace(\"Z\", \"+00:00\"))\n            except ValueError:\n                errors.append(\"Field 'created' must be in ISO 8601 format\")\n\n        # Validate keywords\n        if \"keywords\" in metadata:\n            if not isinstance(metadata[\"keywords\"], list) or len(metadata[\"keywords\"]) &lt; 3:\n                errors.append(\"Field 'keywords' must be a list with at least 3 items\")\n\n        return errors\n\n    @staticmethod\n    def validate_invoice_data_quality(invoice: Dict[str, Any]) -&gt; List[str]:\n        \"\"\"Validate invoice data quality and consistency.\"\"\"\n        errors = []\n\n        # Check if sample data exists\n        if \"sample\" not in invoice:\n            errors.append(\"Invoice must contain 'sample' section\")\n            return errors\n\n        sample = invoice[\"sample\"]\n\n        # Validate general attributes\n        if \"generalAttributes\" in sample:\n            gen_attrs = sample[\"generalAttributes\"]\n            if not isinstance(gen_attrs, list) or len(gen_attrs) == 0:\n                errors.append(\"generalAttributes must be a non-empty list\")\n\n            # Check for duplicate keys\n            keys = [attr.get(\"key\") for attr in gen_attrs if isinstance(attr, dict)]\n            if len(keys) != len(set(keys)):\n                errors.append(\"Duplicate keys found in generalAttributes\")\n\n        # Validate specific attributes\n        if \"specificAttributes\" in sample:\n            spec_attrs = sample[\"specificAttributes\"]\n            if not isinstance(spec_attrs, list):\n                errors.append(\"specificAttributes must be a list\")\n\n            # Validate units for numeric values\n            for attr in spec_attrs:\n                if isinstance(attr, dict) and \"value\" in attr:\n                    try:\n                        float(attr[\"value\"])\n                        if \"unit\" not in attr:\n                            errors.append(f\"Numeric attribute '{attr.get('key', 'unknown')}' should have a unit\")\n                    except ValueError:\n                        pass  # Non-numeric value, unit not required\n\n        return errors\n\nclass EnhancedValidator:\n    \"\"\"Enhanced validator with custom rules and reporting.\"\"\"\n\n    def __init__(self):\n        self.metadata_validator = MetadataValidator()\n        self.custom_rules = CustomValidationRules()\n\n    def validate_metadata_enhanced(self, path: Path) -&gt; Dict[str, Any]:\n        \"\"\"Enhanced metadata validation with custom rules.\"\"\"\n        result = {\n            \"file\": str(path),\n            \"schema_valid\": False,\n            \"custom_valid\": False,\n            \"schema_errors\": [],\n            \"custom_errors\": [],\n            \"data\": None\n        }\n\n        try:\n            # Schema validation\n            validated_data = self.metadata_validator.validate(path=path)\n            result[\"schema_valid\"] = True\n            result[\"data\"] = validated_data\n\n            # Custom rules validation\n            custom_errors = self.custom_rules.validate_metadata_completeness(validated_data)\n            if custom_errors:\n                result[\"custom_errors\"] = custom_errors\n            else:\n                result[\"custom_valid\"] = True\n\n        except Exception as e:\n            result[\"schema_errors\"] = [str(e)]\n\n        return result\n\n    def validate_invoice_enhanced(self, invoice_path: Path, schema_path: Path) -&gt; Dict[str, Any]:\n        \"\"\"Enhanced invoice validation with custom rules.\"\"\"\n        result = {\n            \"file\": str(invoice_path),\n            \"schema_valid\": False,\n            \"custom_valid\": False,\n            \"schema_errors\": [],\n            \"custom_errors\": [],\n            \"data\": None\n        }\n\n        try:\n            # Schema validation\n            invoice_validator = InvoiceValidator(schema_path)\n            validated_data = invoice_validator.validate(path=invoice_path)\n            result[\"schema_valid\"] = True\n            result[\"data\"] = validated_data\n\n            # Custom rules validation\n            custom_errors = self.custom_rules.validate_invoice_data_quality(validated_data)\n            if custom_errors:\n                result[\"custom_errors\"] = custom_errors\n            else:\n                result[\"custom_valid\"] = True\n\n        except Exception as e:\n            result[\"schema_errors\"] = [str(e)]\n\n        return result\n\n# Usage example\ndef validate_with_custom_rules():\n    \"\"\"Example of using enhanced validation with custom rules.\"\"\"\n\n    validator = EnhancedValidator()\n\n    # Validate metadata with custom rules\n    metadata_path = Path(\"data/meta/metadata.json\")\n    if metadata_path.exists():\n        result = validator.validate_metadata_enhanced(metadata_path)\n\n        print(f\"Metadata Validation Results for {metadata_path.name}:\")\n        print(f\"  Schema Valid: {result['schema_valid']}\")\n        print(f\"  Custom Rules Valid: {result['custom_valid']}\")\n\n        if result['schema_errors']:\n            print(\"  Schema Errors:\")\n            for error in result['schema_errors']:\n                print(f\"    - {error}\")\n\n        if result['custom_errors']:\n            print(\"  Custom Rule Violations:\")\n            for error in result['custom_errors']:\n                print(f\"    - {error}\")\n\n    # Validate invoice with custom rules\n    invoice_path = Path(\"data/invoice/invoice.json\")\n    schema_path = Path(\"data/tasksupport/invoice.schema.json\")\n\n    if invoice_path.exists() and schema_path.exists():\n        result = validator.validate_invoice_enhanced(invoice_path, schema_path)\n\n        print(f\"\\nInvoice Validation Results for {invoice_path.name}:\")\n        print(f\"  Schema Valid: {result['schema_valid']}\")\n        print(f\"  Custom Rules Valid: {result['custom_valid']}\")\n\n        if result['schema_errors']:\n            print(\"  Schema Errors:\")\n            for error in result['schema_errors']:\n                print(f\"    - {error}\")\n\n        if result['custom_errors']:\n            print(\"  Custom Rule Violations:\")\n            for error in result['custom_errors']:\n                print(f\"    - {error}\")\n\n# Run enhanced validation\nvalidate_with_custom_rules()\n</code></pre>"},{"location":"rdetoolkit/validation/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/validation/#exception-types","title":"Exception Types","text":"<p>The validation module raises specific exceptions for different error conditions:</p>"},{"location":"rdetoolkit/validation/#metadatavalidationerror","title":"MetadataValidationError","text":"<p>Raised when metadata validation fails against the Pydantic schema.</p> <pre><code>from rdetoolkit.exceptions import MetadataValidationError\n\ntry:\n    metadata_validate(\"invalid_metadata.json\")\nexcept MetadataValidationError as e:\n    print(f\"Metadata validation failed: {e}\")\n    # Error message contains detailed field-by-field errors\n</code></pre>"},{"location":"rdetoolkit/validation/#invoiceschemavalidationerror","title":"InvoiceSchemaValidationError","text":"<p>Raised when invoice validation fails against the JSON schema.</p> <pre><code>from rdetoolkit.exceptions import InvoiceSchemaValidationError\n\ntry:\n    invoice_validate(\"invalid_invoice.json\", \"invoice.schema.json\")\nexcept InvoiceSchemaValidationError as e:\n    print(f\"Invoice validation failed: {e}\")\n    # Error message contains detailed validation errors with field paths\n</code></pre>"},{"location":"rdetoolkit/validation/#error-message-format","title":"Error Message Format","text":"<p>Validation errors provide detailed, structured information:</p> <pre><code>Validation Errors in metadata.json. Please correct the following fields\n1. Field: title\n   Type: missing\n   Context: Field required\n\n2. Field: created\n   Type: value_error\n   Context: invalid datetime format\n\n3. Field: keywords.0\n   Type: type_error\n   Context: str type expected\n</code></pre>"},{"location":"rdetoolkit/validation/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Catch Specific Exceptions:    <pre><code>def safe_validate_metadata(file_path: str) -&gt; Tuple[bool, List[str]]:\n    \"\"\"Safely validate metadata and return status with errors.\"\"\"\n    try:\n        metadata_validate(file_path)\n        return True, []\n    except FileNotFoundError:\n        return False, [f\"File not found: {file_path}\"]\n    except MetadataValidationError as e:\n        return False, [str(e)]\n    except Exception as e:\n        return False, [f\"Unexpected error: {e}\"]\n</code></pre></p> </li> <li> <p>Provide User-Friendly Error Messages:    <pre><code>def validate_with_user_feedback(metadata_path: str):\n    \"\"\"Validate with user-friendly error reporting.\"\"\"\n    success, errors = safe_validate_metadata(metadata_path)\n\n    if success:\n        print(\"\u2705 Validation successful!\")\n    else:\n        print(\"\u274c Validation failed:\")\n        for error in errors:\n            print(f\"   {error}\")\n        print(\"\\nPlease fix the errors and try again.\")\n</code></pre></p> </li> <li> <p>Implement Retry Logic:    <pre><code>def validate_with_retry(file_path: str, max_retries: int = 3) -&gt; bool:\n    \"\"\"Validate with retry logic for transient errors.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            metadata_validate(file_path)\n            return True\n        except FileNotFoundError:\n            return False  # Don't retry for missing files\n        except MetadataValidationError:\n            return False  # Don't retry for validation errors\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise  # Re-raise on final attempt\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n            time.sleep(1)  # Brief delay before retry\n\n    return False\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/validation/#performance-notes","title":"Performance Notes","text":""},{"location":"rdetoolkit/validation/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Schema Caching: InvoiceValidator caches loaded schemas to avoid repeated file I/O</li> <li>Lazy Loading: Validators only load schemas when first used</li> <li>Memory Management: Large JSON files are processed efficiently with streaming where possible</li> <li>Validation Short-Circuiting: Validation stops at first critical error to save processing time</li> </ol>"},{"location":"rdetoolkit/validation/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>File Size: Large invoice files (&gt;10MB) may require additional memory</li> <li>Schema Complexity: Complex schemas with many nested validations impact performance</li> <li>Batch Processing: Use validator instances for multiple files to benefit from schema caching</li> <li>Error Reporting: Detailed error collection can impact performance for large files with many errors</li> </ul>"},{"location":"rdetoolkit/validation/#performance-best-practices","title":"Performance Best Practices","text":"<pre><code># Efficient batch validation\ndef efficient_batch_validation(file_paths: List[Path], schema_path: Path):\n    \"\"\"Efficiently validate multiple files using cached validator.\"\"\"\n\n    # Create validator once for all files\n    invoice_validator = InvoiceValidator(schema_path)\n    metadata_validator = MetadataValidator()\n\n    results = []\n    for file_path in file_paths:\n        try:\n            if file_path.name.startswith('metadata'):\n                metadata_validator.validate(path=file_path)\n            elif file_path.name.startswith('invoice'):\n                invoice_validator.validate(path=file_path)\n            results.append((file_path, True, None))\n        except Exception as e:\n            results.append((file_path, False, str(e)))\n\n    return results\n</code></pre>"},{"location":"rdetoolkit/validation/#see-also","title":"See Also","text":"<ul> <li>Core Module - For file operations and encoding detection</li> <li>Models - Metadata - For metadata data structures</li> <li>Models - Invoice Schema - For invoice schema definitions</li> <li>Exceptions - For validation exception types</li> <li>File Operations - For JSON file reading utilities</li> <li>Usage - Validation - For practical validation examples</li> <li>Usage - Structured Process - For validation in workflows</li> <li>Usage - Error Handling - For error handling strategies</li> </ul>"},{"location":"rdetoolkit/workflows/","title":"Workflows Module","text":"<p>The <code>rdetoolkit.workflows</code> module provides the core workflow functionality for RDE (Research Data Exchange) data structuring processes. This module orchestrates the complete data processing pipeline, from input validation to structured output generation.</p>"},{"location":"rdetoolkit/workflows/#overview","title":"Overview","text":"<p>The workflows module serves as the main entry point for RDE data processing operations. It handles:</p> <ul> <li>Input File Classification: Automatic detection and classification of input data types</li> <li>Mode Processing: Support for multiple processing modes (Invoice, ExcelInvoice, SmartTable, etc.)</li> <li>Directory Management: Creation and management of output directory structures</li> <li>Workflow Execution: Coordinated execution of the complete structuring pipeline</li> <li>Error Handling: Comprehensive error management with structured reporting</li> </ul>"},{"location":"rdetoolkit/workflows/#functions","title":"Functions","text":""},{"location":"rdetoolkit/workflows/#check_files","title":"check_files","text":"<p>Classify input files to determine the appropriate processing pattern.</p> <pre><code>def check_files(srcpaths: RdeInputDirPaths, *, mode: str | None) -&gt; tuple[RawFiles, Path | None, Path | None]\n</code></pre> <p>Parameters: - <code>srcpaths</code> (RdeInputDirPaths): Input directory paths containing source data - <code>mode</code> (str | None): Processing mode override (optional)</p> <p>Returns: - <code>tuple[RawFiles, Path | None, Path | None]</code>: A tuple containing:   - <code>RawFiles</code>: List of tuples with registered data file path groups   - <code>Path | None</code>: Excel invoice file path (if present)   - <code>Path | None</code>: SmartTable file path (if present)</p> <p>Processing Modes:</p> <ol> <li>Invoice Mode</li> <li>File mode: Single file processing (e.g., <code>sample.txt</code>)</li> <li>Folder mode: Multiple file processing (e.g., <code>sample1.txt</code>, <code>sample2.txt</code>)</li> <li> <p>No input: Empty input processing</p> </li> <li> <p>ExcelInvoice Mode</p> </li> <li>File mode: ZIP file with Excel invoice (e.g., <code>sample.zip</code> + <code>*_excel_invoice.xlsx</code>)</li> <li>Folder mode: Compressed folder with Excel invoice</li> <li> <p>Excel-only: Excel invoice file without data files</p> </li> <li> <p>Format Mode</p> </li> <li> <p>ZIP files with RDE format specification (e.g., <code>*.zip</code>, <code>tasksupport/rdeformat.txt</code>)</p> </li> <li> <p>Multiple Files Mode</p> </li> <li>Flat structure with multiple files (e.g., <code>sample1.txt</code>, <code>sample2.txt</code>, <code>sample3.txt</code>)</li> </ol> <p>Examples:</p> <pre><code>from rdetoolkit.workflows import check_files\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths\nfrom pathlib import Path\n\n# Setup input paths\nsrcpaths = RdeInputDirPaths(\n    inputdata=Path(\"data/inputdata\"),\n    invoice=Path(\"data/invoice\"),\n    tasksupport=Path(\"data/tasksupport\")\n)\n\n# Invoice mode - single file\nrawfiles, excel_file, smarttable_file = check_files(srcpaths, mode=\"invoice\")\n# Returns: ([(Path('data/inputdata/sample.txt'),)], None, None)\n\n# Invoice mode - multiple files\nrawfiles, excel_file, smarttable_file = check_files(srcpaths, mode=\"invoice\")\n# Returns: ([(Path('data/inputdata/sample1.txt'), Path('data/inputdata/sample2.txt'))], None, None)\n\n# ExcelInvoice mode - with data files\nrawfiles, excel_file, smarttable_file = check_files(srcpaths, mode=\"excelinvoice\")\n# Returns: ([(Path('data/temp/sample.txt'),)], Path(\"data/inputdata/dataset_excel_invoice.xlsx\"), None)\n\n# SmartTable mode\nrawfiles, excel_file, smarttable_file = check_files(srcpaths, mode=\"smarttable\")\n# Returns: ([(Path('data/temp/sample.txt'),)], None, Path(\"data/inputdata/dataset_smarttable.xlsx\"))\n</code></pre> <p>Note: - Destination paths differ between modes:   - Invoice: <code>/data/inputdata/&lt;registered_files&gt;</code>   - ExcelInvoice/SmartTable: <code>/data/temp/&lt;registered_files&gt;</code> - The function automatically detects file patterns and selects appropriate processing mode</p>"},{"location":"rdetoolkit/workflows/#generate_folder_paths_iterator","title":"generate_folder_paths_iterator","text":"<p>Generate an iterator for RDE output folder paths with proper directory structure.</p> <pre><code>def generate_folder_paths_iterator(\n    raw_files_group: RawFiles,\n    invoice_org_filepath: Path,\n    invoice_schema_filepath: Path,\n) -&gt; Generator[RdeOutputResourcePath, None, None]\n</code></pre> <p>Parameters: - <code>raw_files_group</code> (RawFiles): List of tuples containing raw file paths - <code>invoice_org_filepath</code> (Path): Path to <code>invoice_org.json</code> file - <code>invoice_schema_filepath</code> (Path): Path to <code>invoice.schema.json</code> file</p> <p>Yields: - <code>RdeOutputResourcePath</code>: Named tuple containing all output folder paths for RDE resources</p> <p>Raises: - <code>StructuredError</code>: When the structured process fails to process correctly</p> <p>Directory Structure:</p> <p>The iterator creates the following directory structure for each data tile:</p> <pre><code>data/\n\u251c\u2500\u2500 raw/                    # Raw data files\n\u251c\u2500\u2500 structured/            # Processed/structured data\n\u251c\u2500\u2500 main_image/           # Primary images\n\u251c\u2500\u2500 other_image/          # Additional images\n\u251c\u2500\u2500 thumbnail/            # Thumbnail images\n\u251c\u2500\u2500 meta/                 # Metadata files\n\u251c\u2500\u2500 logs/                 # Processing logs\n\u251c\u2500\u2500 invoice/              # Invoice data\n\u251c\u2500\u2500 temp/                 # Temporary files\n\u251c\u2500\u2500 nonshared_raw/        # Non-shared raw data\n\u251c\u2500\u2500 invoice_patch/        # Invoice patches\n\u2514\u2500\u2500 attachment/           # File attachments\n</code></pre> <p>Example:</p> <pre><code>from rdetoolkit.workflows import generate_folder_paths_iterator\nfrom pathlib import Path\n\n# Example raw files\nraw_files_group = [\n    (Path('data/temp/sample1.txt'),),\n    (Path('data/temp/sample2.txt'),),\n    (Path('data/temp/sample3.txt'),)\n]\n\n# Invoice file paths\ninvoice_org = Path(\"data/invoice/invoice_org.json\")\ninvoice_schema = Path(\"data/tasksupport/invoice.schema.json\")\n\n# Generate folder paths\nfor idx, resource_path in enumerate(generate_folder_paths_iterator(\n    raw_files_group, invoice_org, invoice_schema\n)):\n    print(f\"Data tile {idx}:\")\n    print(f\"  Raw: {resource_path.raw}\")\n    print(f\"  Structured: {resource_path.struct}\")\n    print(f\"  Main Image: {resource_path.main_image}\")\n    print(f\"  Thumbnail: {resource_path.thumbnail}\")\n    # ... other paths\n</code></pre> <p>RdeOutputResourcePath Attributes:</p> <ul> <li><code>raw</code>: Raw data directory</li> <li><code>rawfiles</code>: Tuple of source raw file paths</li> <li><code>struct</code>: Structured data directory</li> <li><code>main_image</code>: Main image directory</li> <li><code>other_image</code>: Other images directory</li> <li><code>thumbnail</code>: Thumbnail directory</li> <li><code>meta</code>: Metadata directory</li> <li><code>logs</code>: Logs directory</li> <li><code>invoice</code>: Invoice directory</li> <li><code>invoice_schema_json</code>: Invoice schema file path</li> <li><code>invoice_org</code>: Original invoice file path</li> <li><code>temp</code>: Temporary directory</li> <li><code>nonshared_raw</code>: Non-shared raw data directory</li> <li><code>invoice_patch</code>: Invoice patch directory</li> <li><code>attachment</code>: Attachment directory</li> </ul>"},{"location":"rdetoolkit/workflows/#run","title":"run","text":"<p>Execute the complete RDE structuring processing pipeline.</p> <pre><code>def run(*, custom_dataset_function: _CallbackType | None = None, config: Config | None = None) -&gt; str\n</code></pre> <p>Parameters: - <code>custom_dataset_function</code> (_CallbackType | None): User-defined structuring function (optional) - <code>config</code> (Config | None): Configuration for the structuring process (optional, defaults loaded automatically)</p> <p>Returns: - <code>str</code>: JSON representation of workflow execution results</p> <p>Raises: - <code>StructuredError</code>: If a structured error occurs during processing - <code>Exception</code>: If a generic error occurs during processing</p> <p>Callback Function Type:</p> <pre><code>_CallbackType = Callable[[RdeInputDirPaths, RdeOutputResourcePath], None]\n</code></pre> <p>The custom dataset function receives: - <code>RdeInputDirPaths</code>: Parsed input directory paths - <code>RdeOutputResourcePath</code>: Output directory paths for the current data tile</p> <p>Processing Modes:</p> <p>The function evaluates execution modes in the following order:</p> <ol> <li>SmartTableInvoice: If SmartTable file is detected</li> <li>ExcelInvoice: If Excel invoice file is detected</li> <li>RDEFormat: If <code>extended_mode = \"rdeformat\"</code> in config</li> <li>MultiDataTile: If <code>extended_mode = \"multidatatile\"</code> in config</li> <li>Invoice: Default mode for standard invoice processing</li> </ol> <p>Examples:</p>"},{"location":"rdetoolkit/workflows/#basic-usage","title":"Basic Usage","text":"<pre><code>from rdetoolkit.workflows import run\n\n# Execute with default configuration\nresult = run()\nprint(\"Workflow completed:\", result)\n</code></pre>"},{"location":"rdetoolkit/workflows/#with-custom-processing-function","title":"With Custom Processing Function","text":"<pre><code>from rdetoolkit.workflows import run\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\nimport shutil\n\ndef custom_dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Custom processing function for special data handling.\"\"\"\n\n    # Custom processing logic\n    for raw_file in resource_paths.rawfiles:\n        # Copy raw files to structured directory with processing\n        processed_file = resource_paths.struct / f\"processed_{raw_file.name}\"\n        shutil.copy2(raw_file, processed_file)\n\n        # Additional custom processing...\n        print(f\"Processed: {raw_file} -&gt; {processed_file}\")\n\n# Execute with custom function\nresult = run(custom_dataset_function=custom_dataset)\n</code></pre>"},{"location":"rdetoolkit/workflows/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from rdetoolkit.workflows import run\nfrom rdetoolkit.models.config import Config, SystemSettings, MultiDataTileSettings\n\n# Standard configuration\nconfig = Config(\n    save_raw=True,\n    save_main_image=False,\n    save_thumbnail_image=False,\n    magic_variable=False\n)\n\nresult = run(config=config)\n\n# Advanced configuration with extended mode\nadvanced_config = Config(\n    system=SystemSettings(\n        extended_mode=\"MultiDataTile\",\n        save_raw=False,\n        save_nonshared_raw=True,\n        save_thumbnail_image=True\n    ),\n    multidata_tile=MultiDataTileSettings(\n        ignore_errors=False\n    )\n)\n\nresult = run(config=advanced_config)\n</code></pre>"},{"location":"rdetoolkit/workflows/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code>from rdetoolkit.workflows import run\nfrom rdetoolkit.models.config import Config, SystemSettings\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\nfrom pathlib import Path\nimport json\n\ndef advanced_custom_processor(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Advanced custom processing with metadata generation.\"\"\"\n\n    # Process each raw file\n    metadata = {\n        \"processed_files\": [],\n        \"processing_timestamp\": \"2024-01-01T00:00:00Z\",\n        \"source_count\": len(resource_paths.rawfiles)\n    }\n\n    for raw_file in resource_paths.rawfiles:\n        if raw_file.suffix.lower() in ['.txt', '.csv']:\n            # Text file processing\n            content = raw_file.read_text(encoding='utf-8')\n            processed_content = content.upper()  # Example processing\n\n            # Save processed file\n            output_file = resource_paths.struct / f\"processed_{raw_file.name}\"\n            output_file.write_text(processed_content, encoding='utf-8')\n\n            metadata[\"processed_files\"].append({\n                \"source\": str(raw_file),\n                \"output\": str(output_file),\n                \"size\": len(processed_content)\n            })\n\n    # Save metadata\n    meta_file = resource_paths.meta / \"processing_metadata.json\"\n    meta_file.write_text(json.dumps(metadata, indent=2), encoding='utf-8')\n\n# Configuration for advanced processing\nconfig = Config(\n    system=SystemSettings(\n        extended_mode=\"MultiDataTile\",\n        save_raw=True,\n        save_nonshared_raw=False,\n        save_thumbnail_image=True\n    ),\n    multidata_tile=MultiDataTileSettings(\n        ignore_errors=True  # Continue processing even if some tiles fail\n    )\n)\n\n# Execute advanced workflow\ntry:\n    result_json = run(\n        custom_dataset_function=advanced_custom_processor,\n        config=config\n    )\n\n    # Parse and display results\n    results = json.loads(result_json)\n    print(f\"Processed {len(results.get('statuses', []))} data tiles\")\n\n    for status in results.get('statuses', []):\n        print(f\"Tile {status['run_id']}: {status['status']} ({status['mode']})\")\n\nexcept Exception as e:\n    print(f\"Workflow failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/workflows/#workflow-execution-process","title":"Workflow Execution Process","text":"<p>The <code>run</code> function follows this execution sequence:</p>"},{"location":"rdetoolkit/workflows/#1-initialization","title":"1. Initialization","text":"<ul> <li>Initialize logging system</li> <li>Create workflow result manager</li> <li>Setup input directory paths</li> </ul>"},{"location":"rdetoolkit/workflows/#2-configuration-loading","title":"2. Configuration Loading","text":"<ul> <li>Load configuration from <code>tasksupport</code> directory</li> <li>Apply user-provided configuration overrides</li> <li>Validate configuration settings</li> </ul>"},{"location":"rdetoolkit/workflows/#3-input-analysis","title":"3. Input Analysis","text":"<ul> <li>Classify input files using <code>check_files</code></li> <li>Determine processing mode</li> <li>Backup invoice files if needed</li> </ul>"},{"location":"rdetoolkit/workflows/#4-resource-path-generation","title":"4. Resource Path Generation","text":"<ul> <li>Generate output directory structure</li> <li>Create iterator for processing multiple data tiles</li> <li>Setup progress tracking</li> </ul>"},{"location":"rdetoolkit/workflows/#5-data-processing","title":"5. Data Processing","text":"<ul> <li>Iterate through each data tile</li> <li>Execute mode-specific processing:</li> <li>SmartTableInvoice: Process with SmartTable integration</li> <li>ExcelInvoice: Process with Excel invoice data</li> <li>RDEFormat: Process using RDE format specifications</li> <li>MultiDataTile: Process multiple data tiles with error tolerance</li> <li>Invoice: Standard invoice processing</li> <li>Execute custom dataset function if provided</li> <li>Track processing status and errors</li> </ul>"},{"location":"rdetoolkit/workflows/#6-result-compilation","title":"6. Result Compilation","text":"<ul> <li>Collect all processing statuses</li> <li>Generate comprehensive workflow results</li> <li>Return JSON representation of results</li> </ul>"},{"location":"rdetoolkit/workflows/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/workflows/#structured-errors","title":"Structured Errors","text":"<p>The module provides comprehensive error handling for structured processing failures:</p> <pre><code>from rdetoolkit.exceptions import StructuredError\nfrom rdetoolkit.workflows import run\n\ntry:\n    result = run()\nexcept StructuredError as e:\n    print(f\"Structured processing error: {e}\")\n    # Handle specific structured errors\nexcept Exception as e:\n    print(f\"General error: {e}\")\n    # Handle other exceptions\n</code></pre>"},{"location":"rdetoolkit/workflows/#error-status-creation","title":"Error Status Creation","text":"<p>Failed processing creates detailed error status information:</p> <pre><code># Error status structure\n{\n    \"run_id\": \"0\",\n    \"title\": \"Structured Process Failed: Invoice\",\n    \"status\": \"failed\",\n    \"mode\": \"Invoice\",\n    \"error_code\": 999,\n    \"error_message\": \"Detailed error description\",\n    \"stacktrace\": \"Full stack trace\",\n    \"target\": \"processed_file1.txt,processed_file2.txt\"\n}\n</code></pre>"},{"location":"rdetoolkit/workflows/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always handle exceptions in custom dataset functions:    <pre><code>def safe_custom_processor(srcpaths, resource_paths):\n    try:\n        # Your processing logic\n        pass\n    except Exception as e:\n        print(f\"Custom processing failed: {e}\")\n        # Log error but don't re-raise to continue workflow\n</code></pre></p> </li> <li> <p>Use appropriate configuration for error tolerance:    <pre><code># For production: strict error handling\nconfig = Config(\n    multidata_tile=MultiDataTileSettings(ignore_errors=False)\n)\n\n# For development: continue on errors\nconfig = Config(\n    multidata_tile=MultiDataTileSettings(ignore_errors=True)\n)\n</code></pre></p> </li> <li> <p>Validate inputs in custom functions:    <pre><code>def validated_processor(srcpaths, resource_paths):\n    if not resource_paths.rawfiles:\n        print(\"No raw files to process\")\n        return\n\n    for raw_file in resource_paths.rawfiles:\n        if not raw_file.exists():\n            print(f\"Raw file not found: {raw_file}\")\n            continue\n        # Process file...\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/workflows/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory Management: The module uses iterators to avoid loading all data tiles into memory simultaneously</li> <li>Progress Tracking: Built-in progress bars provide real-time processing feedback</li> <li>Parallel Processing: Consider using multiprocessing for CPU-intensive custom functions</li> <li>Error Recovery: Configure error tolerance based on your processing requirements</li> </ul>"},{"location":"rdetoolkit/workflows/#integration-examples","title":"Integration Examples","text":""},{"location":"rdetoolkit/workflows/#with-configuration-management","title":"With Configuration Management","text":"<pre><code>from rdetoolkit.workflows import run\nfrom rdetoolkit.models.config import load_config, Config\nfrom pathlib import Path\n\n# Load configuration from file\nconfig_path = Path(\"config/custom_config.yaml\")\nconfig = load_config(str(config_path))\n\n# Modify configuration programmatically\nconfig.system.save_raw = True\nconfig.system.save_thumbnail_image = False\n\nresult = run(config=config)\n</code></pre>"},{"location":"rdetoolkit/workflows/#with-logging-integration","title":"with Logging Integration","text":"<pre><code>import logging\nfrom rdetoolkit.workflows import run\n\n# Setup custom logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('workflow.log'),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger(__name__)\n\ndef logged_processor(srcpaths, resource_paths):\n    logger.info(f\"Processing {len(resource_paths.rawfiles)} files\")\n    # Your processing logic\n    logger.info(\"Processing completed successfully\")\n\nresult = run(custom_dataset_function=logged_processor)\n</code></pre>"},{"location":"rdetoolkit/workflows/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - For detailed configuration options</li> <li>Core Module - For directory management and file operations</li> <li>Models - For data type definitions and structures</li> <li>Mode Processing - For specific processing mode implementations</li> </ul>"},{"location":"rdetoolkit/artifact/report/","title":"Report Module","text":"<p>The <code>rdetoolkit.artifact.report</code> module provides comprehensive report generation and code analysis functionality for RDE (Research Data Exchange) artifact creation. This module offers security scanning, external communication detection, and automated markdown report generation capabilities.</p>"},{"location":"rdetoolkit/artifact/report/#overview","title":"Overview","text":"<p>The report module offers specialized functionality for artifact analysis and reporting:</p> <ul> <li>Security Scanning: Detection of common security vulnerabilities in Python code</li> <li>External Communication Analysis: Identification of external network communication patterns</li> <li>Report Generation: Automated markdown report creation with customizable templates</li> <li>Code Analysis: Pattern-based code scanning with context extraction</li> <li>Flexible Architecture: Interface-based design supporting multiple scanner types</li> <li>Logging Integration: Comprehensive error logging and debugging support</li> </ul>"},{"location":"rdetoolkit/artifact/report/#classes","title":"Classes","text":""},{"location":"rdetoolkit/artifact/report/#templatemarkdownreportgenerator","title":"TemplateMarkdownReportGenerator","text":"<p>A report generator that creates markdown reports using customizable templates with data substitution.</p>"},{"location":"rdetoolkit/artifact/report/#constructor","title":"Constructor","text":"<pre><code>TemplateMarkdownReportGenerator(template_str: str | None = None) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>template_str</code> (str | None): Custom template string (optional, uses default template if None)</li> </ul> <p>Attributes:</p> <ul> <li><code>template_str</code> (str): The template string with placeholder variables</li> <li><code>template</code> (Template): Python Template object for variable substitution</li> <li><code>text</code> (str): Generated report text after calling generate()</li> </ul> <p>Default Template Structure:</p> <ul> <li>Execution date and status information</li> <li>Dockerfile and requirements file status</li> <li>Included directories listing</li> <li>Code security scan results</li> <li>External communication check results</li> </ul>"},{"location":"rdetoolkit/artifact/report/#templatemarkdownreportgenerator-methods","title":"TemplateMarkdownReportGenerator Methods","text":""},{"location":"rdetoolkit/artifact/report/#generate","title":"generate","text":"<p>Generate a report string based on provided data.</p> <pre><code>def generate(self, data: ReportItem) -&gt; str\n</code></pre> <p>Parameters:</p> <ul> <li><code>data</code> (ReportItem): Object containing report data including scan results and metadata</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Generated report as markdown string</li> </ul> <p>Report Components:</p> <ul> <li>Dockerfile status (exists/not found with path)</li> <li>Requirements file status (exists/not found with path)</li> <li>List of included directories/files</li> <li>Security vulnerability analysis results</li> <li>External communication code snippets</li> </ul>"},{"location":"rdetoolkit/artifact/report/#save","title":"save","text":"<p>Save the generated report to a specified file path.</p> <pre><code>def save(self, output_path: str | Path) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>output_path</code> (str | Path): Path where the report will be saved</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If no report has been generated (text is empty)</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.artifact.report import TemplateMarkdownReportGenerator\nfrom rdetoolkit.models.reports import ReportItem\nfrom pathlib import Path\n\n# Create generator with default template\ngenerator = TemplateMarkdownReportGenerator()\n\n# Create custom template\ncustom_template = \"\"\"\n# Security Analysis Report\n\n**Date:** $exec_date\n\n## Infrastructure\n- Dockerfile: $dockerfile_status\n- Requirements: $requirements_status\n\n## Vulnerabilities\n$vuln_results\n\n## External Communications\n$ext_comm_results\n\"\"\"\n\ncustom_generator = TemplateMarkdownReportGenerator(custom_template)\n\n# Generate report (assuming report_data is populated)\nreport_text = generator.generate(report_data)\n\n# Save report\ngenerator.save(\"reports/security_analysis.md\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#codesecurityscanner","title":"CodeSecurityScanner","text":"<p>A security scanner that detects common vulnerabilities in Python code using pattern matching.</p>"},{"location":"rdetoolkit/artifact/report/#codesecurityscanner-constructor","title":"CodeSecurityScanner Constructor","text":"<pre><code>CodeSecurityScanner(source_dir: str | Path)\n</code></pre> <p>Parameters:</p> <ul> <li><code>source_dir</code> (str | Path): Directory path to scan for Python files</li> </ul> <p>Attributes:</p> <ul> <li><code>source_dir</code> (Path): Source directory for scanning</li> <li><code>results</code> (list[CodeSnippet]): List of detected vulnerability snippets</li> <li><code>_vuln_patterns</code> (tuple): Predefined vulnerability patterns to detect</li> </ul> <p>Vulnerability Patterns Detected:</p> <ul> <li><code>eval()</code> usage - Arbitrary code execution risk</li> <li><code>os.system()</code> usage - Command injection vulnerabilities</li> <li><code>subprocess</code> calls - Command injection risks</li> <li><code>pickle.load()</code> usage - Untrusted data deserialization</li> <li><code>mktemp()</code> usage - Race condition risks</li> <li>SQL injection via string formatting</li> </ul>"},{"location":"rdetoolkit/artifact/report/#codesecurityscanner-methods","title":"CodeSecurityScanner Methods","text":""},{"location":"rdetoolkit/artifact/report/#scan_file","title":"scan_file","text":"<p>Scan a single file for security vulnerabilities.</p> <pre><code>def scan_file(self, file_path: Path) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code> (Path): Path to the file to scan</li> </ul> <p>Behavior:</p> <ul> <li>Reads file line by line</li> <li>Searches for vulnerability patterns using regex</li> <li>Extracts code snippets with context (3 lines before, 4 lines after)</li> <li>Stores results in internal results list</li> <li>Logs errors if file cannot be read</li> </ul>"},{"location":"rdetoolkit/artifact/report/#scan","title":"scan","text":"<p>Scan the entire source directory for Python files.</p> <pre><code>def scan(self) -&gt; list[CodeSnippet]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[CodeSnippet]</code>: List of detected vulnerability code snippets</li> </ul> <p>Behavior:</p> <ul> <li>Recursively traverses source directory</li> <li>Excludes \"venv\" and \"site-packages\" directories</li> <li>Processes only Python (.py) files</li> <li>Returns accumulated scan results</li> </ul>"},{"location":"rdetoolkit/artifact/report/#get_results","title":"get_results","text":"<p>Retrieve the current scan results.</p> <pre><code>def get_results(self) -&gt; list[CodeSnippet]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[CodeSnippet]</code>: List of detected code snippets</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.artifact.report import CodeSecurityScanner\nfrom pathlib import Path\n\n# Create scanner\nscanner = CodeSecurityScanner(\"src/myproject\")\n\n# Scan all Python files\nvulnerabilities = scanner.scan()\n\n# Process results\nfor vuln in vulnerabilities:\n    print(f\"Vulnerability in {vuln.file_path}:\")\n    print(f\"Description: {vuln.description}\")\n    print(f\"Code snippet:\\n{vuln.snippet}\")\n    print(\"-\" * 50)\n\n# Scan specific file\nscanner.scan_file(Path(\"src/myproject/utils.py\"))\nspecific_results = scanner.get_results()\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#externalconnscanner","title":"ExternalConnScanner","text":"<p>A scanner that detects external communication patterns in Python code.</p>"},{"location":"rdetoolkit/artifact/report/#externalconnscanner-constructor","title":"ExternalConnScanner Constructor","text":"<pre><code>ExternalConnScanner(source_dir: str | Path)\n</code></pre> <p>Parameters:</p> <ul> <li><code>source_dir</code> (str | Path): Directory path to scan for external communication</li> </ul> <p>Attributes:</p> <ul> <li><code>source_dir</code> (Path): Source directory for scanning</li> <li><code>external_comm_packages</code> (list[str]): List of packages that indicate external communication</li> </ul> <p>Monitored Packages:</p> <ul> <li><code>requests</code>, <code>urllib</code>, <code>urllib3</code></li> <li><code>httplib</code>, <code>http.client</code></li> <li><code>socket</code>, <code>ftplib</code>, <code>telnetlib</code></li> <li><code>smtplib</code>, <code>aiohttp</code>, <code>httpx</code></li> <li><code>pycurl</code></li> </ul>"},{"location":"rdetoolkit/artifact/report/#externalconnscanner-methods","title":"ExternalConnScanner Methods","text":""},{"location":"rdetoolkit/artifact/report/#scan_directory","title":"scan_directory","text":"<p>Scan the source directory for external communication patterns.</p> <pre><code>def scan(self) -&gt; list[CodeSnippet]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[CodeSnippet]</code>: List of code snippets containing external communication</li> </ul> <p>Behavior:</p> <ul> <li>Builds regex patterns for import statements and package usage</li> <li>Scans Python files for pattern matches</li> <li>Extracts code snippets with line numbers and context</li> <li>Excludes \"venv\" and \"site-packages\" directories</li> <li>Returns collected code snippets</li> </ul> <p>Pattern Detection:</p> <ul> <li>Import statements (<code>import requests</code>)</li> <li>From imports (<code>from urllib import request</code>)</li> <li>Aliased imports (<code>import requests as req</code>)</li> <li>Package usage (<code>requests.get()</code>)</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.artifact.report import ExternalConnScanner\n\n# Create scanner\next_scanner = ExternalConnScanner(\"src/myproject\")\n\n# Scan for external communication\nexternal_comms = ext_scanner.scan()\n\n# Process results\nfor comm in external_comms:\n    print(f\"External communication found in {comm.file_path}:\")\n    print(f\"Code snippet:\\n{comm.snippet}\")\n    print(\"-\" * 50)\n\n# Check if any external communications were found\nif external_comms:\n    print(f\"Found {len(external_comms)} instances of external communication\")\nelse:\n    print(\"No external communication detected\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#functions","title":"Functions","text":""},{"location":"rdetoolkit/artifact/report/#get_scanner","title":"get_scanner","text":"<p>Factory function to create scanner instances based on type.</p> <pre><code>def get_scanner(scanner_type: Literal[\"vulnerability\", \"external\"], source_dir: str | Path) -&gt; ICodeScanner\n</code></pre> <p>Parameters:</p> <ul> <li><code>scanner_type</code> (Literal[\"vulnerability\", \"external\"]): Type of scanner to create</li> <li><code>source_dir</code> (str | Path): Directory to scan</li> </ul> <p>Returns:</p> <ul> <li><code>ICodeScanner</code>: Instance of the appropriate scanner</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If scanner_type is not \"vulnerability\" or \"external\"</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.artifact.report import get_scanner\n\n# Create vulnerability scanner\nvuln_scanner = get_scanner(\"vulnerability\", \"src/myproject\")\nvulnerabilities = vuln_scanner.scan()\n\n# Create external communication scanner\next_scanner = get_scanner(\"external\", \"src/myproject\")\nexternal_comms = ext_scanner.scan()\n\n# Process both types of results\nprint(f\"Found {len(vulnerabilities)} vulnerabilities\")\nprint(f\"Found {len(external_comms)} external communications\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#integrated-analysis-workflow","title":"Integrated Analysis Workflow","text":""},{"location":"rdetoolkit/artifact/report/#basic-security-analysis","title":"Basic Security Analysis","text":"<pre><code>from rdetoolkit.artifact.report import (\n    TemplateMarkdownReportGenerator,\n    get_scanner\n)\nfrom rdetoolkit.models.reports import ReportItem, CodeSnippet\nfrom datetime import datetime\nfrom pathlib import Path\n\ndef analyze_project_security(project_dir: str, output_file: str):\n    \"\"\"Perform comprehensive security analysis of a project.\"\"\"\n\n    project_path = Path(project_dir)\n\n    # Create scanners\n    vuln_scanner = get_scanner(\"vulnerability\", project_path)\n    ext_scanner = get_scanner(\"external\", project_path)\n\n    # Perform scans\n    print(\"Scanning for vulnerabilities...\")\n    vulnerabilities = vuln_scanner.scan()\n\n    print(\"Scanning for external communications...\")\n    external_comms = ext_scanner.scan()\n\n    # Check for infrastructure files\n    dockerfile_path = project_path / \"Dockerfile\"\n    requirements_path = project_path / \"requirements.txt\"\n\n    # Collect directory information\n    include_dirs = [\n        str(p.relative_to(project_path))\n        for p in project_path.rglob(\"*\")\n        if p.is_dir() and not any(exclude in str(p) for exclude in [\"__pycache__\", \".git\", \"venv\"])\n    ]\n\n    # Create report data\n    report_data = ReportItem(\n        exec_date=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        dockerfile_path=str(dockerfile_path) if dockerfile_path.exists() else None,\n        requirements_path=str(requirements_path) if requirements_path.exists() else None,\n        include_dirs=include_dirs,\n        code_security_scan_results=vulnerabilities,\n        code_ext_requests_scan_results=external_comms\n    )\n\n    # Generate report\n    generator = TemplateMarkdownReportGenerator()\n    report_text = generator.generate(report_data)\n    generator.save(output_file)\n\n    print(f\"Analysis complete. Report saved to: {output_file}\")\n    print(f\"Vulnerabilities found: {len(vulnerabilities)}\")\n    print(f\"External communications found: {len(external_comms)}\")\n\n# Usage\nanalyze_project_security(\"src/myproject\", \"reports/security_analysis.md\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#custom-template-usage","title":"Custom Template Usage","text":"<pre><code>from rdetoolkit.artifact.report import TemplateMarkdownReportGenerator\n\ndef create_custom_security_report(data, output_path):\n    \"\"\"Create a security report with custom formatting.\"\"\"\n\n    custom_template = \"\"\"\n# \ud83d\udd12 Security Analysis Report\n\n**Analysis Date:** $exec_date\n\n---\n\n## \ud83d\udccb Project Infrastructure\n\n| Component    | Status               |\n| ------------ | -------------------- |\n| Dockerfile   | $dockerfile_status   |\n| Requirements | $requirements_status |\n\n## \ud83d\udcc1 Project Structure\n\n$included_dirs\n\n---\n\n## \u26a0\ufe0f Security Vulnerabilities\n\n$vuln_results\n\n---\n\n## \ud83c\udf10 External Communications\n\n$ext_comm_results\n\n---\n\n**Report Generated by RDE Toolkit**\n\"\"\"\n\n    generator = TemplateMarkdownReportGenerator(custom_template)\n    report = generator.generate(data)\n    generator.save(output_path)\n\n    return report\n\n# Usage with custom template\n# custom_report = create_custom_security_report(report_data, \"custom_report.md\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#scanner-configuration-and-filtering","title":"Scanner Configuration and Filtering","text":"<pre><code>from rdetoolkit.artifact.report import CodeSecurityScanner, ExternalConnScanner\n\ndef filtered_security_scan(source_dir: str, exclude_patterns: list[str] = None):\n    \"\"\"Perform security scan with file filtering.\"\"\"\n\n    exclude_patterns = exclude_patterns or [\"test_\", \"_test.py\", \"tests/\"]\n\n    # Create scanner\n    scanner = CodeSecurityScanner(source_dir)\n\n    # Get all Python files\n    source_path = Path(source_dir)\n    python_files = list(source_path.rglob(\"*.py\"))\n\n    # Filter files\n    filtered_files = [\n        f for f in python_files\n        if not any(pattern in str(f) for pattern in exclude_patterns)\n    ]\n\n    # Scan filtered files\n    for file_path in filtered_files:\n        scanner.scan_file(file_path)\n\n    return scanner.get_results()\n\ndef analyze_external_dependencies(source_dir: str):\n    \"\"\"Analyze external dependencies and categorize them.\"\"\"\n\n    ext_scanner = ExternalConnScanner(source_dir)\n    external_comms = ext_scanner.scan()\n\n    # Categorize by package type\n    categories = {\n        \"HTTP\": [\"requests\", \"urllib\", \"httplib\", \"aiohttp\", \"httpx\"],\n        \"Network\": [\"socket\", \"ftplib\", \"telnetlib\"],\n        \"Email\": [\"smtplib\"],\n        \"Other\": [\"pycurl\"]\n    }\n\n    categorized = {cat: [] for cat in categories}\n\n    for comm in external_comms:\n        snippet_lower = comm.snippet.lower()\n        categorized_item = False\n\n        for category, packages in categories.items():\n            if any(pkg in snippet_lower for pkg in packages):\n                categorized[category].append(comm)\n                categorized_item = True\n                break\n\n        if not categorized_item:\n            categorized[\"Other\"].append(comm)\n\n    return categorized\n\n# Usage\nvulnerabilities = filtered_security_scan(\"src/myproject\", [\"test_\", \"demo_\"])\ndependencies = analyze_external_dependencies(\"src/myproject\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/artifact/report/#scanner-error-management","title":"Scanner Error Management","text":"<pre><code>from rdetoolkit.artifact.report import CodeSecurityScanner\nimport logging\n\ndef robust_security_scan(source_dir: str):\n    \"\"\"Perform security scan with comprehensive error handling.\"\"\"\n\n    try:\n        scanner = CodeSecurityScanner(source_dir)\n\n        # Verify source directory exists\n        if not Path(source_dir).exists():\n            raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n\n        # Perform scan\n        results = scanner.scan()\n\n        if not results:\n            print(\"No security vulnerabilities detected\")\n        else:\n            print(f\"Found {len(results)} potential security issues\")\n\n        return results\n\n    except FileNotFoundError as e:\n        logging.error(f\"Directory error: {e}\")\n        return []\n    except Exception as e:\n        logging.error(f\"Scan error: {e}\")\n        return []\n\ndef safe_report_generation(data, output_path):\n    \"\"\"Generate report with error handling.\"\"\"\n\n    try:\n        generator = TemplateMarkdownReportGenerator()\n\n        # Validate data\n        if not hasattr(data, 'exec_date'):\n            raise ValueError(\"Invalid report data: missing exec_date\")\n\n        # Generate and save\n        report = generator.generate(data)\n        generator.save(output_path)\n\n        return True\n\n    except FileNotFoundError as e:\n        logging.error(f\"Report generation error: {e}\")\n        return False\n    except Exception as e:\n        logging.error(f\"Unexpected error: {e}\")\n        return False\n\n# Usage\nresults = robust_security_scan(\"src/myproject\")\nsuccess = safe_report_generation(report_data, \"analysis.md\")\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#best-practices","title":"Best Practices","text":""},{"location":"rdetoolkit/artifact/report/#scanner-usage","title":"Scanner Usage","text":"<pre><code># Always verify directory exists before scanning\nif Path(source_dir).exists():\n    scanner = get_scanner(\"vulnerability\", source_dir)\n    results = scanner.scan()\n\n# Handle empty results gracefully\nvulnerabilities = vuln_scanner.scan()\nif not vulnerabilities:\n    print(\"No vulnerabilities detected\")\n\n# Use appropriate scanner for the task\nvuln_scanner = get_scanner(\"vulnerability\", source_dir)  # For security issues\next_scanner = get_scanner(\"external\", source_dir)       # For external communications\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#report-generation","title":"Report Generation","text":"<pre><code># Always generate before saving\ngenerator = TemplateMarkdownReportGenerator()\nreport_text = generator.generate(data)  # Generate first\ngenerator.save(output_path)             # Then save\n\n# Validate template variables\nrequired_vars = [\"exec_date\", \"dockerfile_status\", \"requirements_status\"]\nfor var in required_vars:\n    if f\"${var}\" not in generator.template_str:\n        print(f\"Warning: Template missing variable: {var}\")\n\n# Use meaningful output paths\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_path = f\"reports/security_analysis_{timestamp}.md\"\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#integration-with-rde-workflows","title":"Integration with RDE Workflows","text":"<pre><code># Integration with RDE processing\nfrom rdetoolkit.artifact.report import get_scanner, TemplateMarkdownReportGenerator\n\ndef integrate_security_analysis(workflow_results):\n    \"\"\"Integrate security analysis into RDE workflow.\"\"\"\n\n    # Scan processed code\n    if workflow_results.get(\"processed_code_dir\"):\n        vuln_scanner = get_scanner(\"vulnerability\", workflow_results[\"processed_code_dir\"])\n        vulnerabilities = vuln_scanner.scan()\n\n        # Add to workflow results\n        workflow_results[\"security_scan\"] = {\n            \"vulnerabilities_count\": len(vulnerabilities),\n            \"vulnerabilities\": vulnerabilities\n        }\n\n    return workflow_results\n</code></pre>"},{"location":"rdetoolkit/artifact/report/#see-also","title":"See Also","text":"<ul> <li>Models - Reports - For ReportItem and CodeSnippet data structures</li> <li>Interfaces - Report - For ICodeScanner and IReportGenerator interfaces</li> <li>RDE Logger - For logging functionality used in scanners</li> <li>Artifact Processing - For artifact creation and management workflows</li> <li>Usage - CLI - For command-line report generation examples</li> </ul>"},{"location":"rdetoolkit/cmd/archive/","title":"Archive Module","text":"<p>The <code>rdetoolkit.cmd.archive</code> module provides functionality for creating project archives with automated security scanning and report generation. This module is designed to package Python projects into compressed archives while analyzing code for potential security vulnerabilities and external dependencies.</p>"},{"location":"rdetoolkit/cmd/archive/#overview","title":"Overview","text":"<p>The archive module offers comprehensive project archiving capabilities with built-in analysis features:</p> <ul> <li>Project Archiving: Create compressed archives of Python projects with configurable exclusion patterns</li> <li>Security Scanning: Automated vulnerability detection and external connection analysis</li> <li>Report Generation: Markdown reports with scan results and project metadata</li> <li>File Detection: Automatic detection of important project files (Dockerfile, requirements.txt)</li> </ul>"},{"location":"rdetoolkit/cmd/archive/#classes","title":"Classes","text":""},{"location":"rdetoolkit/cmd/archive/#createartifactcommand","title":"CreateArtifactCommand","text":"<p>A command class that orchestrates the complete artifact creation process, including archiving, scanning, and report generation.</p>"},{"location":"rdetoolkit/cmd/archive/#constructor","title":"Constructor","text":"<pre><code>CreateArtifactCommand(source_dir: pathlib.Path, *, output_archive_path: pathlib.Path | None = None, exclude_patterns: list[str] | None = None)\n</code></pre> <p>Parameters: - <code>source_dir</code> (pathlib.Path): Source directory containing the project to archive - <code>output_archive_path</code> (pathlib.Path | None): Output path for the archive file. If None, generates a default filename with timestamp and UUID - <code>exclude_patterns</code> (list[str] | None): List of patterns to exclude from archiving. Defaults to <code>['.*', 'venv', '.venv', 'site-packages']</code></p>"},{"location":"rdetoolkit/cmd/archive/#class-constants","title":"Class Constants","text":"<p>The class defines several emoji markers for output formatting:</p> <ul> <li><code>MARK_SUCCESS</code> = \"\u2705\": Success operations</li> <li><code>MARK_WARNING</code> = \"\u26a0\ufe0f\": Warning messages</li> <li><code>MARK_ERROR</code> = \"\ud83d\udd25\": Error conditions</li> <li><code>MARK_INFO</code> = \"\ud83d\udccc\": Information messages</li> <li><code>MARK_SCAN</code> = \"\ud83d\udd0d\": Scanning operations</li> <li><code>MARK_ARCHIVE</code> = \"\ud83d\udce6\": Archive operations</li> </ul>"},{"location":"rdetoolkit/cmd/archive/#attributes","title":"Attributes","text":"<ul> <li><code>source_dir</code> (pathlib.Path): The source directory to archive</li> <li><code>output_archive_path</code> (pathlib.Path): Path where the archive will be created</li> <li><code>exclude_patterns</code> (list[str]): Patterns to exclude during archiving</li> <li><code>template_report_generator</code> (TemplateMarkdownReportGenerator): Report generator instance</li> </ul>"},{"location":"rdetoolkit/cmd/archive/#methods","title":"Methods","text":""},{"location":"rdetoolkit/cmd/archive/#invoke","title":"invoke()","text":"<p>Execute the complete artifact creation process.</p> <pre><code>def invoke() -&gt; None\n</code></pre> <p>Raises: - <code>click.Abort</code>: If any step in the process fails</p> <p>Example: <pre><code>import pathlib\nfrom rdetoolkit.cmd.archive import CreateArtifactCommand\n\n# Create an artifact with default settings\nsource = pathlib.Path(\"/path/to/project\")\ncommand = CreateArtifactCommand(source)\ncommand.invoke()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/archive/#_check_filetarget_filename-logonone","title":"_check_file(target_filename, *, logo=None)","text":"<p>Check for the existence of a specific file in the project directory.</p> <pre><code>def _check_file(target_filename: str, *, logo: str | None = None) -&gt; str\n</code></pre> <p>Parameters: - <code>target_filename</code> (str): Name of the file to search for - <code>logo</code> (str | None): Optional emoji or symbol to display with the filename</p> <p>Returns: - <code>str</code>: Relative path to the file if found, or \"{filename} not found\" if not found</p> <p>Example: <pre><code># Check for Dockerfile\ndockerfile_path = command._check_file(\"Dockerfile\", logo=\"\ud83d\udc33\")\n</code></pre></p>"},{"location":"rdetoolkit/cmd/archive/#_check_extention_type","title":"_check_extention_type()","text":"<p>Validate the output archive file extension.</p> <pre><code>def _check_extention_type() -&gt; str\n</code></pre> <p>Returns: - <code>str</code>: The file extension without the leading dot</p> <p>Raises: - <code>click.Abort</code>: If the extension is not .zip</p>"},{"location":"rdetoolkit/cmd/archive/#_archive_target_dirfmt","title":"_archive_target_dir(fmt)","text":"<p>Create the archive file using the specified format.</p> <pre><code>def _archive_target_dir(fmt: str) -&gt; list[pathlib.Path] | None\n</code></pre> <p>Parameters: - <code>fmt</code> (str): Archive format (currently supports \"zip\")</p> <p>Returns: - <code>list[pathlib.Path] | None</code>: List of directories included in the archive</p> <p>Raises: - <code>click.Abort</code>: If archiving fails</p>"},{"location":"rdetoolkit/cmd/archive/#_scan_external_conn","title":"_scan_external_conn()","text":"<p>Scan the project for external connection references.</p> <pre><code>def _scan_external_conn() -&gt; list[CodeSnippet]\n</code></pre> <p>Returns: - <code>list[CodeSnippet]</code>: List of code snippets containing external connections</p> <p>Raises: - <code>click.Abort</code>: If scanning fails</p>"},{"location":"rdetoolkit/cmd/archive/#_scan_code_security","title":"_scan_code_security()","text":"<p>Scan the project for potential security vulnerabilities.</p> <pre><code>def _scan_code_security() -&gt; list[CodeSnippet]\n</code></pre> <p>Returns: - <code>list[CodeSnippet]</code>: List of code snippets with potential security issues</p> <p>Raises: - <code>click.Abort</code>: If scanning fails</p>"},{"location":"rdetoolkit/cmd/archive/#_generate_reportitem","title":"_generate_report(item)","text":"<p>Generate a markdown report with scan results and project information.</p> <pre><code>def _generate_report(item: ReportItem) -&gt; None\n</code></pre> <p>Parameters: - <code>item</code> (ReportItem): Report data containing scan results and metadata</p> <p>Raises: - <code>click.Abort</code>: If report generation fails</p>"},{"location":"rdetoolkit/cmd/archive/#_safe_relativep","title":"_safe_relative(p)","text":"<p>Safely convert a path to a relative path string.</p> <pre><code>def _safe_relative(p: pathlib.Path) -&gt; str\n</code></pre> <p>Parameters: - <code>p</code> (pathlib.Path): Path to convert</p> <p>Returns: - <code>str</code>: Relative path string or absolute path string if conversion fails</p>"},{"location":"rdetoolkit/cmd/archive/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/cmd/archive/#basic-project-archiving","title":"Basic Project Archiving","text":"<pre><code>import pathlib\nfrom rdetoolkit.cmd.archive import CreateArtifactCommand\n\n# Archive a project with default settings\nproject_dir = pathlib.Path(\"/path/to/my_project\")\ncommand = CreateArtifactCommand(project_dir)\ncommand.invoke()\n\n# This will create:\n# - An archive file with timestamp and UUID in the filename\n# - A markdown report with the same base name\n# - Security and external connection scan results\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#custom-archive-configuration","title":"Custom Archive Configuration","text":"<pre><code>import pathlib\nfrom rdetoolkit.cmd.archive import CreateArtifactCommand\n\n# Custom archive path and exclusion patterns\nsource_dir = pathlib.Path(\"/path/to/project\")\noutput_path = pathlib.Path(\"/output/my_project_archive.zip\")\nexclude_patterns = [\n    \".*\",           # Hidden files\n    \"venv\",         # Virtual environment\n    \"__pycache__\",  # Python cache\n    \"node_modules\", # Node.js modules\n    \"*.log\",        # Log files\n]\n\ncommand = CreateArtifactCommand(\n    source_dir,\n    output_archive_path=output_path,\n    exclude_patterns=exclude_patterns\n)\n\ncommand.invoke()\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#programmatic-archive-creation","title":"Programmatic Archive Creation","text":"<pre><code>import pathlib\nfrom datetime import datetime\nfrom rdetoolkit.cmd.archive import CreateArtifactCommand\n\ndef create_project_backup(project_path: str, backup_dir: str) -&gt; tuple[pathlib.Path, pathlib.Path]:\n    \"\"\"Create a project backup with timestamp.\"\"\"\n\n    source = pathlib.Path(project_path)\n    backup_base = pathlib.Path(backup_dir)\n\n    # Create timestamped filename\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    project_name = source.name\n    archive_name = f\"{project_name}_{timestamp}_backup.zip\"\n\n    output_path = backup_base / archive_name\n\n    # Create the archive\n    command = CreateArtifactCommand(\n        source,\n        output_archive_path=output_path,\n        exclude_patterns=[\n            \".*\",\n            \"venv\",\n            \".venv\",\n            \"__pycache__\",\n            \"*.pyc\",\n            \".git\"\n        ]\n    )\n\n    try:\n        command.invoke()\n        return output_path, output_path.with_suffix(\".md\")\n    except Exception as e:\n        print(f\"Backup failed: {e}\")\n        raise\n\n# Usage\narchive_path, report_path = create_project_backup(\n    \"/path/to/project\",\n    \"/backups\"\n)\nprint(f\"Archive created: {archive_path}\")\nprint(f\"Report created: {report_path}\")\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#batch-project-archiving","title":"Batch Project Archiving","text":"<pre><code>import pathlib\nfrom rdetoolkit.cmd.archive import CreateArtifactCommand\n\ndef archive_multiple_projects(projects_dir: str, output_dir: str):\n    \"\"\"Archive multiple projects in a directory.\"\"\"\n\n    projects_path = pathlib.Path(projects_dir)\n    output_path = pathlib.Path(output_dir)\n\n    # Ensure output directory exists\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Find all project directories (containing Python files)\n    for project_dir in projects_path.iterdir():\n        if not project_dir.is_dir():\n            continue\n\n        # Check if directory contains Python files\n        python_files = list(project_dir.rglob(\"*.py\"))\n        if not python_files:\n            continue\n\n        print(f\"Archiving project: {project_dir.name}\")\n\n        # Create archive for this project\n        archive_name = f\"{project_dir.name}_archive.zip\"\n        archive_path = output_path / archive_name\n\n        try:\n            command = CreateArtifactCommand(\n                project_dir,\n                output_archive_path=archive_path\n            )\n            command.invoke()\n            print(f\"\u2705 Successfully archived: {project_dir.name}\")\n\n        except Exception as e:\n            print(f\"\u274c Failed to archive {project_dir.name}: {e}\")\n\n# Usage\narchive_multiple_projects(\"/path/to/projects\", \"/path/to/archives\")\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/cmd/archive/#common-exceptions","title":"Common Exceptions","text":"<p>The archive module operations may raise the following exceptions:</p>"},{"location":"rdetoolkit/cmd/archive/#clickabort","title":"click.Abort","text":"<p>Raised when critical errors occur during the archiving process:</p> <pre><code>try:\n    command = CreateArtifactCommand(source_dir)\n    command.invoke()\nexcept click.Abort:\n    print(\"Archive creation was aborted due to an error\")\n    # Check console output for specific error details\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#filenotfounderror","title":"FileNotFoundError","text":"<p>May be raised if the source directory doesn't exist:</p> <pre><code>import pathlib\nfrom rdetoolkit.cmd.archive import CreateArtifactCommand\n\nsource_dir = pathlib.Path(\"/nonexistent/path\")\n\nif not source_dir.exists():\n    print(f\"Source directory does not exist: {source_dir}\")\nelse:\n    command = CreateArtifactCommand(source_dir)\n    command.invoke()\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#permissionerror","title":"PermissionError","text":"<p>May be raised if there are insufficient permissions:</p> <pre><code>try:\n    command = CreateArtifactCommand(source_dir, output_archive_path=output_path)\n    command.invoke()\nexcept PermissionError as e:\n    print(f\"Permission denied: {e}\")\n    print(\"Check write permissions for the output directory\")\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Validate paths before archiving:    <pre><code>if not source_dir.exists():\n    raise ValueError(f\"Source directory does not exist: {source_dir}\")\nif not source_dir.is_dir():\n    raise ValueError(f\"Source path is not a directory: {source_dir}\")\n</code></pre></p> </li> <li> <p>Ensure output directory exists:    <pre><code>output_path.parent.mkdir(parents=True, exist_ok=True)\n</code></pre></p> </li> <li> <p>Handle large projects gracefully:    <pre><code># Add more exclusion patterns for large projects\nexclude_patterns = [\n    \".*\",\n    \"venv\", \".venv\",\n    \"node_modules\",\n    \"__pycache__\", \"*.pyc\",\n    \"*.log\", \"*.tmp\",\n    \".git\", \".svn\",\n    \"build\", \"dist\"\n]\n</code></pre></p> </li> <li> <p>Monitor disk space:    <pre><code>import shutil\n\n# Check available disk space before archiving\nfree_space = shutil.disk_usage(output_path.parent).free\nif free_space &lt; 1_000_000_000:  # Less than 1GB\n    print(\"Warning: Low disk space available\")\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/cmd/archive/#performance-notes","title":"Performance Notes","text":"<ul> <li>The archiving process is optimized for typical Python project structures</li> <li>Large projects with many files may take significant time to scan and archive</li> <li>Exclude patterns are applied during directory traversal to improve performance</li> <li>Security scanning performance depends on the size and complexity of the codebase</li> <li>Memory usage scales with the number of files and the complexity of scan patterns</li> </ul>"},{"location":"rdetoolkit/cmd/archive/#integration-with-other-modules","title":"Integration with Other Modules","text":""},{"location":"rdetoolkit/cmd/archive/#report-generation","title":"Report Generation","text":"<p>The archive module integrates with the report generation system:</p> <pre><code>from rdetoolkit.artifact.report import TemplateMarkdownReportGenerator\nfrom rdetoolkit.models.reports import ReportItem\n\n# The CreateArtifactCommand uses these internally\n# but you can also use them directly for custom reporting\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#compression-support","title":"Compression Support","text":"<p>Archives are created using the compressed controller:</p> <pre><code>from rdetoolkit.impl.compressed_controller import get_artifact_archiver\n\n# This is used internally by CreateArtifactCommand\n# Currently supports ZIP format only\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#code-scanning","title":"Code Scanning","text":"<p>Security and external connection scanning is performed by:</p> <pre><code>from rdetoolkit.artifact.report import get_scanner\n\n# Used internally for vulnerability and external connection analysis\nscanner_vuln = get_scanner('vulnerability', source_dir)\nscanner_ext = get_scanner('external', source_dir)\n</code></pre>"},{"location":"rdetoolkit/cmd/archive/#see-also","title":"See Also","text":"<ul> <li>Artifact Report Module - For custom report generation</li> <li>Models Module - For report data structures</li> <li>Compressed Controller - For archive creation</li> <li>Configuration Guide - For project configuration options</li> </ul>"},{"location":"rdetoolkit/cmd/command/","title":"Command Module","text":"<p>The <code>rdetoolkit.cmd.command</code> module provides command-line interface functionality for RDE (Research Data Exchange) project initialization and management. This module includes commands for project setup, version display, and file generation utilities for structured RDE development environments.</p>"},{"location":"rdetoolkit/cmd/command/#overview","title":"Overview","text":"<p>The command module offers comprehensive project initialization and management capabilities:</p> <ul> <li>Project Initialization: Create complete RDE project structures with necessary directories and files</li> <li>Version Management: Display current toolkit version information</li> <li>File Generation: Automated creation of configuration files, scripts, and templates</li> <li>Container Support: Generate Docker and containerization files for reproducible environments</li> </ul>"},{"location":"rdetoolkit/cmd/command/#classes","title":"Classes","text":""},{"location":"rdetoolkit/cmd/command/#command","title":"Command","text":"<p>Extended Click command class for custom command functionality.</p>"},{"location":"rdetoolkit/cmd/command/#constructor","title":"Constructor","text":"<pre><code>Command(name: str, **attrs: Any)\n</code></pre> <p>Parameters: - <code>name</code> (str): The name of the command - <code>**attrs</code> (Any): Additional attributes passed to the parent Click command</p> <p>Inherits from: <code>click.Command</code></p>"},{"location":"rdetoolkit/cmd/command/#initcommand","title":"InitCommand","text":"<p>Command class for initializing RDE project structures with all necessary files and directories.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_1","title":"Constructor","text":"<pre><code>InitCommand()\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#class-attributes","title":"Class Attributes","text":""},{"location":"rdetoolkit/cmd/command/#default_dirs","title":"default_dirs","text":"<pre><code>default_dirs: list[Path] = [\n    Path(\"container/modules\"),\n    Path(\"container/data/inputdata\"),\n    Path(\"container/data/invoice\"),\n    Path(\"container/data/tasksupport\"),\n    Path(\"input/invoice\"),\n    Path(\"input/inputdata\"),\n    Path(\"templates/tasksupport\"),\n]\n</code></pre> <p>List of default directories created during project initialization.</p>"},{"location":"rdetoolkit/cmd/command/#methods","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#invoke","title":"invoke()","text":"<p>Execute the complete project initialization process.</p> <pre><code>def invoke() -&gt; None\n</code></pre> <p>Raises: - <code>click.Abort</code>: If any step in the initialization process fails</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import InitCommand\n\n# Initialize a new RDE project\ninit_cmd = InitCommand()\ninit_cmd.invoke()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#private-methods","title":"Private Methods","text":"<p>The class includes several private methods for specific file generation tasks:</p> <ul> <li><code>_info_msg(msg: str) -&gt; None</code>: Display information messages</li> <li><code>_success_msg(msg: str) -&gt; None</code>: Display success messages in green</li> <li><code>_error_msg(msg: str) -&gt; None</code>: Display error messages in red</li> </ul>"},{"location":"rdetoolkit/cmd/command/#versioncommand","title":"VersionCommand","text":"<p>Command class for displaying the current toolkit version.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_2","title":"Constructor","text":"<pre><code>VersionCommand()\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#invoke_1","title":"invoke()","text":"<p>Display the current RDEToolkit version.</p> <pre><code>def invoke() -&gt; None\n</code></pre> <p>Example: <pre><code>from rdetoolkit.cmd.command import VersionCommand\n\n# Display version information\nversion_cmd = VersionCommand()\nversion_cmd.invoke()  # Prints the current version\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#dockerfilegenerator","title":"DockerfileGenerator","text":"<p>Generator class for creating Dockerfile configurations.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_3","title":"Constructor","text":"<pre><code>DockerfileGenerator(path: str | Path = \"Dockerfile\")\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path where the Dockerfile will be created. Defaults to \"Dockerfile\"</p>"},{"location":"rdetoolkit/cmd/command/#attributes","title":"Attributes","text":"<ul> <li><code>path</code> (str | Path): The target path for the generated Dockerfile</li> </ul>"},{"location":"rdetoolkit/cmd/command/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#generate","title":"generate()","text":"<p>Generate a Dockerfile with standard RDE configuration.</p> <pre><code>def generate() -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of strings representing the Dockerfile content</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import DockerfileGenerator\nfrom pathlib import Path\n\n# Generate Dockerfile in custom location\ngenerator = DockerfileGenerator(Path(\"docker/Dockerfile\"))\ncontent = generator.generate()\nprint(\"Generated Dockerfile with\", len(content), \"lines\")\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#requirementstxtgenerator","title":"RequirementsTxtGenerator","text":"<p>Generator class for creating requirements.txt files with RDEToolkit dependencies.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_4","title":"Constructor","text":"<pre><code>RequirementsTxtGenerator(path: str | Path = \"requirements.txt\")\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path where the requirements.txt will be created. Defaults to \"requirements.txt\"</p>"},{"location":"rdetoolkit/cmd/command/#attributes_1","title":"Attributes","text":"<ul> <li><code>path</code> (str | Path): The target path for the generated requirements.txt</li> </ul>"},{"location":"rdetoolkit/cmd/command/#methods_3","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#generate_1","title":"generate()","text":"<p>Generate a requirements.txt file with RDEToolkit dependency.</p> <pre><code>def generate() -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of strings representing the requirements.txt content</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import RequirementsTxtGenerator\n\n# Generate requirements.txt\ngenerator = RequirementsTxtGenerator(\"container/requirements.txt\")\ncontent = generator.generate()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#invoiceschemajsongenerator","title":"InvoiceSchemaJsonGenerator","text":"<p>Generator class for creating invoice schema JSON files.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_5","title":"Constructor","text":"<pre><code>InvoiceSchemaJsonGenerator(path: str | Path = \"invoice.schema.json\")\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path where the schema file will be created. Defaults to \"invoice.schema.json\"</p>"},{"location":"rdetoolkit/cmd/command/#attributes_2","title":"Attributes","text":"<ul> <li><code>path</code> (str | Path): The target path for the generated schema file</li> </ul>"},{"location":"rdetoolkit/cmd/command/#methods_4","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#generate_2","title":"generate()","text":"<p>Generate an invoice schema JSON file based on RDE specifications.</p> <pre><code>def generate() -&gt; dict[str, Any]\n</code></pre> <p>Returns: - <code>dict[str, Any]</code>: Dictionary containing the generated schema structure</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import InvoiceSchemaJsonGenerator\n\n# Generate invoice schema\ngenerator = InvoiceSchemaJsonGenerator(\"schemas/invoice.schema.json\")\nschema = generator.generate()\nprint(f\"Generated schema with {len(schema)} top-level keys\")\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#metadatadefjsongenerator","title":"MetadataDefJsonGenerator","text":"<p>Generator class for creating metadata definition JSON files.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_6","title":"Constructor","text":"<pre><code>MetadataDefJsonGenerator(path: str | Path = \"metadata-def.json\")\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path where the metadata definition will be created. Defaults to \"metadata-def.json\"</p>"},{"location":"rdetoolkit/cmd/command/#attributes_3","title":"Attributes","text":"<ul> <li><code>path</code> (str | Path): The target path for the generated metadata definition</li> </ul>"},{"location":"rdetoolkit/cmd/command/#methods_5","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#generate_3","title":"generate()","text":"<p>Generate a metadata definition JSON file.</p> <pre><code>def generate() -&gt; dict[str, Any]\n</code></pre> <p>Returns: - <code>dict[str, Any]</code>: Dictionary containing the metadata definition (initially empty)</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import MetadataDefJsonGenerator\n\n# Generate metadata definition\ngenerator = MetadataDefJsonGenerator(\"config/metadata-def.json\")\nmetadata = generator.generate()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#invoicejsongenerator","title":"InvoiceJsonGenerator","text":"<p>Generator class for creating invoice JSON files.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_7","title":"Constructor","text":"<pre><code>InvoiceJsonGenerator(path: str | Path = \"invoice.json\")\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path where the invoice file will be created. Defaults to \"invoice.json\"</p>"},{"location":"rdetoolkit/cmd/command/#attributes_4","title":"Attributes","text":"<ul> <li><code>path</code> (str | Path): The target path for the generated invoice file</li> </ul>"},{"location":"rdetoolkit/cmd/command/#methods_6","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#generate_4","title":"generate()","text":"<p>Generate an invoice JSON file with default RDE structure.</p> <pre><code>def generate() -&gt; dict[str, Any]\n</code></pre> <p>Returns: - <code>dict[str, Any]</code>: Dictionary containing the generated invoice data</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import InvoiceJsonGenerator\n\n# Generate invoice file\ngenerator = InvoiceJsonGenerator(\"data/invoice.json\")\ninvoice_data = generator.generate()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#mainscriptgenerator","title":"MainScriptGenerator","text":"<p>Generator class for creating main Python script templates.</p>"},{"location":"rdetoolkit/cmd/command/#constructor_8","title":"Constructor","text":"<pre><code>MainScriptGenerator(path: str | Path)\n</code></pre> <p>Parameters: - <code>path</code> (str | Path): Path where the main script will be created</p>"},{"location":"rdetoolkit/cmd/command/#attributes_5","title":"Attributes","text":"<ul> <li><code>path</code> (str | Path): The target path for the generated main script</li> </ul>"},{"location":"rdetoolkit/cmd/command/#methods_7","title":"Methods","text":""},{"location":"rdetoolkit/cmd/command/#generate_5","title":"generate()","text":"<p>Generate a main Python script template for RDE workflows.</p> <pre><code>def generate() -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of strings representing the script content</p> <p>Example: <pre><code>from rdetoolkit.cmd.command import MainScriptGenerator\n\n# Generate main script\ngenerator = MainScriptGenerator(\"container/main.py\")\nscript_lines = generator.generate()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/command/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/cmd/command/#basic-project-initialization","title":"Basic Project Initialization","text":"<pre><code>from rdetoolkit.cmd.command import InitCommand\n\n# Initialize a complete RDE project\ndef setup_new_project():\n    \"\"\"Set up a new RDE project with all necessary files.\"\"\"\n    try:\n        init_command = InitCommand()\n        init_command.invoke()\n        print(\"Project initialized successfully!\")\n    except Exception as e:\n        print(f\"Initialization failed: {e}\")\n\nsetup_new_project()\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#custom-file-generation","title":"Custom File Generation","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.cmd.command import (\n    DockerfileGenerator,\n    RequirementsTxtGenerator,\n    MainScriptGenerator\n)\n\ndef create_custom_project_structure(base_path: str):\n    \"\"\"Create a custom project structure with generated files.\"\"\"\n\n    base = Path(base_path)\n    base.mkdir(parents=True, exist_ok=True)\n\n    # Generate Dockerfile\n    dockerfile_gen = DockerfileGenerator(base / \"Dockerfile\")\n    dockerfile_gen.generate()\n    print(f\"Created Dockerfile at {base / 'Dockerfile'}\")\n\n    # Generate requirements.txt\n    req_gen = RequirementsTxtGenerator(base / \"requirements.txt\")\n    req_gen.generate()\n    print(f\"Created requirements.txt at {base / 'requirements.txt'}\")\n\n    # Generate main script\n    main_gen = MainScriptGenerator(base / \"main.py\")\n    main_gen.generate()\n    print(f\"Created main.py at {base / 'main.py'}\")\n\n# Usage\ncreate_custom_project_structure(\"my_rde_project\")\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#batch-file-generation-for-multiple-environments","title":"Batch File Generation for Multiple Environments","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.cmd.command import (\n    DockerfileGenerator,\n    RequirementsTxtGenerator,\n    InvoiceSchemaJsonGenerator,\n    MetadataDefJsonGenerator,\n    InvoiceJsonGenerator\n)\n\ndef setup_multiple_environments(environments: list[str], base_dir: str):\n    \"\"\"Set up multiple RDE environments with generated files.\"\"\"\n\n    base_path = Path(base_dir)\n\n    for env in environments:\n        env_path = base_path / env\n        env_path.mkdir(parents=True, exist_ok=True)\n\n        print(f\"Setting up environment: {env}\")\n\n        # Create container files\n        container_path = env_path / \"container\"\n        container_path.mkdir(exist_ok=True)\n\n        # Generate Docker files\n        DockerfileGenerator(container_path / \"Dockerfile\").generate()\n        RequirementsTxtGenerator(container_path / \"requirements.txt\").generate()\n\n        # Create data directories and files\n        data_path = container_path / \"data\"\n        invoice_path = data_path / \"invoice\"\n        tasksupport_path = data_path / \"tasksupport\"\n\n        invoice_path.mkdir(parents=True, exist_ok=True)\n        tasksupport_path.mkdir(parents=True, exist_ok=True)\n\n        # Generate JSON files\n        InvoiceJsonGenerator(invoice_path / \"invoice.json\").generate()\n        InvoiceSchemaJsonGenerator(tasksupport_path / \"invoice.schema.json\").generate()\n        MetadataDefJsonGenerator(tasksupport_path / \"metadata-def.json\").generate()\n\n        print(f\"\u2705 Environment {env} setup complete\")\n\n# Usage\nenvironments = [\"development\", \"testing\", \"production\"]\nsetup_multiple_environments(environments, \"rde_environments\")\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#advanced-project-template-creation","title":"Advanced Project Template Creation","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.cmd.command import InitCommand\nimport shutil\n\nclass CustomRDEProject:\n    \"\"\"Custom RDE project creator with additional features.\"\"\"\n\n    def __init__(self, project_name: str, base_dir: str = \".\"):\n        self.project_name = project_name\n        self.base_dir = Path(base_dir)\n        self.project_path = self.base_dir / project_name\n\n    def create_project(self, include_samples: bool = True):\n        \"\"\"Create a complete RDE project with optional sample data.\"\"\"\n\n        # Create project directory\n        self.project_path.mkdir(parents=True, exist_ok=True)\n\n        # Change to project directory and initialize\n        original_cwd = Path.cwd()\n        try:\n            import os\n            os.chdir(self.project_path)\n\n            # Initialize standard RDE structure\n            init_cmd = InitCommand()\n            init_cmd.invoke()\n\n            if include_samples:\n                self._create_sample_data()\n\n            self._create_readme()\n\n        finally:\n            os.chdir(original_cwd)\n\n        print(f\"Project '{self.project_name}' created successfully at {self.project_path}\")\n\n    def _create_sample_data(self):\n        \"\"\"Create sample data files for development.\"\"\"\n        sample_data = {\n            \"input/inputdata/sample.csv\": \"id,name,value\\n1,Sample,100\\n2,Test,200\\n\",\n            \"container/modules/sample_module.py\": (\n                \"# Sample module for RDE processing\\n\\n\"\n                \"def process_data(data):\\n\"\n                \"    \\\"\\\"\\\"Process input data.\\\"\\\"\\\"\\n\"\n                \"    return data\\n\"\n            )\n        }\n\n        for file_path, content in sample_data.items():\n            full_path = Path(file_path)\n            full_path.parent.mkdir(parents=True, exist_ok=True)\n            full_path.write_text(content, encoding=\"utf-8\")\n\n    def _create_readme(self):\n        \"\"\"Create a README file with project information.\"\"\"\n        readme_content = f\"\"\"# {self.project_name}\n\nRDE (Research Data Exchange) Project\n\n## Structure\n\n- `container/`: Main application container\n- `input/`: Input data and configurations\n- `templates/`: Template files for data processing\n\n## Getting Started\n\n1. Install dependencies:\n   ```bash\n   cd container\n   pip install -r requirements.txt\n   ```\n\n2. Run the main script:\n   ```bash\n   python main.py\n   ```\n\n## Generated with RDEToolkit\n\nThis project was generated using RDEToolkit command utilities.\n\"\"\"\n        (Path(\"README.md\")).write_text(readme_content, encoding=\"utf-8\")\n\n# Usage\nproject = CustomRDEProject(\"my_research_project\")\nproject.create_project(include_samples=True)\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#version-management-integration","title":"Version Management Integration","text":"<pre><code>from rdetoolkit.cmd.command import VersionCommand\nimport subprocess\nimport sys\n\ndef check_toolkit_version():\n    \"\"\"Check and display toolkit version information.\"\"\"\n\n    print(\"RDEToolkit Version Information:\")\n    print(\"=\" * 40)\n\n    # Display current version\n    version_cmd = VersionCommand()\n    print(\"Current version: \", end=\"\")\n    version_cmd.invoke()\n\n    # Check if updates are available (conceptual)\n    try:\n        # This would typically check against a package repository\n        print(\"\\nChecking for updates...\")\n        result = subprocess.run([\n            sys.executable, \"-m\", \"pip\", \"list\", \"--outdated\"\n        ], capture_output=True, text=True)\n\n        if \"rdetoolkit\" in result.stdout:\n            print(\"\u26a0\ufe0f  Update available!\")\n        else:\n            print(\"\u2705 You're using the latest version\")\n\n    except Exception as e:\n        print(f\"Could not check for updates: {e}\")\n\n# Usage\ncheck_toolkit_version()\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/cmd/command/#common-exceptions","title":"Common Exceptions","text":"<p>The command module operations may raise the following exceptions:</p>"},{"location":"rdetoolkit/cmd/command/#clickabort","title":"click.Abort","text":"<p>Raised when command execution is aborted due to errors:</p> <pre><code>from rdetoolkit.cmd.command import InitCommand\n\ntry:\n    init_cmd = InitCommand()\n    init_cmd.invoke()\nexcept click.Abort:\n    print(\"Command execution was aborted\")\n    # Check console output for specific error details\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#fileexistserror","title":"FileExistsError","text":"<p>May be raised when files already exist and cannot be overwritten:</p> <pre><code>from pathlib import Path\nfrom rdetoolkit.cmd.command import DockerfileGenerator\n\n# Check if file exists before generation\ndockerfile_path = Path(\"Dockerfile\")\nif dockerfile_path.exists():\n    response = input(f\"{dockerfile_path} exists. Overwrite? (y/N): \")\n    if response.lower() != 'y':\n        print(\"Generation cancelled\")\n    else:\n        generator = DockerfileGenerator(dockerfile_path)\n        generator.generate()\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#permissionerror","title":"PermissionError","text":"<p>May be raised if there are insufficient permissions to create files or directories:</p> <pre><code>try:\n    init_cmd = InitCommand()\n    init_cmd.invoke()\nexcept PermissionError as e:\n    print(f\"Permission denied: {e}\")\n    print(\"Check write permissions for the current directory\")\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Check directory permissions before initialization:    <pre><code>import os\nfrom pathlib import Path\n\ndef check_permissions(path: Path) -&gt; bool:\n    \"\"\"Check if directory is writable.\"\"\"\n    try:\n        test_file = path / \".test_write\"\n        test_file.touch()\n        test_file.unlink()\n        return True\n    except (OSError, PermissionError):\n        return False\n\nif check_permissions(Path.cwd()):\n    init_cmd = InitCommand()\n    init_cmd.invoke()\nelse:\n    print(\"Insufficient permissions to create project files\")\n</code></pre></p> </li> <li> <p>Backup existing files before overwriting:    <pre><code>from pathlib import Path\nimport shutil\n\ndef safe_generate_file(generator, target_path: Path):\n    \"\"\"Safely generate file with backup.\"\"\"\n    if target_path.exists():\n        backup_path = target_path.with_suffix(f\"{target_path.suffix}.backup\")\n        shutil.copy2(target_path, backup_path)\n        print(f\"Backed up existing file to {backup_path}\")\n\n    generator.generate()\n</code></pre></p> </li> <li> <p>Validate project structure after initialization:    <pre><code>def validate_project_structure():\n    \"\"\"Validate that all required files were created.\"\"\"\n    required_files = [\n        \"container/main.py\",\n        \"container/Dockerfile\",\n        \"container/requirements.txt\",\n        \"input/invoice/invoice.json\"\n    ]\n\n    missing_files = []\n    for file_path in required_files:\n        if not Path(file_path).exists():\n            missing_files.append(file_path)\n\n    if missing_files:\n        print(\"Warning: Missing files:\")\n        for file in missing_files:\n            print(f\"  - {file}\")\n    else:\n        print(\"\u2705 All required files created successfully\")\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/cmd/command/#performance-notes","title":"Performance Notes","text":"<ul> <li>File generation operations are lightweight and typically complete in milliseconds</li> <li>Directory creation is optimized with <code>parents=True</code> and <code>exist_ok=True</code> options</li> <li>JSON file generation uses efficient encoding with <code>ensure_ascii=False</code> for international character support</li> <li>The initialization process creates files sequentially; for large projects, consider parallel generation</li> <li>Memory usage is minimal as files are written directly to disk without buffering large content</li> </ul>"},{"location":"rdetoolkit/cmd/command/#integration-with-other-modules","title":"Integration with Other Modules","text":""},{"location":"rdetoolkit/cmd/command/#logging-integration","title":"Logging Integration","text":"<p>The command module integrates with the RDE logging system:</p> <pre><code>from rdetoolkit.rdelogger import get_logger\n\n# Used internally for error logging and debugging\nlogger = get_logger(__name__)\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#schema-integration","title":"Schema Integration","text":"<p>Commands use schema models for structured data generation:</p> <pre><code>from rdetoolkit.models.invoice_schema import InvoiceSchemaJson, Properties\nfrom rdetoolkit.cmd.default import INVOICE_JSON, PROPATIES\n\n# Used for generating valid invoice schemas and data\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#version-integration","title":"Version Integration","text":"<p>Version information is automatically included in generated files:</p> <pre><code>from rdetoolkit import __version__\n\n# Automatically included in requirements.txt and other generated files\n</code></pre>"},{"location":"rdetoolkit/cmd/command/#see-also","title":"See Also","text":"<ul> <li>Default Configuration - For default values and templates</li> <li>Invoice Schema Models - For schema data structures</li> <li>RDE Logger - For logging functionality</li> <li>Workflows Module - For processing workflows integration</li> </ul>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/","title":"Generate Excel Invoice Module","text":"<p>The <code>rdetoolkit.cmd.gen_excelinvoice</code> module provides functionality for generating Excel invoice templates from JSON schema files. This module is designed to create standardized Excel files that conform to RDE (Research Data Exchange) invoice specifications, supporting both file and folder processing modes.</p>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#overview","title":"Overview","text":"<p>The generate Excel invoice module offers specialized template generation capabilities:</p> <ul> <li>Excel Template Generation: Create Excel invoice templates from JSON schema definitions</li> <li>Schema Validation: Validate invoice schemas before template generation</li> <li>Mode Support: Support for both file and folder processing modes</li> <li>Error Handling: Comprehensive error handling with user-friendly messages</li> </ul>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#classes","title":"Classes","text":""},{"location":"rdetoolkit/cmd/gen_excelinvoice/#generateexcelinvoicecommand","title":"GenerateExcelInvoiceCommand","text":"<p>A command class that generates Excel invoice templates based on JSON schema files with support for different processing modes.</p>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#constructor","title":"Constructor","text":"<pre><code>GenerateExcelInvoiceCommand(invoice_schema_file: pathlib.Path, output_path: pathlib.Path, mode: Literal[\"file\", \"folder\"])\n</code></pre> <p>Parameters: - <code>invoice_schema_file</code> (pathlib.Path): Path to the JSON schema file that defines the invoice structure - <code>output_path</code> (pathlib.Path): Path where the generated Excel template will be saved - <code>mode</code> (Literal[\"file\", \"folder\"]): Processing mode - either \"file\" for single file processing or \"folder\" for batch processing</p>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#attributes","title":"Attributes","text":"<ul> <li><code>invoice_schema_file</code> (pathlib.Path): The source schema file path</li> <li><code>output_path</code> (pathlib.Path): The target output file path</li> <li><code>mode</code> (Literal[\"file\", \"folder\"]): The processing mode</li> </ul>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#methods","title":"Methods","text":""},{"location":"rdetoolkit/cmd/gen_excelinvoice/#invoke","title":"invoke()","text":"<p>Execute the Excel invoice template generation process.</p> <pre><code>def invoke() -&gt; None\n</code></pre> <p>Raises: - <code>click.Abort</code>: If validation fails, schema file is not found, or generation encounters errors - <code>FileNotFoundError</code>: If the schema file does not exist - <code>InvoiceSchemaValidationError</code>: If the schema validation fails - <code>Exception</code>: For any unexpected errors during generation</p> <p>Example: <pre><code>import pathlib\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\n# Generate Excel invoice template\nschema_file = pathlib.Path(\"schemas/invoice.schema.json\")\noutput_file = pathlib.Path(\"templates/my_invoice_excel_invoice.xlsx\")\n\ncommand = GenerateExcelInvoiceCommand(\n    invoice_schema_file=schema_file,\n    output_path=output_file,\n    mode=\"file\"\n)\ncommand.invoke()\n</code></pre></p>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#file-naming-conventions","title":"File Naming Conventions","text":"<p>The generated Excel files must follow specific naming conventions:</p> <ul> <li>Required Suffix: All output files must end with <code>_excel_invoice.xlsx</code></li> <li>Example Valid Names:</li> <li><code>project_excel_invoice.xlsx</code></li> <li><code>2024_data_excel_invoice.xlsx</code></li> <li><code>research_template_excel_invoice.xlsx</code></li> </ul>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/cmd/gen_excelinvoice/#basic-excel-invoice-generation","title":"Basic Excel Invoice Generation","text":"<pre><code>import pathlib\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\ndef generate_basic_invoice_template():\n    \"\"\"Generate a basic Excel invoice template.\"\"\"\n\n    # Define paths\n    schema_path = pathlib.Path(\"data/invoice.schema.json\")\n    output_path = pathlib.Path(\"output/basic_excel_invoice.xlsx\")\n\n    # Create command instance\n    command = GenerateExcelInvoiceCommand(\n        invoice_schema_file=schema_path,\n        output_path=output_path,\n        mode=\"file\"\n    )\n\n    try:\n        command.invoke()\n        print(f\"\u2705 Template generated successfully: {output_path}\")\n    except Exception as e:\n        print(f\"\u274c Generation failed: {e}\")\n\n# Usage\ngenerate_basic_invoice_template()\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#batch-template-generation","title":"Batch Template Generation","text":"<pre><code>import pathlib\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\ndef generate_multiple_templates(schemas_dir: str, output_dir: str):\n    \"\"\"Generate Excel templates for multiple schema files.\"\"\"\n\n    schemas_path = pathlib.Path(schemas_dir)\n    output_path = pathlib.Path(output_dir)\n\n    # Ensure output directory exists\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Find all schema files\n    schema_files = list(schemas_path.glob(\"*.schema.json\"))\n\n    if not schema_files:\n        print(f\"No schema files found in {schemas_path}\")\n        return\n\n    print(f\"Found {len(schema_files)} schema files\")\n\n    for schema_file in schema_files:\n        # Generate output filename\n        template_name = schema_file.stem.replace('.schema', '_excel_invoice.xlsx')\n        output_file = output_path / template_name\n\n        print(f\"Processing: {schema_file.name}\")\n\n        try:\n            command = GenerateExcelInvoiceCommand(\n                invoice_schema_file=schema_file,\n                output_path=output_file,\n                mode=\"file\"  # Use \"folder\" mode for batch processing if supported\n            )\n            command.invoke()\n            print(f\"  \u2705 Generated: {output_file.name}\")\n\n        except Exception as e:\n            print(f\"  \u274c Failed: {e}\")\n\n# Usage\ngenerate_multiple_templates(\"schemas\", \"templates\")\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#advanced-template-generation-with-validation","title":"Advanced Template Generation with Validation","text":"<pre><code>import pathlib\nimport json\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\ndef generate_validated_template(schema_file: str, output_file: str):\n    \"\"\"Generate Excel template with pre-validation.\"\"\"\n\n    schema_path = pathlib.Path(schema_file)\n    output_path = pathlib.Path(output_file)\n\n    # Validate file naming convention\n    if not output_path.name.endswith('_excel_invoice.xlsx'):\n        print(\"\u274c Error: Output filename must end with '_excel_invoice.xlsx'\")\n        return False\n\n    # Validate schema file exists and is valid JSON\n    if not schema_path.exists():\n        print(f\"\u274c Error: Schema file not found: {schema_path}\")\n        return False\n\n    try:\n        # Test if schema is valid JSON\n        with open(schema_path, 'r', encoding='utf-8') as f:\n            schema_data = json.load(f)\n\n        # Check for required schema properties\n        required_keys = ['type', 'properties']\n        missing_keys = [key for key in required_keys if key not in schema_data]\n\n        if missing_keys:\n            print(f\"\u274c Error: Schema missing required keys: {missing_keys}\")\n            return False\n\n        print(f\"\u2705 Schema validation passed\")\n\n        # Ensure output directory exists\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Generate template\n        command = GenerateExcelInvoiceCommand(\n            invoice_schema_file=schema_path,\n            output_path=output_path,\n            mode=\"file\"\n        )\n        command.invoke()\n\n        print(f\"\u2705 Template generated successfully: {output_path}\")\n        return True\n\n    except json.JSONDecodeError as e:\n        print(f\"\u274c Error: Invalid JSON in schema file: {e}\")\n        return False\n    except Exception as e:\n        print(f\"\u274c Error: Template generation failed: {e}\")\n        return False\n\n# Usage\nsuccess = generate_validated_template(\n    \"schemas/project.schema.json\",\n    \"templates/project_excel_invoice.xlsx\"\n)\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#integration-with-project-workflow","title":"Integration with Project Workflow","text":"<pre><code>import pathlib\nfrom typing import List, Tuple\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\nclass ExcelInvoiceTemplateManager:\n    \"\"\"Manager class for Excel invoice template operations.\"\"\"\n\n    def __init__(self, schemas_dir: str, templates_dir: str):\n        self.schemas_dir = pathlib.Path(schemas_dir)\n        self.templates_dir = pathlib.Path(templates_dir)\n        self.templates_dir.mkdir(parents=True, exist_ok=True)\n\n    def generate_all_templates(self) -&gt; List[Tuple[str, bool, str]]:\n        \"\"\"Generate templates for all schema files.\n\n        Returns:\n            List of tuples: (schema_name, success, message)\n        \"\"\"\n        results = []\n        schema_files = list(self.schemas_dir.glob(\"*.schema.json\"))\n\n        for schema_file in schema_files:\n            result = self._generate_single_template(schema_file)\n            results.append(result)\n\n        return results\n\n    def _generate_single_template(self, schema_file: pathlib.Path) -&gt; Tuple[str, bool, str]:\n        \"\"\"Generate template for a single schema file.\"\"\"\n\n        # Generate output filename\n        template_name = schema_file.stem.replace('.schema', '_excel_invoice.xlsx')\n        output_file = self.templates_dir / template_name\n\n        try:\n            command = GenerateExcelInvoiceCommand(\n                invoice_schema_file=schema_file,\n                output_path=output_file,\n                mode=\"file\"\n            )\n            command.invoke()\n            return (schema_file.name, True, f\"Generated: {output_file.name}\")\n\n        except Exception as e:\n            return (schema_file.name, False, f\"Error: {str(e)}\")\n\n    def update_templates(self, force_update: bool = False) -&gt; None:\n        \"\"\"Update existing templates if schema files are newer.\"\"\"\n\n        schema_files = list(self.schemas_dir.glob(\"*.schema.json\"))\n\n        for schema_file in schema_files:\n            template_name = schema_file.stem.replace('.schema', '_excel_invoice.xlsx')\n            template_file = self.templates_dir / template_name\n\n            # Check if update is needed\n            if not force_update and template_file.exists():\n                schema_mtime = schema_file.stat().st_mtime\n                template_mtime = template_file.stat().st_mtime\n\n                if schema_mtime &lt;= template_mtime:\n                    print(f\"\u23ed\ufe0f  Skipping {schema_file.name} (template is up to date)\")\n                    continue\n\n            print(f\"\ud83d\udd04 Updating template for {schema_file.name}\")\n            schema_name, success, message = self._generate_single_template(schema_file)\n\n            if success:\n                print(f\"  \u2705 {message}\")\n            else:\n                print(f\"  \u274c {message}\")\n\n    def list_templates(self) -&gt; List[pathlib.Path]:\n        \"\"\"List all generated Excel invoice templates.\"\"\"\n        return list(self.templates_dir.glob(\"*_excel_invoice.xlsx\"))\n\n    def cleanup_orphaned_templates(self) -&gt; List[str]:\n        \"\"\"Remove templates that no longer have corresponding schema files.\"\"\"\n\n        # Get all schema base names\n        schema_names = {f.stem.replace('.schema', '') for f in self.schemas_dir.glob(\"*.schema.json\")}\n\n        # Find orphaned templates\n        orphaned = []\n        for template in self.list_templates():\n            template_base = template.stem.replace('_excel_invoice', '')\n            if template_base not in schema_names:\n                template.unlink()\n                orphaned.append(template.name)\n\n        return orphaned\n\n# Usage example\ndef manage_project_templates():\n    \"\"\"Complete template management workflow.\"\"\"\n\n    manager = ExcelInvoiceTemplateManager(\"schemas\", \"templates\")\n\n    # Generate all templates\n    print(\"\ud83d\ude80 Generating Excel invoice templates...\")\n    results = manager.generate_all_templates()\n\n    # Report results\n    successful = sum(1 for _, success, _ in results if success)\n    total = len(results)\n    print(f\"\\n\ud83d\udcca Generation complete: {successful}/{total} successful\")\n\n    for schema_name, success, message in results:\n        status = \"\u2705\" if success else \"\u274c\"\n        print(f\"{status} {schema_name}: {message}\")\n\n    # List generated templates\n    templates = manager.list_templates()\n    print(f\"\\n\ud83d\udcc1 Generated templates ({len(templates)}):\")\n    for template in templates:\n        print(f\"  - {template.name}\")\n\n# Usage\nmanage_project_templates()\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#command-line-integration-example","title":"Command-Line Integration Example","text":"<pre><code>import argparse\nimport pathlib\nimport sys\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\ndef main():\n    \"\"\"Command-line interface for Excel invoice generation.\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description=\"Generate Excel invoice templates from JSON schemas\"\n    )\n    parser.add_argument(\n        \"schema_file\",\n        type=pathlib.Path,\n        help=\"Path to the invoice schema JSON file\"\n    )\n    parser.add_argument(\n        \"output_file\",\n        type=pathlib.Path,\n        help=\"Path for the generated Excel template (must end with '_excel_invoice.xlsx')\"\n    )\n    parser.add_argument(\n        \"--mode\",\n        choices=[\"file\", \"folder\"],\n        default=\"file\",\n        help=\"Processing mode (default: file)\"\n    )\n    parser.add_argument(\n        \"--force\",\n        action=\"store_true\",\n        help=\"Overwrite existing output file\"\n    )\n\n    args = parser.parse_args()\n\n    # Validate arguments\n    if not args.schema_file.exists():\n        print(f\"\u274c Error: Schema file not found: {args.schema_file}\")\n        sys.exit(1)\n\n    if not args.output_file.name.endswith('_excel_invoice.xlsx'):\n        print(\"\u274c Error: Output filename must end with '_excel_invoice.xlsx'\")\n        sys.exit(1)\n\n    if args.output_file.exists() and not args.force:\n        print(f\"\u274c Error: Output file exists: {args.output_file}\")\n        print(\"Use --force to overwrite\")\n        sys.exit(1)\n\n    # Generate template\n    try:\n        command = GenerateExcelInvoiceCommand(\n            invoice_schema_file=args.schema_file,\n            output_path=args.output_file,\n            mode=args.mode\n        )\n        command.invoke()\n\n    except KeyboardInterrupt:\n        print(\"\\n\u26a0\ufe0f  Generation cancelled by user\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\u274c Generation failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/cmd/gen_excelinvoice/#common-exceptions","title":"Common Exceptions","text":"<p>The Excel invoice generation module may raise the following exceptions:</p>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#clickabort","title":"click.Abort","text":"<p>Raised when command execution is aborted due to validation or processing errors:</p> <pre><code>from rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\nimport pathlib\n\ntry:\n    command = GenerateExcelInvoiceCommand(\n        invoice_schema_file=pathlib.Path(\"schema.json\"),\n        output_path=pathlib.Path(\"invalid_name.xlsx\"),  # Missing required suffix\n        mode=\"file\"\n    )\n    command.invoke()\nexcept click.Abort:\n    print(\"Command execution was aborted\")\n    # Check console output for specific error details\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#filenotfounderror","title":"FileNotFoundError","text":"<p>Raised when the schema file cannot be found:</p> <pre><code>import pathlib\nfrom rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\n\nschema_path = pathlib.Path(\"nonexistent_schema.json\")\n\n# Always check file existence before processing\nif not schema_path.exists():\n    print(f\"Schema file not found: {schema_path}\")\nelse:\n    command = GenerateExcelInvoiceCommand(\n        invoice_schema_file=schema_path,\n        output_path=pathlib.Path(\"output_excel_invoice.xlsx\"),\n        mode=\"file\"\n    )\n    command.invoke()\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#invoiceschemavalidationerror","title":"InvoiceSchemaValidationError","text":"<p>Raised when the schema file is invalid or doesn't conform to expected format:</p> <pre><code>from rdetoolkit.cmd.gen_excelinvoice import GenerateExcelInvoiceCommand\nfrom rdetoolkit.exceptions import InvoiceSchemaValidationError\nimport pathlib\n\ntry:\n    command = GenerateExcelInvoiceCommand(\n        invoice_schema_file=pathlib.Path(\"invalid_schema.json\"),\n        output_path=pathlib.Path(\"template_excel_invoice.xlsx\"),\n        mode=\"file\"\n    )\n    command.invoke()\nexcept InvoiceSchemaValidationError as e:\n    print(f\"Schema validation failed: {e}\")\n    print(\"Please check your schema file format and content\")\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Validate file paths before processing:    <pre><code>def validate_paths(schema_file: pathlib.Path, output_file: pathlib.Path) -&gt; bool:\n    \"\"\"Validate input and output paths.\"\"\"\n\n    # Check schema file\n    if not schema_file.exists():\n        print(f\"Schema file not found: {schema_file}\")\n        return False\n\n    if not schema_file.suffix == '.json':\n        print(f\"Schema file must be JSON: {schema_file}\")\n        return False\n\n    # Check output file naming\n    if not output_file.name.endswith('_excel_invoice.xlsx'):\n        print(f\"Output file must end with '_excel_invoice.xlsx': {output_file}\")\n        return False\n\n    # Check output directory\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n\n    return True\n</code></pre></p> </li> <li> <p>Handle schema validation gracefully:    <pre><code>import json\n\ndef validate_schema_content(schema_file: pathlib.Path) -&gt; bool:\n    \"\"\"Validate schema file content before processing.\"\"\"\n    try:\n        with open(schema_file, 'r', encoding='utf-8') as f:\n            schema = json.load(f)\n\n        # Check required fields\n        if 'type' not in schema:\n            print(\"Schema missing 'type' field\")\n            return False\n\n        if 'properties' not in schema:\n            print(\"Schema missing 'properties' field\")\n            return False\n\n        return True\n\n    except json.JSONDecodeError as e:\n        print(f\"Invalid JSON in schema file: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Error reading schema file: {e}\")\n        return False\n</code></pre></p> </li> <li> <p>Implement retry logic for transient failures:    <pre><code>import time\n\ndef generate_with_retry(command: GenerateExcelInvoiceCommand, max_retries: int = 3):\n    \"\"\"Generate template with retry logic.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            command.invoke()\n            return True\n        except Exception as e:\n            if attempt &lt; max_retries - 1:\n                print(f\"Attempt {attempt + 1} failed: {e}\")\n                print(f\"Retrying in {2 ** attempt} seconds...\")\n                time.sleep(2 ** attempt)\n            else:\n                print(f\"All {max_retries} attempts failed\")\n                raise\n\n    return False\n</code></pre></p> </li> <li> <p>Monitor file permissions and disk space:    <pre><code>import shutil\n\ndef check_system_requirements(output_path: pathlib.Path) -&gt; bool:\n    \"\"\"Check system requirements before generation.\"\"\"\n\n    # Check disk space (minimum 10MB)\n    free_space = shutil.disk_usage(output_path.parent).free\n    if free_space &lt; 10 * 1024 * 1024:\n        print(\"Insufficient disk space (need at least 10MB)\")\n        return False\n\n    # Check write permissions\n    try:\n        test_file = output_path.parent / \".test_write\"\n        test_file.touch()\n        test_file.unlink()\n    except (OSError, PermissionError):\n        print(f\"No write permission for directory: {output_path.parent}\")\n        return False\n\n    return True\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#performance-notes","title":"Performance Notes","text":"<ul> <li>Template generation is typically fast for standard schemas (&lt; 1 second)</li> <li>Large schemas with many properties may take longer to process</li> <li>File I/O operations are optimized for typical Excel file sizes</li> <li>Memory usage scales with schema complexity and number of properties</li> <li>Network file systems may impact performance; use local storage when possible</li> </ul>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#integration-with-other-modules","title":"Integration with Other Modules","text":""},{"location":"rdetoolkit/cmd/gen_excelinvoice/#invoice-file-integration","title":"Invoice File Integration","text":"<p>The module uses the ExcelInvoiceFile class for template generation:</p> <pre><code>from rdetoolkit.invoicefile import ExcelInvoiceFile\n\n# Used internally for actual Excel file generation\nExcelInvoiceFile.generate_template(schema_file, output_path, mode)\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#exception-handling-integration","title":"Exception Handling Integration","text":"<p>Uses custom exceptions for better error reporting:</p> <pre><code>from rdetoolkit.exceptions import InvoiceSchemaValidationError\n\n# Provides specific error types for schema validation issues\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#logging-integration","title":"Logging Integration","text":"<p>Integrates with the RDE logging system:</p> <pre><code>from rdetoolkit.rdelogger import get_logger\n\n# Used for detailed error logging and debugging\nlogger = get_logger(__name__)\n</code></pre>"},{"location":"rdetoolkit/cmd/gen_excelinvoice/#see-also","title":"See Also","text":"<ul> <li>Invoice File Module - For Excel invoice file operations</li> <li>Exceptions Module - For custom exception types</li> <li>RDE Logger - For logging functionality</li> <li>Command Module - For other command-line utilities</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/","title":"Compressed Controller Module","text":"<p>The <code>rdetoolkit.impl.compressed_controller</code> module provides comprehensive functionality for handling compressed files and creating archives. This module includes parsers for different compressed file structures, archive creation utilities, and encoding-aware extraction capabilities specifically designed for RDE (Research Data Exchange) workflows.</p>"},{"location":"rdetoolkit/impl/compressed_controller/#overview","title":"Overview","text":"<p>The compressed controller module offers specialized archive management capabilities:</p> <ul> <li>Compressed File Parsing: Extract and validate contents from ZIP files with different structural modes</li> <li>Archive Creation: Create ZIP and TAR.GZ archives with customizable exclusion patterns</li> <li>Encoding Handling: Robust filename encoding detection and correction during extraction</li> <li>Structure Validation: Ensure extracted files match expected structures and avoid naming conflicts</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/#classes","title":"Classes","text":""},{"location":"rdetoolkit/impl/compressed_controller/#compressedflatfileparser","title":"CompressedFlatFileParser","text":"<p>Parser for compressed flat files that validates extracted contents against an Excel invoice structure.</p>"},{"location":"rdetoolkit/impl/compressed_controller/#constructor","title":"Constructor","text":"<pre><code>CompressedFlatFileParser(xlsx_invoice: pd.DataFrame)\n</code></pre> <p>Parameters: - <code>xlsx_invoice</code> (pd.DataFrame): DataFrame representing the expected structure or content description of the compressed files</p>"},{"location":"rdetoolkit/impl/compressed_controller/#attributes","title":"Attributes","text":"<ul> <li><code>xlsx_invoice</code> (pd.DataFrame): The Excel invoice DataFrame used for validation</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/#methods","title":"Methods","text":""},{"location":"rdetoolkit/impl/compressed_controller/#readzipfile-target_path","title":"read(zipfile, target_path)","text":"<p>Extract ZIP file contents and validate against the Excel invoice structure.</p> <pre><code>def read(zipfile: Path, target_path: Path) -&gt; list[tuple[Path, ...]]\n</code></pre> <p>Parameters: - <code>zipfile</code> (Path): Path to the compressed flat file to be read - <code>target_path</code> (Path): Destination directory where the zipfile will be extracted</p> <p>Returns: - <code>list[tuple[Path, ...]]</code>: List of tuples containing file paths that matched the xlsx_invoice structure</p> <p>Example: <pre><code>import pandas as pd\nfrom pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import CompressedFlatFileParser\n\n# Create invoice DataFrame\ninvoice_df = pd.DataFrame({'data_file_names/name': ['file1.txt', 'file2.csv']})\n\n# Parse compressed flat file\nparser = CompressedFlatFileParser(invoice_df)\nmatched_files = parser.read(\n    zipfile=Path(\"data.zip\"),\n    target_path=Path(\"extracted\")\n)\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#compressedfolderparser","title":"CompressedFolderParser","text":"<p>Parser for compressed folders that extracts contents and validates unique directory structures.</p>"},{"location":"rdetoolkit/impl/compressed_controller/#constructor_1","title":"Constructor","text":"<pre><code>CompressedFolderParser(xlsx_invoice: pd.DataFrame)\n</code></pre> <p>Parameters: - <code>xlsx_invoice</code> (pd.DataFrame): DataFrame representing the expected structure or content description of the compressed folder contents</p>"},{"location":"rdetoolkit/impl/compressed_controller/#attributes_1","title":"Attributes","text":"<ul> <li><code>xlsx_invoice</code> (pd.DataFrame): The Excel invoice DataFrame used for validation</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/impl/compressed_controller/#readzipfile-target_path_1","title":"read(zipfile, target_path)","text":"<p>Extract ZIP file contents and return validated file paths based on unique directory names.</p> <pre><code>def read(zipfile: Path, target_path: Path) -&gt; list[tuple[Path, ...]]\n</code></pre> <p>Parameters: - <code>zipfile</code> (Path): Path to the compressed folder to be read - <code>target_path</code> (Path): Destination directory where the zipfile will be extracted</p> <p>Returns: - <code>list[tuple[Path, ...]]</code>: List of tuples containing file paths validated based on unique directory names</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import CompressedFolderParser\n\n# Parse compressed folder\nparser = CompressedFolderParser(invoice_df)\nfolder_files = parser.read(\n    zipfile=Path(\"folder_archive.zip\"),\n    target_path=Path(\"extracted_folders\")\n)\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#validation_uniq_fspathtarget_path-exclude_names","title":"validation_uniq_fspath(target_path, exclude_names)","text":"<p>Check for unique directory names and detect case-sensitive duplicates.</p> <pre><code>def validation_uniq_fspath(target_path: str | Path, exclude_names: list[str]) -&gt; dict[str, list[Path]]\n</code></pre> <p>Parameters: - <code>target_path</code> (str | Path): The directory path to scan - <code>exclude_names</code> (list[str]): List of filenames to exclude from validation</p> <p>Returns: - <code>dict[str, list[Path]]</code>: Dictionary mapping unique directory names to lists of files</p> <p>Raises: - <code>StructuredError</code>: When duplicate directory names are detected (case-insensitive)</p> <p>Example: <pre><code># Validate unique folder structure\nunique_folders = parser.validation_uniq_fspath(\n    target_path=\"extracted_data\",\n    exclude_names=[\"invoice_org.json\", \".DS_Store\"]\n)\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#zipartifactpackagecompressor","title":"ZipArtifactPackageCompressor","text":"<p>Archive compressor for creating ZIP files with customizable exclusion patterns and case-insensitive duplicate detection.</p>"},{"location":"rdetoolkit/impl/compressed_controller/#constructor_2","title":"Constructor","text":"<pre><code>ZipArtifactPackageCompressor(source_dir: str | Path, exclude_patterns: list[str])\n</code></pre> <p>Parameters: - <code>source_dir</code> (str | Path): Source directory to be archived - <code>exclude_patterns</code> (list[str]): List of regex patterns for files/directories to exclude</p>"},{"location":"rdetoolkit/impl/compressed_controller/#attributes_2","title":"Attributes","text":"<ul> <li><code>source_dir</code> (Path): The source directory path</li> <li><code>exclude_patterns</code> (list[str]): List of exclusion patterns (property with getter/setter)</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/impl/compressed_controller/#archiveoutput_zip","title":"archive(output_zip)","text":"<p>Create a ZIP archive from the source directory.</p> <pre><code>def archive(output_zip: str | Path) -&gt; list[Path]\n</code></pre> <p>Parameters: - <code>output_zip</code> (str | Path): Path to the output ZIP file</p> <p>Returns: - <code>list[Path]</code>: List of top-level directories included in the archive</p> <p>Raises: - <code>StructuredError</code>: When case-insensitive duplicate paths are detected</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import ZipArtifactPackageCompressor\n\n# Create ZIP archive\ncompressor = ZipArtifactPackageCompressor(\n    source_dir=\"project_files\",\n    exclude_patterns=[r\"\\.git\", r\"__pycache__\", r\"\\.pyc$\"]\n)\nincluded_dirs = compressor.archive(\"project_archive.zip\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#targzartifactpackagecompressor","title":"TarGzArtifactPackageCompressor","text":"<p>Archive compressor for creating TAR.GZ files with customizable exclusion patterns.</p>"},{"location":"rdetoolkit/impl/compressed_controller/#constructor_3","title":"Constructor","text":"<pre><code>TarGzArtifactPackageCompressor(source_dir: str | Path, exclude_patterns: list[str])\n</code></pre> <p>Parameters: - <code>source_dir</code> (str | Path): Source directory to be archived - <code>exclude_patterns</code> (list[str]): List of regex patterns for files/directories to exclude</p>"},{"location":"rdetoolkit/impl/compressed_controller/#attributes_3","title":"Attributes","text":"<ul> <li><code>source_dir</code> (Path): The source directory path</li> <li><code>exclude_patterns</code> (list[str]): List of exclusion patterns (property with getter/setter)</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/#methods_3","title":"Methods","text":""},{"location":"rdetoolkit/impl/compressed_controller/#archiveoutput_tar","title":"archive(output_tar)","text":"<p>Create a TAR.GZ archive from the source directory.</p> <pre><code>def archive(output_tar: str | Path) -&gt; list[Path]\n</code></pre> <p>Parameters: - <code>output_tar</code> (str | Path): Path to the output TAR.GZ file</p> <p>Returns: - <code>list[Path]</code>: List of top-level directories included in the archive</p> <p>Raises: - <code>StructuredError</code>: When case-insensitive duplicate paths are detected</p> <p>Example: <pre><code>from rdetoolkit.impl.compressed_controller import TarGzArtifactPackageCompressor\n\n# Create TAR.GZ archive\ncompressor = TarGzArtifactPackageCompressor(\n    source_dir=\"project_files\",\n    exclude_patterns=[r\"\\.git\", r\"node_modules\", r\"\\.log$\"]\n)\nincluded_dirs = compressor.archive(\"project_archive.tar.gz\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#functions","title":"Functions","text":""},{"location":"rdetoolkit/impl/compressed_controller/#parse_compressedfile_mode","title":"parse_compressedfile_mode","text":"<p>Factory function to determine the appropriate parser based on Excel invoice structure.</p> <pre><code>def parse_compressedfile_mode(xlsx_invoice: pd.DataFrame) -&gt; ICompressedFileStructParser\n</code></pre> <p>Parameters: - <code>xlsx_invoice</code> (pd.DataFrame): The invoice data in Excel format</p> <p>Returns: - <code>ICompressedFileStructParser</code>: Instance of either CompressedFlatFileParser or CompressedFolderParser</p> <p>Example: <pre><code>import pandas as pd\nfrom rdetoolkit.impl.compressed_controller import parse_compressedfile_mode\n\n# Auto-detect parser type\ninvoice_df = pd.DataFrame({'data_file_names/name': ['file1.txt']})\nparser = parse_compressedfile_mode(invoice_df)  # Returns CompressedFlatFileParser\n\nfolder_invoice_df = pd.DataFrame({'folder_structure': ['data']})\nfolder_parser = parse_compressedfile_mode(folder_invoice_df)  # Returns CompressedFolderParser\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#get_artifact_archiver","title":"get_artifact_archiver","text":"<p>Factory function to get the appropriate archiver based on the specified format.</p> <pre><code>def get_artifact_archiver(fmt: str, source_dir: str | Path, exclude_patterns: list[str]) -&gt; IArtifactPackageCompressor\n</code></pre> <p>Parameters: - <code>fmt</code> (str): The format of the archive ('zip', 'tar.gz', 'targz', 'tgz') - <code>source_dir</code> (str | Path): The source directory to be archived - <code>exclude_patterns</code> (list[str]): List of patterns to exclude</p> <p>Returns: - <code>IArtifactPackageCompressor</code>: Instance of the appropriate archiver class</p> <p>Raises: - <code>ValueError</code>: If the format is not supported</p> <p>Example: <pre><code>from rdetoolkit.impl.compressed_controller import get_artifact_archiver\n\n# Get ZIP archiver\nzip_archiver = get_artifact_archiver(\n    fmt=\"zip\",\n    source_dir=\"my_project\",\n    exclude_patterns=[r\"\\.git\", r\"__pycache__\"]\n)\n\n# Get TAR.GZ archiver\ntar_archiver = get_artifact_archiver(\n    fmt=\"tar.gz\",\n    source_dir=\"my_project\",\n    exclude_patterns=[r\"\\.git\", r\"__pycache__\"]\n)\n</code></pre></p>"},{"location":"rdetoolkit/impl/compressed_controller/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/impl/compressed_controller/#basic-archive-creation","title":"Basic Archive Creation","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import get_artifact_archiver\n\ndef create_project_archive(project_dir: str, output_file: str, format_type: str = \"zip\"):\n    \"\"\"Create a project archive with standard exclusions.\"\"\"\n\n    # Standard exclusion patterns\n    exclude_patterns = [\n        r\"\\.git\",           # Git repository\n        r\"__pycache__\",     # Python cache\n        r\"\\.pyc$\",          # Python compiled files\n        r\"node_modules\",    # Node.js modules\n        r\"\\.env$\",          # Environment files\n        r\"\\.log$\",          # Log files\n        r\"\\.tmp$\",          # Temporary files\n        r\"\\.DS_Store$\",     # macOS metadata\n    ]\n\n    try:\n        # Get appropriate archiver\n        archiver = get_artifact_archiver(\n            fmt=format_type,\n            source_dir=project_dir,\n            exclude_patterns=exclude_patterns\n        )\n\n        # Create archive\n        included_files = archiver.archive(output_file)\n\n        print(f\"\u2705 Archive created: {output_file}\")\n        print(f\"\ud83d\udcc1 Files included: {len(included_files)}\")\n\n        return included_files\n\n    except Exception as e:\n        print(f\"\u274c Archive creation failed: {e}\")\n        raise\n\n# Usage\nincluded = create_project_archive(\n    project_dir=\"my_python_project\",\n    output_file=\"backup.zip\",\n    format_type=\"zip\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#advanced-archive-with-custom-exclusions","title":"Advanced Archive with Custom Exclusions","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import ZipArtifactPackageCompressor\n\nclass CustomArchiver:\n    \"\"\"Custom archiver with advanced exclusion logic.\"\"\"\n\n    def __init__(self, source_dir: str):\n        self.source_dir = Path(source_dir)\n        self.base_exclusions = [\n            r\"\\.git\",\n            r\"__pycache__\",\n            r\"\\.pyc$\",\n            r\"\\.pyo$\",\n        ]\n\n    def create_source_archive(self, output_path: str) -&gt; list[Path]:\n        \"\"\"Create archive with source code only.\"\"\"\n\n        source_exclusions = self.base_exclusions + [\n            r\"build\",\n            r\"dist\",\n            r\"\\.egg-info\",\n            r\"coverage\",\n            r\"\\.pytest_cache\",\n            r\"\\.mypy_cache\",\n        ]\n\n        compressor = ZipArtifactPackageCompressor(\n            source_dir=self.source_dir,\n            exclude_patterns=source_exclusions\n        )\n\n        return compressor.archive(output_path)\n\n    def create_deployment_archive(self, output_path: str) -&gt; list[Path]:\n        \"\"\"Create archive for deployment (includes dependencies).\"\"\"\n\n        deployment_exclusions = self.base_exclusions + [\n            r\"tests?\",          # Test directories\n            r\"\\.pytest_cache\",\n            r\"\\.coverage\",\n            r\"docs?\",           # Documentation\n            r\"examples?\",       # Example files\n            r\"\\.md$\",           # Markdown files\n        ]\n\n        compressor = ZipArtifactPackageCompressor(\n            source_dir=self.source_dir,\n            exclude_patterns=deployment_exclusions\n        )\n\n        return compressor.archive(output_path)\n\n    def create_full_backup(self, output_path: str) -&gt; list[Path]:\n        \"\"\"Create complete backup archive.\"\"\"\n\n        minimal_exclusions = [\n            r\"\\.git/objects\",   # Exclude large git objects only\n            r\"__pycache__\",\n            r\"\\.pyc$\",\n        ]\n\n        compressor = ZipArtifactPackageCompressor(\n            source_dir=self.source_dir,\n            exclude_patterns=minimal_exclusions\n        )\n\n        return compressor.archive(output_path)\n\n# Usage\narchiver = CustomArchiver(\"my_project\")\n\n# Create different types of archives\nsource_files = archiver.create_source_archive(\"source_code.zip\")\ndeployment_files = archiver.create_deployment_archive(\"deployment.zip\")\nbackup_files = archiver.create_full_backup(\"full_backup.zip\")\n\nprint(f\"Source archive: {len(source_files)} files\")\nprint(f\"Deployment archive: {len(deployment_files)} files\")\nprint(f\"Full backup: {len(backup_files)} files\")\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#compressed-file-extraction-and-validation","title":"Compressed File Extraction and Validation","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import parse_compressedfile_mode\n\ndef extract_and_validate_archive(archive_path: str, invoice_file: str, extract_dir: str):\n    \"\"\"Extract archive and validate contents against invoice.\"\"\"\n\n    archive_path = Path(archive_path)\n    extract_path = Path(extract_dir)\n\n    # Ensure extraction directory exists\n    extract_path.mkdir(parents=True, exist_ok=True)\n\n    try:\n        # Load invoice DataFrame\n        invoice_df = pd.read_excel(invoice_file)\n\n        # Get appropriate parser\n        parser = parse_compressedfile_mode(invoice_df)\n\n        # Extract and validate\n        validated_files = parser.read(archive_path, extract_path)\n\n        print(f\"\u2705 Archive extracted successfully\")\n        print(f\"\ud83d\udcc1 Extraction directory: {extract_path}\")\n        print(f\"\u2714\ufe0f  Validated file groups: {len(validated_files)}\")\n\n        # Display validated files\n        for i, file_group in enumerate(validated_files):\n            print(f\"  Group {i+1}: {len(file_group)} files\")\n            for file_path in file_group:\n                print(f\"    - {file_path}\")\n\n        return validated_files\n\n    except Exception as e:\n        print(f\"\u274c Extraction/validation failed: {e}\")\n        raise\n\n# Usage\nvalidated = extract_and_validate_archive(\n    archive_path=\"data_archive.zip\",\n    invoice_file=\"invoice.xlsx\",\n    extract_dir=\"extracted_data\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#batch-archive-processing","title":"Batch Archive Processing","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import get_artifact_archiver\n\ndef batch_archive_directories(base_dir: str, output_dir: str, format_type: str = \"zip\"):\n    \"\"\"Create archives for multiple directories.\"\"\"\n\n    base_path = Path(base_dir)\n    output_path = Path(output_dir)\n\n    # Ensure output directory exists\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Find all subdirectories\n    subdirs = [d for d in base_path.iterdir() if d.is_dir()]\n\n    if not subdirs:\n        print(f\"No subdirectories found in {base_path}\")\n        return\n\n    print(f\"Found {len(subdirs)} directories to archive\")\n\n    # Standard exclusions\n    exclude_patterns = [\n        r\"\\.git\", r\"__pycache__\", r\"\\.pyc$\",\n        r\"node_modules\", r\"\\.DS_Store$\"\n    ]\n\n    results = []\n\n    for subdir in subdirs:\n        archive_name = f\"{subdir.name}_archive.{format_type}\"\n        archive_path = output_path / archive_name\n\n        print(f\"\ud83d\udce6 Archiving: {subdir.name}\")\n\n        try:\n            # Get archiver\n            archiver = get_artifact_archiver(\n                fmt=format_type,\n                source_dir=subdir,\n                exclude_patterns=exclude_patterns\n            )\n\n            # Create archive\n            included_files = archiver.archive(archive_path)\n\n            results.append({\n                'directory': subdir.name,\n                'archive': archive_path,\n                'files_count': len(included_files),\n                'success': True,\n                'error': None\n            })\n\n            print(f\"  \u2705 Success: {len(included_files)} files\")\n\n        except Exception as e:\n            results.append({\n                'directory': subdir.name,\n                'archive': None,\n                'files_count': 0,\n                'success': False,\n                'error': str(e)\n            })\n\n            print(f\"  \u274c Failed: {e}\")\n\n    # Summary\n    successful = sum(1 for r in results if r['success'])\n    total_files = sum(r['files_count'] for r in results)\n\n    print(f\"\\n\ud83d\udcca Batch archiving complete:\")\n    print(f\"  \u2705 Successful: {successful}/{len(subdirs)}\")\n    print(f\"  \ud83d\udcc1 Total files archived: {total_files}\")\n\n    return results\n\n# Usage\nresults = batch_archive_directories(\n    base_dir=\"projects\",\n    output_dir=\"archives\",\n    format_type=\"zip\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#encoding-aware-archive-extraction","title":"Encoding-Aware Archive Extraction","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\nfrom rdetoolkit.impl.compressed_controller import CompressedFlatFileParser\n\ndef extract_international_archive(archive_path: str, extract_dir: str):\n    \"\"\"Extract archive with international filenames.\"\"\"\n\n    # Create a simple invoice for validation\n    invoice_df = pd.DataFrame({\n        'data_file_names/name': ['*']  # Accept all files\n    })\n\n    parser = CompressedFlatFileParser(invoice_df)\n\n    try:\n        # The parser handles encoding detection automatically\n        validated_files = parser.read(\n            zipfile=Path(archive_path),\n            target_path=Path(extract_dir)\n        )\n\n        print(f\"\u2705 Archive with international filenames extracted\")\n        print(f\"\ud83d\udcc1 Files extracted: {len(validated_files)}\")\n\n        # Display files with their detected encoding\n        for file_group in validated_files:\n            for file_path in file_group:\n                print(f\"  \ud83d\udcc4 {file_path}\")\n\n                # Check if filename contains non-ASCII characters\n                try:\n                    file_path.name.encode('ascii')\n                    print(f\"    \u2713 ASCII filename\")\n                except UnicodeEncodeError:\n                    print(f\"    \ud83c\udf10 International filename detected\")\n\n        return validated_files\n\n    except Exception as e:\n        print(f\"\u274c Extraction failed: {e}\")\n        raise\n\n# Usage\ninternational_files = extract_international_archive(\n    archive_path=\"\u56fd\u969b\u7684\u306a\u30d5\u30a1\u30a4\u30eb.zip\",  # Archive with Japanese filename\n    extract_dir=\"extracted_international\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/impl/compressed_controller/#common-exceptions","title":"Common Exceptions","text":"<p>The compressed controller module may raise the following exceptions:</p>"},{"location":"rdetoolkit/impl/compressed_controller/#structurederror","title":"StructuredError","text":"<p>Raised when structural validation fails or duplicate paths are detected:</p> <pre><code>from rdetoolkit.impl.compressed_controller import ZipArtifactPackageCompressor\nfrom rdetoolkit.exceptions import StructuredError\n\ntry:\n    compressor = ZipArtifactPackageCompressor(\n        source_dir=\"project_with_duplicates\",\n        exclude_patterns=[]\n    )\n    compressor.archive(\"output.zip\")\nexcept StructuredError as e:\n    print(f\"Structural error: {e}\")\n    # Handle case-insensitive duplicate paths\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#valueerror","title":"ValueError","text":"<p>Raised when unsupported archive formats are specified:</p> <pre><code>from rdetoolkit.impl.compressed_controller import get_artifact_archiver\n\ntry:\n    archiver = get_artifact_archiver(\n        fmt=\"unsupported_format\",\n        source_dir=\"project\",\n        exclude_patterns=[]\n    )\nexcept ValueError as e:\n    print(f\"Unsupported format: {e}\")\n    # Use supported formats: 'zip', 'tar.gz', 'targz', 'tgz'\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#unicodedecodeerror","title":"UnicodeDecodeError","text":"<p>May be raised during filename encoding detection:</p> <pre><code>from rdetoolkit.impl.compressed_controller import CompressedFlatFileParser\n\ntry:\n    parser = CompressedFlatFileParser(invoice_df)\n    parser.read(archive_path, extract_path)\nexcept UnicodeDecodeError as e:\n    print(f\"Encoding error: {e}\")\n    # The parser attempts automatic encoding detection\n</code></pre>"},{"location":"rdetoolkit/impl/compressed_controller/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Validate paths before archiving:    <pre><code>def validate_source_directory(source_dir: Path) -&gt; bool:\n    \"\"\"Validate source directory before archiving.\"\"\"\n    if not source_dir.exists():\n        print(f\"Source directory does not exist: {source_dir}\")\n        return False\n\n    if not source_dir.is_dir():\n        print(f\"Source path is not a directory: {source_dir}\")\n        return False\n\n    # Check for files to archive\n    files = list(source_dir.rglob(\"*\"))\n    if not files:\n        print(f\"No files found in source directory: {source_dir}\")\n        return False\n\n    return True\n</code></pre></p> </li> <li> <p>Handle exclusion patterns carefully:    <pre><code>def create_safe_exclusions(custom_patterns: list[str]) -&gt; list[str]:\n    \"\"\"Create safe exclusion patterns with validation.\"\"\"\n    import re\n\n    safe_patterns = []\n    for pattern in custom_patterns:\n        try:\n            # Test if pattern is valid regex\n            re.compile(pattern)\n            safe_patterns.append(pattern)\n        except re.error as e:\n            print(f\"Invalid regex pattern '{pattern}': {e}\")\n\n    return safe_patterns\n</code></pre></p> </li> <li> <p>Monitor disk space and permissions:    <pre><code>import shutil\n\ndef check_archive_requirements(source_dir: Path, output_path: Path) -&gt; bool:\n    \"\"\"Check system requirements for archiving.\"\"\"\n\n    # Estimate source size\n    total_size = sum(\n        f.stat().st_size for f in source_dir.rglob(\"*\") if f.is_file()\n    )\n\n    # Check available disk space (with 20% buffer)\n    free_space = shutil.disk_usage(output_path.parent).free\n    required_space = int(total_size * 1.2)  # Compression may not reduce size much\n\n    if free_space &lt; required_space:\n        print(f\"Insufficient disk space. Required: {required_space}, Available: {free_space}\")\n        return False\n\n    # Check write permissions\n    try:\n        test_file = output_path.parent / \".test_write\"\n        test_file.touch()\n        test_file.unlink()\n    except (OSError, PermissionError):\n        print(f\"No write permission for: {output_path.parent}\")\n        return False\n\n    return True\n</code></pre></p> </li> <li> <p>Implement progress tracking for large archives:    <pre><code>import os\nfrom typing import Callable\n\nclass ProgressTrackingArchiver:\n    \"\"\"Archiver with progress tracking capabilities.\"\"\"\n\n    def __init__(self, source_dir: Path, exclude_patterns: list[str]):\n        self.source_dir = source_dir\n        self.exclude_patterns = exclude_patterns\n        self.progress_callback: Callable[[int, int], None] | None = None\n\n    def set_progress_callback(self, callback: Callable[[int, int], None]):\n        \"\"\"Set callback for progress updates.\"\"\"\n        self.progress_callback = callback\n\n    def archive_with_progress(self, output_path: Path) -&gt; list[Path]:\n        \"\"\"Create archive with progress tracking.\"\"\"\n\n        # Count total files first\n        total_files = 0\n        for root, dirs, files in os.walk(self.source_dir):\n            total_files += len([f for f in files if not self._is_excluded(os.path.join(root, f))])\n\n        # Create archive with progress\n        processed_files = 0\n\n        compressor = ZipArtifactPackageCompressor(\n            self.source_dir, self.exclude_patterns\n        )\n\n        # This would need to be integrated into the actual archiver\n        # For demonstration purposes\n        if self.progress_callback:\n            self.progress_callback(processed_files, total_files)\n\n        return compressor.archive(output_path)\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/impl/compressed_controller/#performance-notes","title":"Performance Notes","text":"<ul> <li>Archive creation performance scales with the number of files and their total size</li> <li>Exclusion pattern matching uses compiled regex for optimal performance</li> <li>Encoding detection during extraction may add overhead for files with international names</li> <li>Memory usage is minimized by streaming file operations</li> <li>TAR.GZ compression typically provides better compression ratios than ZIP but may be slower</li> <li>Case-insensitive duplicate detection requires additional processing but prevents extraction issues</li> </ul>"},{"location":"rdetoolkit/impl/compressed_controller/#see-also","title":"See Also","text":"<ul> <li>Interface Definitions - For compressor and parser interfaces</li> <li>Invoice File Module - For file validation functionality</li> <li>Exceptions Module - For custom exception types</li> <li>RDE Logger - For logging functionality</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/","title":"Input Controller Module","text":"<p>The <code>rdetoolkit.impl.input_controller</code> module provides comprehensive input file processing capabilities for RDE (Research Data Exchange) workflows. This module includes specialized checker classes for different input file formats and modes, enabling automatic detection and processing of various data structures including Excel invoices, ZIP archives, multi-file inputs, and SmartTable formats.</p>"},{"location":"rdetoolkit/impl/input_controller/#overview","title":"Overview","text":"<p>The input controller module offers specialized file processing capabilities for different RDE input modes:</p> <ul> <li>Invoice Processing: Handle general invoice files with flexible input types</li> <li>Excel Invoice Processing: Process structured Excel invoice files with ZIP archive validation</li> <li>RDE Format Processing: Handle structured RDE format ZIP archives with numbered directories</li> <li>Multi-File Processing: Process multiple individual files simultaneously</li> <li>SmartTable Processing: Handle SmartTable files with automatic CSV generation and file mapping</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#classes","title":"Classes","text":""},{"location":"rdetoolkit/impl/input_controller/#invoicechecker","title":"InvoiceChecker","text":"<p>A basic checker class for processing invoice files with minimal validation requirements.</p>"},{"location":"rdetoolkit/impl/input_controller/#constructor","title":"Constructor","text":"<pre><code>InvoiceChecker(unpacked_dir_basename: Path)\n</code></pre> <p>Parameters: - <code>unpacked_dir_basename</code> (Path): Temporary directory for unpacked content</p>"},{"location":"rdetoolkit/impl/input_controller/#attributes","title":"Attributes","text":"<ul> <li><code>out_dir_temp</code> (Path): Temporary directory for the unpacked content</li> <li><code>checker_type</code> (str): Returns \"invoice\" as the type identifier</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#methods","title":"Methods","text":""},{"location":"rdetoolkit/impl/input_controller/#parsesrc_dir_input","title":"parse(src_dir_input)","text":"<p>Parse the source input directory and group files by type.</p> <pre><code>def parse(src_dir_input: Path) -&gt; tuple[RawFiles, Path | None]\n</code></pre> <p>Parameters: - <code>src_dir_input</code> (Path): Source directory containing the input files</p> <p>Returns: - <code>tuple[RawFiles, Path | None]</code>: Tuple containing list of raw file tuples and None</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import InvoiceChecker\n\n# Create checker instance\nchecker = InvoiceChecker(Path(\"temp\"))\n\n# Parse input directory\nraw_files, invoice_file = checker.parse(Path(\"input_data\"))\nprint(f\"Found {len(raw_files)} file groups\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/input_controller/#excelinvoicechecker","title":"ExcelInvoiceChecker","text":"<p>A sophisticated checker class for processing Excel invoice files with ZIP archive validation and structured data extraction.</p>"},{"location":"rdetoolkit/impl/input_controller/#constructor_1","title":"Constructor","text":"<pre><code>ExcelInvoiceChecker(unpacked_dir_basename: Path)\n</code></pre> <p>Parameters: - <code>unpacked_dir_basename</code> (Path): Temporary directory for unpacked content</p>"},{"location":"rdetoolkit/impl/input_controller/#attributes_1","title":"Attributes","text":"<ul> <li><code>out_dir_temp</code> (Path): Temporary directory for unpacked content</li> <li><code>checker_type</code> (str): Returns \"excel_invoice\" as the type identifier</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/impl/input_controller/#parsesrc_dir_input_1","title":"parse(src_dir_input)","text":"<p>Parse and validate Excel invoice files with optional ZIP archives.</p> <pre><code>def parse(src_dir_input: Path) -&gt; tuple[RawFiles, Path | None]\n</code></pre> <p>Parameters: - <code>src_dir_input</code> (Path): Source directory containing the input files</p> <p>Returns: - <code>tuple[RawFiles, Path | None]</code>: Tuple containing list of raw file tuples and Excel invoice file path</p> <p>Raises: - <code>StructuredError</code>: If validation fails or file structure is inconsistent</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import ExcelInvoiceChecker\n\n# Create checker with temporary directory\nchecker = ExcelInvoiceChecker(Path(\"temp_extract\"))\n\n# Process Excel invoice with ZIP\ntry:\n    raw_files, excel_file = checker.parse(Path(\"excel_invoice_data\"))\n    print(f\"Excel invoice: {excel_file}\")\n    print(f\"Raw file groups: {len(raw_files)}\")\nexcept StructuredError as e:\n    print(f\"Validation error: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/input_controller/#get_indexpaths-sort_items","title":"get_index(paths, sort_items)","text":"<p>Get the index of a file path based on sorted items from Excel invoice.</p> <pre><code>def get_index(paths: Path, sort_items: Sequence) -&gt; int\n</code></pre> <p>Parameters: - <code>paths</code> (Path): Directory path of the raw files - <code>sort_items</code> (Sequence): List of files sorted in Excel invoice order</p> <p>Returns: - <code>int</code>: The index number or length of sort_items if not found</p>"},{"location":"rdetoolkit/impl/input_controller/#rdeformatchecker","title":"RDEFormatChecker","text":"<p>A checker class for processing structured RDE format ZIP archives with numbered directory organization.</p>"},{"location":"rdetoolkit/impl/input_controller/#constructor_2","title":"Constructor","text":"<pre><code>RDEFormatChecker(unpacked_dir_basename: Path)\n</code></pre> <p>Parameters: - <code>unpacked_dir_basename</code> (Path): Temporary directory for unpacked content</p>"},{"location":"rdetoolkit/impl/input_controller/#attributes_2","title":"Attributes","text":"<ul> <li><code>out_dir_temp</code> (Path): Temporary directory for unpacked content</li> <li><code>checker_type</code> (str): Returns \"rde_format\" as the type identifier</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/impl/input_controller/#parsesrc_dir_input_2","title":"parse(src_dir_input)","text":"<p>Parse RDE format ZIP files and extract structured data.</p> <pre><code>def parse(src_dir_input: Path) -&gt; tuple[RawFiles, Path | None]\n</code></pre> <p>Parameters: - <code>src_dir_input</code> (Path): Source directory containing the input files</p> <p>Returns: - <code>tuple[RawFiles, Path | None]</code>: Tuple containing list of raw file tuples grouped by directory numbers and None</p> <p>Raises: - <code>StructuredError</code>: If no ZIP file or multiple ZIP files are found</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import RDEFormatChecker\n\n# Create RDE format checker\nchecker = RDEFormatChecker(Path(\"rde_temp\"))\n\n# Process RDE format ZIP\ntry:\n    raw_files, _ = checker.parse(Path(\"rde_archive\"))\n    print(f\"RDE groups: {len(raw_files)}\")\n    for i, group in enumerate(raw_files):\n        print(f\"  Group {i}: {len(group)} files\")\nexcept StructuredError as e:\n    print(f\"RDE format error: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/input_controller/#multifilechecker","title":"MultiFileChecker","text":"<p>A checker class for processing multiple individual files without specific structural requirements.</p>"},{"location":"rdetoolkit/impl/input_controller/#constructor_3","title":"Constructor","text":"<pre><code>MultiFileChecker(unpacked_dir_basename: Path)\n</code></pre> <p>Parameters: - <code>unpacked_dir_basename</code> (Path): Temporary directory used for certain operations</p>"},{"location":"rdetoolkit/impl/input_controller/#attributes_3","title":"Attributes","text":"<ul> <li><code>out_dir_temp</code> (Path): Temporary directory for operations</li> <li><code>checker_type</code> (str): Returns \"multifile\" as the type identifier</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#methods_3","title":"Methods","text":""},{"location":"rdetoolkit/impl/input_controller/#parsesrc_dir_input_3","title":"parse(src_dir_input)","text":"<p>Parse multiple individual files from the input directory.</p> <pre><code>def parse(src_dir_input: Path) -&gt; tuple[RawFiles, Path | None]\n</code></pre> <p>Parameters: - <code>src_dir_input</code> (Path): Source directory containing the input files</p> <p>Returns: - <code>tuple[RawFiles, Path | None]</code>: Tuple containing list of single-file tuples sorted by filename and None</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import MultiFileChecker\n\n# Create multi-file checker\nchecker = MultiFileChecker(Path(\"multi_temp\"))\n\n# Process multiple files\nraw_files, _ = checker.parse(Path(\"multiple_files\"))\nprint(f\"Individual files: {len(raw_files)}\")\nfor file_tuple in raw_files:\n    print(f\"  File: {file_tuple[0].name}\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/input_controller/#smarttablechecker","title":"SmartTableChecker","text":"<p>A sophisticated checker class for processing SmartTable files with automatic CSV generation and file mapping capabilities.</p>"},{"location":"rdetoolkit/impl/input_controller/#constructor_4","title":"Constructor","text":"<pre><code>SmartTableChecker(unpacked_dir_basename: Path)\n</code></pre> <p>Parameters: - <code>unpacked_dir_basename</code> (Path): Temporary directory for unpacked content</p>"},{"location":"rdetoolkit/impl/input_controller/#attributes_4","title":"Attributes","text":"<ul> <li><code>out_dir_temp</code> (Path): Temporary directory for unpacked content</li> <li><code>checker_type</code> (str): Returns \"smarttable\" as the type identifier</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#methods_4","title":"Methods","text":""},{"location":"rdetoolkit/impl/input_controller/#parsesrc_dir_input_4","title":"parse(src_dir_input)","text":"<p>Parse SmartTable files and generate individual CSV files with file mappings.</p> <pre><code>def parse(src_dir_input: Path) -&gt; tuple[RawFiles, Path | None]\n</code></pre> <p>Parameters: - <code>src_dir_input</code> (Path): Source directory containing the input files</p> <p>Returns: - <code>tuple[RawFiles, Path | None]</code>: Tuple containing list of CSV-file mapping tuples and SmartTable file path</p> <p>Raises: - <code>StructuredError</code>: If no SmartTable files found or multiple SmartTable files present</p> <p>Example: <pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import SmartTableChecker\n\n# Create SmartTable checker\nchecker = SmartTableChecker(Path(\"smarttable_temp\"))\n\n# Process SmartTable with related files\ntry:\n    raw_files, smarttable_file = checker.parse(Path(\"smarttable_data\"))\n    print(f\"SmartTable file: {smarttable_file}\")\n\n    for csv_file_tuple in raw_files:\n        csv_file = csv_file_tuple[0]\n        related_files = csv_file_tuple[1:]\n        print(f\"CSV: {csv_file.name} -&gt; {len(related_files)} related files\")\n\nexcept StructuredError as e:\n    print(f\"SmartTable error: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/impl/input_controller/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/impl/input_controller/#basic-input-processing-workflow","title":"Basic Input Processing Workflow","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import (\n    InvoiceChecker, ExcelInvoiceChecker, RDEFormatChecker,\n    MultiFileChecker, SmartTableChecker\n)\n\ndef detect_and_process_input(input_dir: str, temp_dir: str):\n    \"\"\"Automatically detect input type and process accordingly.\"\"\"\n\n    input_path = Path(input_dir)\n    temp_path = Path(temp_dir)\n    temp_path.mkdir(parents=True, exist_ok=True)\n\n    # Get list of input files\n    input_files = list(input_path.glob(\"*\"))\n\n    # Detect input type based on file patterns\n    has_excel_invoice = any(\n        f.suffix.lower() in [\".xls\", \".xlsx\"] and f.stem.endswith(\"_excel_invoice\")\n        for f in input_files\n    )\n\n    has_smarttable = any(\n        f.name.startswith(\"smarttable_\") and f.suffix.lower() in [\".xlsx\", \".csv\", \".tsv\"]\n        for f in input_files\n    )\n\n    has_zip = any(f.suffix.lower() == \".zip\" for f in input_files)\n\n    # Select appropriate checker\n    if has_smarttable:\n        checker = SmartTableChecker(temp_path)\n        print(\"\ud83d\udd0d Detected: SmartTable format\")\n    elif has_excel_invoice:\n        checker = ExcelInvoiceChecker(temp_path)\n        print(\"\ud83d\udd0d Detected: Excel Invoice format\")\n    elif has_zip and len([f for f in input_files if f.suffix.lower() == \".zip\"]) == 1:\n        checker = RDEFormatChecker(temp_path)\n        print(\"\ud83d\udd0d Detected: RDE Format\")\n    elif len(input_files) &gt; 1:\n        checker = MultiFileChecker(temp_path)\n        print(\"\ud83d\udd0d Detected: Multi-file format\")\n    else:\n        checker = InvoiceChecker(temp_path)\n        print(\"\ud83d\udd0d Detected: General invoice format\")\n\n    # Process input\n    try:\n        raw_files, metadata_file = checker.parse(input_path)\n\n        print(f\"\u2705 Processing complete:\")\n        print(f\"  Checker type: {checker.checker_type}\")\n        print(f\"  File groups: {len(raw_files)}\")\n        print(f\"  Metadata file: {metadata_file.name if metadata_file else 'None'}\")\n\n        return raw_files, metadata_file, checker.checker_type\n\n    except Exception as e:\n        print(f\"\u274c Processing failed: {e}\")\n        raise\n\n# Usage\nraw_files, metadata, input_type = detect_and_process_input(\n    input_dir=\"input_data\",\n    temp_dir=\"temp_processing\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/input_controller/#excel-invoice-processing-with-validation","title":"Excel Invoice Processing with Validation","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import ExcelInvoiceChecker\nfrom rdetoolkit.exceptions import StructuredError\n\ndef process_excel_invoice_with_validation(input_dir: str, temp_dir: str):\n    \"\"\"Process Excel invoice with comprehensive validation.\"\"\"\n\n    input_path = Path(input_dir)\n    temp_path = Path(temp_dir)\n\n    # Pre-validation\n    input_files = list(input_path.glob(\"*\"))\n\n    # Check for Excel invoice files\n    excel_files = [\n        f for f in input_files\n        if f.suffix.lower() in [\".xls\", \".xlsx\"] and f.stem.endswith(\"_excel_invoice\")\n    ]\n\n    if not excel_files:\n        raise ValueError(\"No Excel invoice files found\")\n\n    if len(excel_files) &gt; 1:\n        raise ValueError(f\"Multiple Excel invoice files found: {[f.name for f in excel_files]}\")\n\n    # Check ZIP files\n    zip_files = [f for f in input_files if f.suffix.lower() == \".zip\"]\n\n    if len(zip_files) &gt; 1:\n        raise ValueError(f\"Multiple ZIP files found: {[f.name for f in zip_files]}\")\n\n    print(f\"\ud83d\udcc4 Excel invoice: {excel_files[0].name}\")\n    if zip_files:\n        print(f\"\ud83d\udce6 ZIP archive: {zip_files[0].name}\")\n\n    # Create checker and process\n    checker = ExcelInvoiceChecker(temp_path)\n\n    try:\n        raw_files, excel_file = checker.parse(input_path)\n\n        print(f\"\u2705 Excel invoice processing complete:\")\n        print(f\"  Raw file groups: {len(raw_files)}\")\n        print(f\"  Files per group:\")\n\n        for i, group in enumerate(raw_files):\n            print(f\"    Group {i+1}: {len(group)} files\")\n            for file_path in group:\n                print(f\"      - {file_path.name}\")\n\n        return raw_files, excel_file\n\n    except StructuredError as e:\n        print(f\"\u274c Excel invoice validation failed: {e}\")\n        raise\n\n# Usage\ntry:\n    raw_files, excel_file = process_excel_invoice_with_validation(\n        input_dir=\"excel_invoice_input\",\n        temp_dir=\"excel_temp\"\n    )\nexcept Exception as e:\n    print(f\"Processing error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/impl/input_controller/#smarttable-processing-with-file-mapping","title":"SmartTable Processing with File Mapping","text":"<pre><code>from pathlib import Path\nfrom rdetoolkit.impl.input_controller import SmartTableChecker\n\ndef process_smarttable_with_mapping(input_dir: str, temp_dir: str):\n    \"\"\"Process SmartTable files with detailed file mapping.\"\"\"\n\n    input_path = Path(input_dir)\n    temp_path = Path(temp_dir)\n    temp_path.mkdir(parents=True, exist_ok=True)\n\n    # Validate SmartTable files\n    smarttable_files = [\n        f for f in input_path.glob(\"*\")\n        if f.name.startswith(\"smarttable_\") and f.suffix.lower() in [\".xlsx\", \".csv\", \".tsv\"]\n    ]\n\n    if not smarttable_files:\n        raise ValueError(\"No SmartTable files found (must start with 'smarttable_')\")\n\n    if len(smarttable_files) &gt; 1:\n        raise ValueError(f\"Multiple SmartTable files: {[f.name for f in smarttable_files]}\")\n\n    print(f\"\ud83d\udcca SmartTable file: {smarttable_files[0].name}\")\n\n    # Check for ZIP files\n    zip_files = [f for f in input_path.glob(\"*.zip\")]\n    if zip_files:\n        print(f\"\ud83d\udce6 ZIP files: {[f.name for f in zip_files]}\")\n\n    # Process with SmartTable checker\n    checker = SmartTableChecker(temp_path)\n\n    try:\n        raw_files, smarttable_file = checker.parse(input_path)\n\n        print(f\"\u2705 SmartTable processing complete:\")\n        print(f\"  Generated CSV files: {len(raw_files)}\")\n        print(f\"  File mappings:\")\n\n        for i, file_tuple in enumerate(raw_files):\n            csv_file = file_tuple[0]\n            related_files = file_tuple[1:]\n\n            print(f\"    Row {i+1} -&gt; {csv_file.name}\")\n            if related_files:\n                print(f\"      Related files: {len(related_files)}\")\n                for related_file in related_files:\n                    print(f\"        - {related_file.name}\")\n            else:\n                print(f\"      No related files\")\n\n        return raw_files, smarttable_file\n\n    except Exception as e:\n        print(f\"\u274c SmartTable processing failed: {e}\")\n        raise\n\n# Usage\nraw_files, smarttable = process_smarttable_with_mapping(\n    input_dir=\"smarttable_input\",\n    temp_dir=\"smarttable_temp\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/input_controller/#multi-format-processing-pipeline","title":"Multi-Format Processing Pipeline","text":"<pre><code>from pathlib import Path\nfrom typing import Dict, Any\nfrom rdetoolkit.impl.input_controller import (\n    InvoiceChecker, ExcelInvoiceChecker, RDEFormatChecker,\n    MultiFileChecker, SmartTableChecker\n)\n\nclass InputProcessingPipeline:\n    \"\"\"Comprehensive input processing pipeline for multiple formats.\"\"\"\n\n    def __init__(self, base_temp_dir: str):\n        self.base_temp_dir = Path(base_temp_dir)\n        self.base_temp_dir.mkdir(parents=True, exist_ok=True)\n\n    def process_directory(self, input_dir: str) -&gt; Dict[str, Any]:\n        \"\"\"Process a directory with automatic format detection.\"\"\"\n\n        input_path = Path(input_dir)\n        temp_dir = self.base_temp_dir / f\"temp_{input_path.name}\"\n        temp_dir.mkdir(exist_ok=True)\n\n        result = {\n            'input_dir': str(input_path),\n            'input_type': None,\n            'raw_files': None,\n            'metadata_file': None,\n            'success': False,\n            'error': None,\n            'stats': {}\n        }\n\n        try:\n            # Detect format\n            checker = self._detect_format(input_path, temp_dir)\n            result['input_type'] = checker.checker_type\n\n            # Process files\n            raw_files, metadata_file = checker.parse(input_path)\n\n            result['raw_files'] = raw_files\n            result['metadata_file'] = metadata_file\n            result['success'] = True\n\n            # Calculate statistics\n            result['stats'] = self._calculate_stats(raw_files)\n\n            print(f\"\u2705 {input_path.name}: {checker.checker_type} format, {len(raw_files)} groups\")\n\n        except Exception as e:\n            result['error'] = str(e)\n            print(f\"\u274c {input_path.name}: {e}\")\n\n        return result\n\n    def _detect_format(self, input_path: Path, temp_dir: Path):\n        \"\"\"Detect input format and return appropriate checker.\"\"\"\n\n        input_files = list(input_path.glob(\"*\"))\n\n        # SmartTable detection\n        if any(f.name.startswith(\"smarttable_\") and f.suffix.lower() in [\".xlsx\", \".csv\", \".tsv\"] for f in input_files):\n            return SmartTableChecker(temp_dir)\n\n        # Excel Invoice detection\n        if any(f.suffix.lower() in [\".xls\", \".xlsx\"] and f.stem.endswith(\"_excel_invoice\") for f in input_files):\n            return ExcelInvoiceChecker(temp_dir)\n\n        # RDE Format detection (single ZIP)\n        zip_files = [f for f in input_files if f.suffix.lower() == \".zip\"]\n        if len(zip_files) == 1 and len(input_files) == 1:\n            return RDEFormatChecker(temp_dir)\n\n        # Multi-file detection\n        if len(input_files) &gt; 1:\n            return MultiFileChecker(temp_dir)\n\n        # Default to Invoice checker\n        return InvoiceChecker(temp_dir)\n\n    def _calculate_stats(self, raw_files) -&gt; Dict[str, Any]:\n        \"\"\"Calculate statistics for processed files.\"\"\"\n\n        total_files = sum(len(group) for group in raw_files)\n        group_sizes = [len(group) for group in raw_files]\n\n        return {\n            'total_groups': len(raw_files),\n            'total_files': total_files,\n            'avg_files_per_group': total_files / len(raw_files) if raw_files else 0,\n            'min_group_size': min(group_sizes) if group_sizes else 0,\n            'max_group_size': max(group_sizes) if group_sizes else 0\n        }\n\n    def process_multiple_directories(self, directories: list[str]) -&gt; Dict[str, Any]:\n        \"\"\"Process multiple directories and return summary.\"\"\"\n\n        results = []\n\n        for directory in directories:\n            result = self.process_directory(directory)\n            results.append(result)\n\n        # Generate summary\n        successful = [r for r in results if r['success']]\n        failed = [r for r in results if not r['success']]\n\n        format_counts = {}\n        for result in successful:\n            format_type = result['input_type']\n            format_counts[format_type] = format_counts.get(format_type, 0) + 1\n\n        summary = {\n            'total_directories': len(directories),\n            'successful': len(successful),\n            'failed': len(failed),\n            'format_distribution': format_counts,\n            'results': results\n        }\n\n        print(f\"\\n\ud83d\udcca Processing Summary:\")\n        print(f\"  Total directories: {summary['total_directories']}\")\n        print(f\"  Successful: {summary['successful']}\")\n        print(f\"  Failed: {summary['failed']}\")\n        print(f\"  Format distribution: {summary['format_distribution']}\")\n\n        return summary\n\n# Usage\npipeline = InputProcessingPipeline(\"temp_processing\")\n\n# Process single directory\nresult = pipeline.process_directory(\"sample_input\")\n\n# Process multiple directories\ndirectories = [\"excel_data\", \"smarttable_data\", \"rde_data\", \"multi_files\"]\nsummary = pipeline.process_multiple_directories(directories)\n</code></pre>"},{"location":"rdetoolkit/impl/input_controller/#advanced-rde-format-processing","title":"Advanced RDE Format Processing","text":"<pre><code>from pathlib import Path\nfrom collections import defaultdict\nfrom rdetoolkit.impl.input_controller import RDEFormatChecker\n\ndef analyze_rde_structure(input_dir: str, temp_dir: str):\n    \"\"\"Analyze RDE format structure in detail.\"\"\"\n\n    input_path = Path(input_dir)\n    temp_path = Path(temp_dir)\n    temp_path.mkdir(parents=True, exist_ok=True)\n\n    # Validate RDE format\n    zip_files = list(input_path.glob(\"*.zip\"))\n\n    if len(zip_files) != 1:\n        raise ValueError(f\"RDE format requires exactly one ZIP file, found {len(zip_files)}\")\n\n    zip_file = zip_files[0]\n    print(f\"\ud83d\udce6 RDE ZIP file: {zip_file.name}\")\n\n    # Process with RDE checker\n    checker = RDEFormatChecker(temp_path)\n\n    try:\n        raw_files, _ = checker.parse(input_path)\n\n        print(f\"\u2705 RDE format analysis:\")\n        print(f\"  Directory groups: {len(raw_files)}\")\n\n        # Analyze structure\n        structure_info = defaultdict(list)\n        total_files = 0\n\n        for i, group in enumerate(raw_files):\n            group_info = {\n                'group_index': i,\n                'file_count': len(group),\n                'file_types': defaultdict(int),\n                'directories': set(),\n                'files': []\n            }\n\n            for file_path in group:\n                total_files += 1\n                group_info['files'].append(file_path)\n                group_info['file_types'][file_path.suffix.lower()] += 1\n                group_info['directories'].add(str(file_path.parent))\n\n            structure_info[i] = group_info\n\n            print(f\"  Group {i}:\")\n            print(f\"    Files: {group_info['file_count']}\")\n            print(f\"    Types: {dict(group_info['file_types'])}\")\n            print(f\"    Directories: {len(group_info['directories'])}\")\n\n        print(f\"  Total files: {total_files}\")\n\n        return raw_files, structure_info\n\n    except Exception as e:\n        print(f\"\u274c RDE analysis failed: {e}\")\n        raise\n\n# Usage\nraw_files, structure = analyze_rde_structure(\n    input_dir=\"rde_archive_input\",\n    temp_dir=\"rde_analysis_temp\"\n)\n</code></pre>"},{"location":"rdetoolkit/impl/input_controller/#performance-notes","title":"Performance Notes","text":"<ul> <li>File pattern detection uses efficient glob operations for fast directory scanning</li> <li>ZIP extraction performance depends on archive size and compression ratio</li> <li>SmartTable CSV generation is optimized for memory efficiency with row-by-row processing</li> <li>Temporary directory operations use system temp space for optimal I/O performance</li> <li>File grouping operations use efficient sorting algorithms based on path structures</li> <li>Memory usage scales linearly with the number of files and their metadata</li> </ul>"},{"location":"rdetoolkit/impl/input_controller/#see-also","title":"See Also","text":"<ul> <li>Compressed Controller - For ZIP archive processing</li> <li>Invoice File Module - For Excel invoice and SmartTable operations</li> <li>Interface Definitions - For checker interface specifications</li> <li>Exceptions Module - For custom exception types</li> <li>RDE Types Models - For type definitions used in input processing</li> </ul>"},{"location":"rdetoolkit/interface/filechecker/","title":"File Checker Interfaces Module","text":"<p>The <code>rdetoolkit.interfaces.filechecker</code> module defines abstract base classes (interfaces) for file checking, input processing, compressed file parsing, and artifact compression operations. These interfaces provide standardized contracts for implementing various file processing components in RDE (Research Data Exchange) workflows, ensuring consistent behavior across different implementation classes.</p>"},{"location":"rdetoolkit/interface/filechecker/#overview","title":"Overview","text":"<p>The file checker interfaces module provides foundational abstractions for:</p> <ul> <li>Input File Processing: Define standard operations for handling and processing input files</li> <li>File Type Checking: Establish contracts for different input file checkers and validators</li> <li>Compressed File Operations: Specify interfaces for parsing and extracting compressed file structures</li> <li>Archive Creation: Define standard operations for creating compressed artifact packages</li> </ul>"},{"location":"rdetoolkit/interface/filechecker/#interfaces","title":"Interfaces","text":""},{"location":"rdetoolkit/interface/filechecker/#iinputfilehelper","title":"IInputFileHelper","text":"<p>Abstract interface for input file helper operations, specifically designed for handling ZIP file operations among input files.</p>"},{"location":"rdetoolkit/interface/filechecker/#abstract-methods-iinputfilechecker","title":"Abstract Methods (IInputFileChecker)","text":""},{"location":"rdetoolkit/interface/filechecker/#get_zipfilesinput_files","title":"get_zipfiles(input_files)","text":"<p>Retrieve ZIP files from a list of input file paths.</p> <pre><code>@abstractmethod\ndef get_zipfiles(input_files: list[Path]) -&gt; ZipFilesPathList\n</code></pre> <p>Parameters:</p> <ul> <li><code>input_files</code> (list[Path]): List of file paths to search for ZIP files</li> </ul> <p>Returns:</p> <ul> <li><code>ZipFilesPathList</code>: List of paths pointing to found ZIP files</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: Must be implemented by concrete classes</li> </ul> <p>Example Implementation:</p> <pre><code>from pathlib import Path\nfrom rdetoolkit.interfaces.filechecker import IInputFileHelper\n\nclass BasicInputFileHelper(IInputFileHelper):\n    def get_zipfiles(self, input_files: list[Path]) -&gt; list[Path]:\n        \"\"\"Find all ZIP files in the input list.\"\"\"\n        return [f for f in input_files if f.suffix.lower() == \".zip\"]\n\n    def unpacked(self, zipfile: Path, target_dir: Path) -&gt; list[Path]:\n        \"\"\"Extract ZIP file and return extracted files.\"\"\"\n        # Implementation details...\n        pass\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#unpackedzipfile-target_dir","title":"unpacked(zipfile, target_dir)","text":"<p>Unpack a ZIP file into a target directory and return paths to extracted files.</p> <pre><code>@abstractmethod\ndef unpacked(zipfile: Path, target_dir: Path) -&gt; UnZipFilesPathList\n</code></pre> <p>Parameters:</p> <ul> <li><code>zipfile</code> (Path): Path to the ZIP file to be unpacked</li> <li><code>target_dir</code> (Path): Directory where ZIP file contents will be extracted</li> </ul> <p>Returns:</p> <ul> <li><code>UnZipFilesPathList</code>: List of paths to the unpacked files</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: Must be implemented by concrete classes</li> </ul>"},{"location":"rdetoolkit/interface/filechecker/#iinputfilechecker","title":"IInputFileChecker","text":"<p>Abstract interface for checking and parsing input files in RDE workflows. This interface defines the structure for classes that handle validation and extraction of information from source input files.</p>"},{"location":"rdetoolkit/interface/filechecker/#abstract-properties","title":"Abstract Properties","text":""},{"location":"rdetoolkit/interface/filechecker/#checker_type","title":"checker_type","text":"<p>Return the type identifier for the checker implementation.</p> <pre><code>@property\n@abstractmethod\ndef checker_type() -&gt; str\n</code></pre> <p>Returns:</p> <ul> <li><code>str</code>: String identifier representing the checker type</li> </ul> <p>Example Implementation:</p> <pre><code>class ExcelInvoiceChecker(IInputFileChecker):\n    @property\n    def checker_type(self) -&gt; str:\n        return \"excel_invoice\"\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#abstract-methods","title":"Abstract Methods","text":""},{"location":"rdetoolkit/interface/filechecker/#parsesrc_input_path","title":"parse(src_input_path)","text":"<p>Parse the source input path and extract relevant file information.</p> <pre><code>@abstractmethod\ndef parse(src_input_path: Path) -&gt; tuple[RawFiles, Path | None]\n</code></pre> <p>Parameters:</p> <ul> <li><code>src_input_path</code> (Path): Path to the source input file(s)</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[RawFiles, Path | None]</code>: Tuple containing extracted raw file data and optional path to additional relevant data</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: Must be implemented by concrete classes</li> </ul> <p>Example Implementation:</p> <pre><code>from pathlib import Path\nfrom rdetoolkit.interfaces.filechecker import IInputFileChecker\n\nclass CustomInputChecker(IInputFileChecker):\n    @property\n    def checker_type(self) -&gt; str:\n        return \"custom\"\n\n    def parse(self, src_input_path: Path) -&gt; tuple[list, Path | None]:\n        \"\"\"Parse input files and return raw files and metadata.\"\"\"\n        # Find all files in the input directory\n        input_files = list(src_input_path.glob(\"*\"))\n\n        # Group files and extract metadata\n        raw_files = [(f,) for f in input_files if f.suffix != \".meta\"]\n        metadata_file = next((f for f in input_files if f.suffix == \".meta\"), None)\n\n        return raw_files, metadata_file\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#icompressedfilestructparser","title":"ICompressedFileStructParser","text":"<p>Abstract interface for parsing the structure of compressed files, focusing on understanding internal organization and extracting structural information.</p>"},{"location":"rdetoolkit/interface/filechecker/#abstract-methods_1","title":"Abstract Methods","text":""},{"location":"rdetoolkit/interface/filechecker/#readzipfile-target_path","title":"read(zipfile, target_path)","text":"<p>Read and parse the structure of a compressed file.</p> <pre><code>@abstractmethod\ndef read(zipfile: Path, target_path: Path) -&gt; list[tuple[Path, ...]]\n</code></pre> <p>Parameters:</p> <ul> <li><code>zipfile</code> (Path): Path to the compressed file to be read</li> <li><code>target_path</code> (Path): Path where contents might be extracted or analyzed</li> </ul> <p>Returns:</p> <ul> <li><code>list[tuple[Path, ...]]</code>: List of tuples containing paths or relevant data extracted from the compressed file</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: Must be implemented by concrete classes</li> </ul> <p>Example Implementation:</p> <pre><code>from pathlib import Path\nfrom rdetoolkit.interfaces.filechecker import ICompressedFileStructParser\n\nclass StandardStructParser(ICompressedFileStructParser):\n    def read(self, zipfile: Path, target_path: Path) -&gt; list[tuple[Path, ...]]:\n        \"\"\"Extract and analyze compressed file structure.\"\"\"\n        import zipfile\n\n        # Extract files\n        with zipfile.ZipFile(zipfile, 'r') as zip_ref:\n            zip_ref.extractall(target_path)\n\n        # Group files by directory structure\n        extracted_files = list(target_path.rglob(\"*\"))\n        file_groups = []\n\n        # Group files by their parent directory\n        from collections import defaultdict\n        dir_groups = defaultdict(list)\n\n        for file_path in extracted_files:\n            if file_path.is_file():\n                dir_groups[file_path.parent].append(file_path)\n\n        return [tuple(files) for files in dir_groups.values()]\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#iartifactpackagecompressor","title":"IArtifactPackageCompressor","text":"<p>Abstract interface for compressing artifacts into archive packages, providing standardized operations for creating compressed archives with exclusion patterns.</p>"},{"location":"rdetoolkit/interface/filechecker/#abstract-properties_1","title":"Abstract Properties","text":""},{"location":"rdetoolkit/interface/filechecker/#exclude_patterns","title":"exclude_patterns","text":"<p>Get the list of exclusion patterns used during compression.</p> <pre><code>@property\n@abstractmethod\ndef exclude_patterns() -&gt; list[str]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[str]</code>: List of exclusion patterns (typically regex patterns)</li> </ul>"},{"location":"rdetoolkit/interface/filechecker/#abstract-methods_2","title":"Abstract Methods","text":""},{"location":"rdetoolkit/interface/filechecker/#archiveoutput_zip","title":"archive(output_zip)","text":"<p>Create an archive from source files with applied exclusion patterns.</p> <pre><code>@abstractmethod\ndef archive(output_zip: str | Path) -&gt; list[Path]\n</code></pre> <p>Parameters:</p> <ul> <li><code>output_zip</code> (str | Path): Path to the output archive file</li> </ul> <p>Returns:</p> <ul> <li><code>list[Path]</code>: List of paths to files that were included in the archive</li> </ul> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: Must be implemented by concrete classes</li> </ul> <p>Example Implementation:</p> <pre><code>from pathlib import Path\nimport zipfile\nimport re\nfrom rdetoolkit.interfaces.filechecker import IArtifactPackageCompressor\n\nclass BasicArchiveCompressor(IArtifactPackageCompressor):\n    def __init__(self, source_dir: Path, patterns: list[str]):\n        self.source_dir = source_dir\n        self._exclude_patterns = patterns\n\n    @property\n    def exclude_patterns(self) -&gt; list[str]:\n        return self._exclude_patterns\n\n    def archive(self, output_zip: str | Path) -&gt; list[Path]:\n        \"\"\"Create ZIP archive with exclusion patterns.\"\"\"\n        output_path = Path(output_zip)\n        included_files = []\n\n        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in self.source_dir.rglob(\"*\"):\n                if file_path.is_file() and not self._is_excluded(file_path):\n                    rel_path = file_path.relative_to(self.source_dir)\n                    zipf.write(file_path, rel_path)\n                    included_files.append(rel_path)\n\n        return included_files\n\n    def _is_excluded(self, file_path: Path) -&gt; bool:\n        \"\"\"Check if file matches exclusion patterns.\"\"\"\n        file_str = str(file_path)\n        return any(re.search(pattern, file_str) for pattern in self._exclude_patterns)\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/interface/filechecker/#implementing-a-custom-input-file-checker","title":"Implementing a Custom Input File Checker","text":"<pre><code>from pathlib import Path\nfrom typing import Optional\nfrom rdetoolkit.interfaces.filechecker import IInputFileChecker\nfrom rdetoolkit.models.rde2types import RawFiles\n\nclass DatasetInputChecker(IInputFileChecker):\n    \"\"\"Custom checker for dataset input files.\"\"\"\n\n    def __init__(self, temp_dir: Path):\n        self.temp_dir = temp_dir\n        self.temp_dir.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def checker_type(self) -&gt; str:\n        return \"dataset\"\n\n    def parse(self, src_input_path: Path) -&gt; tuple[RawFiles, Optional[Path]]:\n        \"\"\"Parse dataset directory structure.\"\"\"\n\n        # Find data files and metadata\n        data_files = []\n        metadata_file = None\n\n        for file_path in src_input_path.rglob(\"*\"):\n            if file_path.is_file():\n                if file_path.name == \"metadata.json\":\n                    metadata_file = file_path\n                elif file_path.suffix.lower() in [\".csv\", \".tsv\", \".xlsx\"]:\n                    data_files.append(file_path)\n\n        # Group files by dataset (assuming directory structure)\n        dataset_groups = {}\n        for data_file in data_files:\n            # Use parent directory as dataset identifier\n            dataset_name = data_file.parent.name\n            if dataset_name not in dataset_groups:\n                dataset_groups[dataset_name] = []\n            dataset_groups[dataset_name].append(data_file)\n\n        # Convert to RawFiles format\n        raw_files = [tuple(files) for files in dataset_groups.values()]\n\n        return raw_files, metadata_file\n\n# Usage\ndef process_dataset_input(input_dir: str, temp_dir: str):\n    \"\"\"Process dataset input using custom checker.\"\"\"\n\n    checker = DatasetInputChecker(Path(temp_dir))\n\n    try:\n        raw_files, metadata = checker.parse(Path(input_dir))\n\n        print(f\"Checker type: {checker.checker_type}\")\n        print(f\"Dataset groups: {len(raw_files)}\")\n        print(f\"Metadata file: {metadata.name if metadata else 'None'}\")\n\n        for i, group in enumerate(raw_files):\n            print(f\"  Group {i+1}: {len(group)} files\")\n            for file_path in group:\n                print(f\"    - {file_path.name}\")\n\n        return raw_files, metadata\n\n    except Exception as e:\n        print(f\"Processing failed: {e}\")\n        raise\n\n# Usage\nraw_files, metadata = process_dataset_input(\"dataset_input\", \"temp\")\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#implementing-a-compressed-file-structure-parser","title":"Implementing a Compressed File Structure Parser","text":"<pre><code>from pathlib import Path\nimport zipfile\nimport json\nfrom rdetoolkit.interfaces.filechecker import ICompressedFileStructParser\n\nclass MetadataStructParser(ICompressedFileStructParser):\n    \"\"\"Parser that groups files based on metadata structure.\"\"\"\n\n    def __init__(self, metadata_config: dict):\n        self.metadata_config = metadata_config\n\n    def read(self, zipfile_path: Path, target_path: Path) -&gt; list[tuple[Path, ...]]:\n        \"\"\"Extract and group files based on metadata structure.\"\"\"\n\n        # Extract ZIP file\n        with zipfile.ZipFile(zipfile_path, 'r') as zip_ref:\n            zip_ref.extractall(target_path)\n\n        # Find metadata files\n        metadata_files = list(target_path.glob(\"**/metadata.json\"))\n\n        if not metadata_files:\n            # Fallback: group by directory\n            return self._group_by_directory(target_path)\n\n        # Group files based on metadata\n        file_groups = []\n\n        for metadata_file in metadata_files:\n            try:\n                with open(metadata_file, 'r') as f:\n                    metadata = json.load(f)\n\n                # Find related files based on metadata\n                related_files = self._find_related_files(\n                    metadata_file.parent, metadata\n                )\n\n                if related_files:\n                    file_groups.append(tuple(related_files))\n\n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"Error reading metadata {metadata_file}: {e}\")\n                continue\n\n        return file_groups\n\n    def _group_by_directory(self, target_path: Path) -&gt; list[tuple[Path, ...]]:\n        \"\"\"Fallback grouping by directory structure.\"\"\"\n        from collections import defaultdict\n\n        dir_groups = defaultdict(list)\n\n        for file_path in target_path.rglob(\"*\"):\n            if file_path.is_file():\n                dir_groups[file_path.parent].append(file_path)\n\n        return [tuple(files) for files in dir_groups.values()]\n\n    def _find_related_files(self, base_dir: Path, metadata: dict) -&gt; list[Path]:\n        \"\"\"Find files related to metadata entry.\"\"\"\n        related_files = []\n\n        # Look for files mentioned in metadata\n        if \"files\" in metadata:\n            for file_info in metadata[\"files\"]:\n                if isinstance(file_info, dict) and \"path\" in file_info:\n                    file_path = base_dir / file_info[\"path\"]\n                    if file_path.exists():\n                        related_files.append(file_path)\n                elif isinstance(file_info, str):\n                    file_path = base_dir / file_info\n                    if file_path.exists():\n                        related_files.append(file_path)\n\n        # Add metadata file itself\n        metadata_file = base_dir / \"metadata.json\"\n        if metadata_file.exists():\n            related_files.append(metadata_file)\n\n        return related_files\n\n# Usage\ndef parse_structured_archive(archive_path: str, extract_dir: str):\n    \"\"\"Parse archive with metadata-based structure.\"\"\"\n\n    # Configuration for metadata parsing\n    config = {\n        \"group_by_metadata\": True,\n        \"required_fields\": [\"files\", \"dataset_id\"]\n    }\n\n    parser = MetadataStructParser(config)\n\n    try:\n        file_groups = parser.read(Path(archive_path), Path(extract_dir))\n\n        print(f\"Extracted {len(file_groups)} file groups:\")\n\n        for i, group in enumerate(file_groups):\n            print(f\"  Group {i+1}: {len(group)} files\")\n\n            # Check for metadata in group\n            metadata_files = [f for f in group if f.name == \"metadata.json\"]\n            if metadata_files:\n                print(f\"    Metadata: {metadata_files[0]}\")\n\n            # List other files\n            other_files = [f for f in group if f.name != \"metadata.json\"]\n            for file_path in other_files:\n                print(f\"    Data: {file_path.name}\")\n\n        return file_groups\n\n    except Exception as e:\n        print(f\"Parsing failed: {e}\")\n        raise\n\n# Usage\ngroups = parse_structured_archive(\"structured_data.zip\", \"extracted\")\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#implementing-a-custom-archive-compressor","title":"Implementing a Custom Archive Compressor","text":"<pre><code>from pathlib import Path\nimport zipfile\nimport tarfile\nimport re\nfrom typing import Union\nfrom rdetoolkit.interfaces.filechecker import IArtifactPackageCompressor\n\nclass FlexibleArchiveCompressor(IArtifactPackageCompressor):\n    \"\"\"Flexible compressor supporting multiple archive formats.\"\"\"\n\n    def __init__(self, source_dir: Path, exclude_patterns: list[str],\n                 format_type: str = \"zip\"):\n        self.source_dir = source_dir\n        self._exclude_patterns = exclude_patterns\n        self.format_type = format_type.lower()\n\n        # Compile regex patterns for efficiency\n        self._compiled_patterns = [re.compile(pattern) for pattern in exclude_patterns]\n\n    @property\n    def exclude_patterns(self) -&gt; list[str]:\n        return self._exclude_patterns\n\n    @exclude_patterns.setter\n    def exclude_patterns(self, patterns: list[str]):\n        self._exclude_patterns = patterns\n        self._compiled_patterns = [re.compile(pattern) for pattern in patterns]\n\n    def archive(self, output_path: Union[str, Path]) -&gt; list[Path]:\n        \"\"\"Create archive in specified format.\"\"\"\n\n        output_path = Path(output_path)\n\n        # Collect files to archive\n        files_to_archive = self._collect_files()\n\n        if self.format_type == \"zip\":\n            return self._create_zip_archive(output_path, files_to_archive)\n        elif self.format_type in [\"tar.gz\", \"tgz\"]:\n            return self._create_tar_archive(output_path, files_to_archive)\n        else:\n            raise ValueError(f\"Unsupported format: {self.format_type}\")\n\n    def _collect_files(self) -&gt; list[tuple[Path, Path]]:\n        \"\"\"Collect files that should be included in archive.\"\"\"\n        files_to_archive = []\n\n        for file_path in self.source_dir.rglob(\"*\"):\n            if file_path.is_file() and not self._is_excluded(file_path):\n                relative_path = file_path.relative_to(self.source_dir)\n                files_to_archive.append((file_path, relative_path))\n\n        return files_to_archive\n\n    def _create_zip_archive(self, output_path: Path,\n                           files: list[tuple[Path, Path]]) -&gt; list[Path]:\n        \"\"\"Create ZIP archive.\"\"\"\n        included_files = []\n\n        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n            for full_path, relative_path in files:\n                zipf.write(full_path, relative_path)\n                included_files.append(relative_path)\n\n        return included_files\n\n    def _create_tar_archive(self, output_path: Path,\n                           files: list[tuple[Path, Path]]) -&gt; list[Path]:\n        \"\"\"Create TAR.GZ archive.\"\"\"\n        included_files = []\n\n        with tarfile.open(output_path, 'w:gz') as tar:\n            for full_path, relative_path in files:\n                tar.add(full_path, arcname=relative_path)\n                included_files.append(relative_path)\n\n        return included_files\n\n    def _is_excluded(self, file_path: Path) -&gt; bool:\n        \"\"\"Check if file should be excluded.\"\"\"\n        file_str = str(file_path)\n        return any(pattern.search(file_str) for pattern in self._compiled_patterns)\n\n    def add_exclusion_pattern(self, pattern: str):\n        \"\"\"Add a new exclusion pattern.\"\"\"\n        self._exclude_patterns.append(pattern)\n        self._compiled_patterns.append(re.compile(pattern))\n\n    def get_archive_stats(self) -&gt; dict:\n        \"\"\"Get statistics about files that would be archived.\"\"\"\n        files_to_archive = self._collect_files()\n\n        total_size = sum(f[0].stat().st_size for f in files_to_archive)\n        file_types = {}\n\n        for full_path, _ in files_to_archive:\n            ext = full_path.suffix.lower() or \"no_extension\"\n            file_types[ext] = file_types.get(ext, 0) + 1\n\n        return {\n            \"total_files\": len(files_to_archive),\n            \"total_size_bytes\": total_size,\n            \"file_types\": file_types,\n            \"exclude_patterns\": self._exclude_patterns\n        }\n\n# Usage\ndef create_flexible_archive(source_dir: str, output_file: str,\n                          archive_type: str = \"zip\"):\n    \"\"\"Create archive with flexible format support.\"\"\"\n\n    # Define exclusion patterns\n    exclude_patterns = [\n        r\"\\.git/\",\n        r\"__pycache__/\",\n        r\"\\.pyc$\",\n        r\"\\.DS_Store$\",\n        r\"\\.log$\",\n        r\"node_modules/\",\n        r\"\\.env$\"\n    ]\n\n    # Create compressor\n    compressor = FlexibleArchiveCompressor(\n        source_dir=Path(source_dir),\n        exclude_patterns=exclude_patterns,\n        format_type=archive_type\n    )\n\n    # Get statistics before archiving\n    stats = compressor.get_archive_stats()\n    print(f\"Archive Statistics:\")\n    print(f\"  Files to archive: {stats['total_files']}\")\n    print(f\"  Total size: {stats['total_size_bytes'] / 1024 / 1024:.2f} MB\")\n    print(f\"  File types: {stats['file_types']}\")\n\n    # Create archive\n    try:\n        included_files = compressor.archive(output_file)\n\n        print(f\"\u2705 Archive created successfully:\")\n        print(f\"  Output: {output_file}\")\n        print(f\"  Format: {archive_type}\")\n        print(f\"  Files included: {len(included_files)}\")\n\n        return included_files\n\n    except Exception as e:\n        print(f\"\u274c Archive creation failed: {e}\")\n        raise\n\n# Usage examples\nzip_files = create_flexible_archive(\"my_project\", \"backup.zip\", \"zip\")\ntar_files = create_flexible_archive(\"my_project\", \"backup.tar.gz\", \"tar.gz\")\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#interface-based-plugin-system","title":"Interface-Based Plugin System","text":"<pre><code>from pathlib import Path\nfrom typing import Type, Dict\nfrom rdetoolkit.interfaces.filechecker import IInputFileChecker\n\nclass CheckerRegistry:\n    \"\"\"Registry for managing different input file checkers.\"\"\"\n\n    def __init__(self):\n        self._checkers: Dict[str, Type[IInputFileChecker]] = {}\n\n    def register_checker(self, checker_class: Type[IInputFileChecker]):\n        \"\"\"Register a new checker class.\"\"\"\n        # Create temporary instance to get type\n        temp_instance = checker_class(Path(\"temp\"))\n        checker_type = temp_instance.checker_type\n\n        self._checkers[checker_type] = checker_class\n        print(f\"Registered checker: {checker_type}\")\n\n    def get_checker(self, checker_type: str, temp_dir: Path) -&gt; IInputFileChecker:\n        \"\"\"Get checker instance by type.\"\"\"\n        if checker_type not in self._checkers:\n            raise ValueError(f\"Unknown checker type: {checker_type}\")\n\n        return self._checkers[checker_type](temp_dir)\n\n    def list_checkers(self) -&gt; list[str]:\n        \"\"\"List all registered checker types.\"\"\"\n        return list(self._checkers.keys())\n\n    def auto_detect_checker(self, input_dir: Path, temp_dir: Path) -&gt; IInputFileChecker:\n        \"\"\"Auto-detect appropriate checker based on input files.\"\"\"\n\n        input_files = list(input_dir.glob(\"*\"))\n\n        # Simple heuristics for auto-detection\n        if any(f.name.startswith(\"smarttable_\") for f in input_files):\n            return self.get_checker(\"smarttable\", temp_dir)\n        elif any(f.stem.endswith(\"_excel_invoice\") for f in input_files):\n            return self.get_checker(\"excel_invoice\", temp_dir)\n        elif len([f for f in input_files if f.suffix == \".zip\"]) == 1:\n            return self.get_checker(\"rde_format\", temp_dir)\n        elif len(input_files) &gt; 1:\n            return self.get_checker(\"multifile\", temp_dir)\n        else:\n            return self.get_checker(\"invoice\", temp_dir)\n\n# Example usage\ndef setup_checker_system():\n    \"\"\"Set up a plugin-based checker system.\"\"\"\n\n    registry = CheckerRegistry()\n\n    # Register built-in checkers (would normally import from actual modules)\n    # registry.register_checker(ExcelInvoiceChecker)\n    # registry.register_checker(SmartTableChecker)\n    # registry.register_checker(RDEFormatChecker)\n\n    # Register custom checkers\n    registry.register_checker(DatasetInputChecker)\n\n    return registry\n\ndef process_with_auto_detection(input_dir: str, temp_dir: str):\n    \"\"\"Process input with automatic checker detection.\"\"\"\n\n    registry = setup_checker_system()\n\n    input_path = Path(input_dir)\n    temp_path = Path(temp_dir)\n    temp_path.mkdir(parents=True, exist_ok=True)\n\n    # Auto-detect appropriate checker\n    checker = registry.auto_detect_checker(input_path, temp_path)\n\n    print(f\"Auto-detected checker: {checker.checker_type}\")\n\n    # Process input\n    raw_files, metadata = checker.parse(input_path)\n\n    print(f\"Processing results:\")\n    print(f\"  File groups: {len(raw_files)}\")\n    print(f\"  Metadata: {metadata.name if metadata else 'None'}\")\n\n    return raw_files, metadata\n\n# Usage\nraw_files, metadata = process_with_auto_detection(\"input_data\", \"temp\")\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#interface-implementation-guidelines","title":"Interface Implementation Guidelines","text":""},{"location":"rdetoolkit/interface/filechecker/#best-practices-for-interface-implementation","title":"Best Practices for Interface Implementation","text":"<ol> <li>Consistent Error Handling:</li> </ol> <pre><code>class MyChecker(IInputFileChecker):\n    def parse(self, src_input_path: Path) -&gt; tuple[RawFiles, Path | None]:\n        try:\n            # Implementation logic\n            pass\n        except FileNotFoundError as e:\n            raise ValueError(f\"Input path not found: {src_input_path}\") from e\n        except Exception as e:\n            raise RuntimeError(f\"Parsing failed: {e}\") from e\n</code></pre> <ol> <li>Type Safety:</li> </ol> <pre><code>from typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from rdetoolkit.models.rde2types import RawFiles\n\nclass TypeSafeChecker(IInputFileChecker):\n    def parse(self, src_input_path: Path) -&gt; tuple['RawFiles', Path | None]:\n        # Implementation with proper type hints\n        pass\n</code></pre> <ol> <li>Resource Management:</li> </ol> <pre><code>class ResourceManagedChecker(IInputFileChecker):\n    def __init__(self, temp_dir: Path):\n        self.temp_dir = temp_dir\n        self._cleanup_files: list[Path] = []\n\n    def parse(self, src_input_path: Path) -&gt; tuple[RawFiles, Path | None]:\n        try:\n            # Processing logic\n            pass\n        finally:\n            self._cleanup_temp_files()\n\n    def _cleanup_temp_files(self):\n        for file_path in self._cleanup_files:\n            if file_path.exists():\n                file_path.unlink()\n</code></pre> <ol> <li>Documentation and Validation:</li> </ol> <pre><code>class WellDocumentedChecker(IInputFileChecker):\n    \"\"\"A well-documented checker implementation.\n\n    This checker handles XYZ format files and provides\n    ABC functionality for DEF workflows.\n    \"\"\"\n\n    @property\n    def checker_type(self) -&gt; str:\n        return \"well_documented\"\n\n    def parse(self, src_input_path: Path) -&gt; tuple[RawFiles, Path | None]:\n        \"\"\"Parse input files with validation.\n\n        Args:\n            src_input_path: Directory containing input files\n\n        Returns:\n            Tuple of (raw_files, metadata_file)\n\n        Raises:\n            ValueError: If input validation fails\n            FileNotFoundError: If required files are missing\n        \"\"\"\n        self._validate_input(src_input_path)\n        # Implementation logic...\n\n    def _validate_input(self, input_path: Path):\n        \"\"\"Validate input directory and files.\"\"\"\n        if not input_path.exists():\n            raise FileNotFoundError(f\"Input directory not found: {input_path}\")\n\n        if not input_path.is_dir():\n            raise ValueError(f\"Input path must be a directory: {input_path}\")\n</code></pre>"},{"location":"rdetoolkit/interface/filechecker/#see-also","title":"See Also","text":"<ul> <li>Input Controller Implementation - For concrete implementations of these interfaces</li> <li>Compressed Controller Implementation - For compressed file parser and compressor implementations</li> <li>RDE Types Models - For type definitions used in interface contracts</li> <li>Exceptions Module - For custom exception types used in implementations</li> </ul>"},{"location":"rdetoolkit/models/config/","title":"Configuration Module","text":"<p>The <code>rdetoolkit.config</code> module provides comprehensive configuration management for RDE (Research Data Exchange) workflows using Pydantic models. This module defines system settings, processing mode configurations, and validation rules to ensure proper toolkit operation and data handling.</p>"},{"location":"rdetoolkit/models/config/#overview","title":"Overview","text":"<p>The configuration module offers structured configuration management with:</p> <ul> <li>System Settings: Core RDEToolkit operational parameters and data handling options</li> <li>Mode-Specific Settings: Specialized configurations for different processing modes (MultiDataTile, etc.)</li> <li>Validation Rules: Built-in validation to ensure configuration consistency and correctness</li> <li>Extensible Design: Support for additional configuration sections through Pydantic's extra fields</li> </ul>"},{"location":"rdetoolkit/models/config/#classes","title":"Classes","text":""},{"location":"rdetoolkit/models/config/#systemsettings","title":"SystemSettings","text":"<p>Core system configuration model that defines fundamental RDEToolkit operational parameters.</p>"},{"location":"rdetoolkit/models/config/#constructor","title":"Constructor","text":"<pre><code>SystemSettings(\n    extended_mode: str | None = None,\n    save_raw: bool = False,\n    save_nonshared_raw: bool = True,\n    save_thumbnail_image: bool = False,\n    magic_variable: bool = False\n)\n</code></pre> <p>Parameters: - <code>extended_mode</code> (str | None): Processing mode selection ('rdeformat', 'MultiDataTile', or None) - <code>save_raw</code> (bool): Enable automatic saving of raw data to the raw directory - <code>save_nonshared_raw</code> (bool): Enable saving of non-shared raw data - <code>save_thumbnail_image</code> (bool): Enable automatic thumbnail generation from main images - <code>magic_variable</code> (bool): Enable filename variable substitution with '${filename}' syntax</p>"},{"location":"rdetoolkit/models/config/#attributes","title":"Attributes","text":"<ul> <li><code>extended_mode</code> (str | None): The current processing mode</li> <li><code>save_raw</code> (bool): Raw data auto-save setting</li> <li><code>save_nonshared_raw</code> (bool): Non-shared raw data save setting</li> <li><code>save_thumbnail_image</code> (bool): Thumbnail auto-generation setting</li> <li><code>magic_variable</code> (bool): Magic variable substitution setting</li> </ul>"},{"location":"rdetoolkit/models/config/#validators","title":"Validators","text":""},{"location":"rdetoolkit/models/config/#check_at_least_one_save_option_enabled","title":"check_at_least_one_save_option_enabled()","text":"<p>Validates that at least one save option is enabled to prevent data loss.</p> <pre><code>@model_validator(mode='after')\ndef check_at_least_one_save_option_enabled() -&gt; SystemSettings\n</code></pre> <p>Returns: - <code>SystemSettings</code>: The validated model instance</p> <p>Raises: - <code>ValueError</code>: If both 'save_raw' and 'save_nonshared_raw' are False</p> <p>Example: <pre><code>from rdetoolkit.models.config import SystemSettings\n\n# Valid configuration\nvalid_config = SystemSettings(\n    extended_mode=\"MultiDataTile\",\n    save_raw=True,\n    save_nonshared_raw=False\n)\n\n# Invalid configuration - will raise ValueError\ntry:\n    invalid_config = SystemSettings(\n        save_raw=False,\n        save_nonshared_raw=False\n    )\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/models/config/#multidatatilesettings","title":"MultiDataTileSettings","text":"<p>Configuration model for MultiDataTile processing mode settings.</p>"},{"location":"rdetoolkit/models/config/#constructor_1","title":"Constructor","text":"<pre><code>MultiDataTileSettings(ignore_errors: bool = False)\n</code></pre> <p>Parameters: - <code>ignore_errors</code> (bool): Continue processing when errors are encountered instead of stopping</p>"},{"location":"rdetoolkit/models/config/#attributes_1","title":"Attributes","text":"<ul> <li><code>ignore_errors</code> (bool): Error handling behavior for MultiDataTile processing</li> </ul> <p>Example: <pre><code>from rdetoolkit.models.config import MultiDataTileSettings\n\n# Configure MultiDataTile to continue on errors\nmdt_settings = MultiDataTileSettings(ignore_errors=True)\nprint(f\"Ignore errors: {mdt_settings.ignore_errors}\")\n</code></pre></p>"},{"location":"rdetoolkit/models/config/#config","title":"Config","text":"<p>Main configuration class that combines all settings sections with support for additional custom fields.</p>"},{"location":"rdetoolkit/models/config/#constructor_2","title":"Constructor","text":"<pre><code>Config(\n    system: SystemSettings = SystemSettings(),\n    multidata_tile: MultiDataTileSettings | None = MultiDataTileSettings(),\n    **kwargs\n)\n</code></pre> <p>Parameters: - <code>system</code> (SystemSettings): System-related settings - <code>multidata_tile</code> (MultiDataTileSettings | None): MultiDataTile-specific settings - <code>**kwargs</code>: Additional configuration fields (enabled by <code>extra=\"allow\"</code>)</p>"},{"location":"rdetoolkit/models/config/#attributes_2","title":"Attributes","text":"<ul> <li><code>system</code> (SystemSettings): Core system configuration</li> <li><code>multidata_tile</code> (MultiDataTileSettings | None): MultiDataTile processing settings</li> </ul> <p>Example: <pre><code>from rdetoolkit.models.config import Config, SystemSettings, MultiDataTileSettings\n\n# Create comprehensive configuration\nconfig = Config(\n    system=SystemSettings(\n        extended_mode=\"MultiDataTile\",\n        save_raw=True,\n        magic_variable=True\n    ),\n    multidata_tile=MultiDataTileSettings(ignore_errors=False)\n)\n</code></pre></p>"},{"location":"rdetoolkit/models/config/#performance-notes","title":"Performance Notes","text":"<ul> <li>Configuration loading is optimized for startup performance with lazy validation</li> <li>Pydantic models provide efficient serialization/deserialization</li> <li>Environment variablereading is cached by the OS</li> <li>JSON file parsing is fast for typical configuration sizes</li> <li>Model validation occurs once during creation, not on every access</li> </ul>"},{"location":"rdetoolkit/models/config/#see-also","title":"See Also","text":"<ul> <li>Workflows Module - For configuration usage in processing workflows</li> <li>System Overview - For understanding RDEToolkit system architecture</li> <li>Processing Modes - For details on extended_mode options</li> <li>Pydantic Documentation - For advanced model features and validation</li> </ul>"},{"location":"rdetoolkit/models/invoice/","title":"Invoice Module","text":"<p>The <code>rdetoolkit.models.invoice</code> module provides comprehensive functionality for managing Excel-based invoice templates and term registries in RDE systems. This module handles the creation, validation, and manipulation of invoice data structures with support for both general and specific term management.</p>"},{"location":"rdetoolkit/models/invoice/#overview","title":"Overview","text":"<p>The invoice module implements a complete system for managing RDE invoice data with the following capabilities:</p> <ul> <li>Excel Template Generation: Structured creation of Excel invoice templates with predefined headers</li> <li>Term Registry Management: Efficient searching and retrieval of general and specific terms</li> <li>Multi-language Support: Support for Japanese and English term descriptions</li> <li>Data Validation: Integration with Pydantic models for robust data validation</li> <li>Flexible Configuration: Configurable template generation with various input modes</li> </ul>"},{"location":"rdetoolkit/models/invoice/#header-classes","title":"Header Classes","text":"<p>The module provides a hierarchical structure of header classes that define the standard Excel invoice template format.</p>"},{"location":"rdetoolkit/models/invoice/#headerrow1","title":"HeaderRow1","text":"<p>Represents the first header row containing format identification.</p>"},{"location":"rdetoolkit/models/invoice/#constructor","title":"Constructor","text":"<pre><code>HeaderRow1(A1: str = \"invoiceList_format_id\")\n</code></pre> <p>Parameters: - <code>A1</code> (str): Format identifier (default: \"invoiceList_format_id\")</p>"},{"location":"rdetoolkit/models/invoice/#example","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import HeaderRow1\n\nheader1 = HeaderRow1()\nprint(header1.A1)  # \"invoiceList_format_id\"\n\n# Custom format ID\ncustom_header1 = HeaderRow1(A1=\"custom_format_v2\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#headerrow2","title":"HeaderRow2","text":"<p>Represents the second header row defining column groupings.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_1","title":"Constructor","text":"<pre><code>HeaderRow2(\n    A2: str = \"data_file_names\",\n    D2_G2: list[str] = [\"basic\"] * 4,\n    H2_M2: list[str] = [\"sample\"] * 6\n)\n</code></pre> <p>Parameters: - <code>A2</code> (str): Data file names identifier (default: \"data_file_names\") - <code>D2_G2</code> (list[str]): Basic data column labels (default: [\"basic\"] * 4) - <code>H2_M2</code> (list[str]): Sample data column labels (default: [\"sample\"] * 6)</p>"},{"location":"rdetoolkit/models/invoice/#example_1","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import HeaderRow2\n\nheader2 = HeaderRow2()\nprint(header2.D2_G2)  # [\"basic\", \"basic\", \"basic\", \"basic\"]\nprint(header2.H2_M2)  # [\"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\"]\n\n# Custom groupings\ncustom_header2 = HeaderRow2(\n    D2_G2=[\"metadata\"] * 4,\n    H2_M2=[\"specimen\"] * 6\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#headerrow3","title":"HeaderRow3","text":"<p>Represents the third header row with English column names.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_2","title":"Constructor","text":"<pre><code>HeaderRow3(\n    A3: str = \"name\",\n    B3: str = \"dataset_title\",\n    C3: str = \"dataOwner\",\n    D3: str = \"dataOwnerId\",\n    E3: str = \"dataName\",\n    F3: str = \"experimentId\",\n    G3: str = \"description\",\n    H3: str = \"names\",\n    I3: str = \"sampleId\",\n    J3: str = \"ownerId\",\n    K3: str = \"composition\",\n    L3: str = \"referenceUrl\",\n    M3: str = \"description\"\n)\n</code></pre> <p>Parameters: All parameters are column name identifiers with their respective defaults.</p>"},{"location":"rdetoolkit/models/invoice/#example_2","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import HeaderRow3\n\nheader3 = HeaderRow3()\nprint(header3.A3)  # \"name\"\nprint(header3.I3)  # \"sampleId\"\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#headerrow4","title":"HeaderRow4","text":"<p>Represents the fourth header row with Japanese column descriptions.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_3","title":"Constructor","text":"<pre><code>HeaderRow4(\n    A4: str = \"\u30d5\u30a1\u30a4\u30eb\u540d\\n(\u62e1\u5f35\u5b50\u3082\u542b\u3081\u5165\u529b)\\n(\u5165\u529b\u4f8b:\u25cb\u25cb.txt)\",\n    B4: str = \"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u540d\\n(\u5fc5\u9808)\",\n    C4: str = \"\u30c7\u30fc\u30bf\u6240\u6709\u8005\\n(NIMS User ID)\",\n    D4: str = \"NIMS user UUID\\n(\u5fc5\u9808)\",\n    E4: str = \"\u30c7\u30fc\u30bf\u540d\\n(\u5fc5\u9808)\",\n    F4: str = \"\u5b9f\u9a13ID\",\n    G4: str = \"\u8aac\u660e\",\n    H4: str = \"\u8a66\u6599\u540d\\n(\u30ed\u30fc\u30ab\u30ebID)\",\n    I4: str = \"\u8a66\u6599UUID\\n(\u5fc5\u9808)\",\n    J4: str = \"\u8a66\u6599\u7ba1\u7406\u8005UUID\",\n    K4: str = \"\u5316\u5b66\u5f0f\u30fb\u7d44\u6210\u5f0f\u30fb\u5206\u5b50\u5f0f\u306a\u3069\",\n    L4: str = \"\u53c2\u8003URL\",\n    M4: str = \"\u8a66\u6599\u306e\u8aac\u660e\"\n)\n</code></pre> <p>Parameters: All parameters are Japanese column descriptions with their respective defaults.</p>"},{"location":"rdetoolkit/models/invoice/#example_3","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import HeaderRow4\n\nheader4 = HeaderRow4()\nprint(header4.B4)  # \"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u540d\\n(\u5fc5\u9808)\"\nprint(header4.I4)  # \"\u8a66\u6599UUID\\n(\u5fc5\u9808)\"\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#fixedheaders","title":"FixedHeaders","text":"<p>Container class that combines all header rows and provides template generation functionality.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_4","title":"Constructor","text":"<pre><code>FixedHeaders(\n    row1: HeaderRow1 = HeaderRow1(),\n    row2: HeaderRow2 = HeaderRow2(),\n    row3: HeaderRow3 = HeaderRow3(),\n    row4: HeaderRow4 = HeaderRow4()\n)\n</code></pre> <p>Parameters: - <code>row1</code> (HeaderRow1): First header row - <code>row2</code> (HeaderRow2): Second header row - <code>row3</code> (HeaderRow3): Third header row - <code>row4</code> (HeaderRow4): Fourth header row</p>"},{"location":"rdetoolkit/models/invoice/#methods","title":"Methods","text":""},{"location":"rdetoolkit/models/invoice/#to_template_dataframe","title":"to_template_dataframe()","text":"<p>Converts the header data to a Polars DataFrame formatted for Excel template generation.</p> <pre><code>def to_template_dataframe() -&gt; pl.DataFrame\n</code></pre> <p>Returns: - <code>pl.DataFrame</code>: A Polars DataFrame with 13 columns (A-M) and 4 rows containing the formatted header data</p> <p>Example:</p> <pre><code>from rdetoolkit.models.invoice import FixedHeaders\nimport polars as pl\n\nheaders = FixedHeaders()\ndf = headers.to_template_dataframe()\n\nprint(df.shape)  # (4, 13)\nprint(df.columns)  # ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']\n\n# Access specific cells\nprint(df[0, 0])  # \"invoiceList_format_id\"\nprint(df[2, 1])  # \"dataset_title\"\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#configuration-classes","title":"Configuration Classes","text":""},{"location":"rdetoolkit/models/invoice/#templateconfig","title":"TemplateConfig","text":"<p>Configuration dataclass for generating Excel invoice templates.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_5","title":"Constructor","text":"<pre><code>TemplateConfig(\n    schema_path: str | Path,\n    general_term_path: str | Path,\n    specific_term_path: str | Path,\n    inputfile_mode: Literal[\"file\", \"folder\"] = \"file\"\n)\n</code></pre> <p>Parameters: - <code>schema_path</code> (str | Path): Path to the invoice schema file - <code>general_term_path</code> (str | Path): Path to the general terms CSV file - <code>specific_term_path</code> (str | Path): Path to the specific terms CSV file - <code>inputfile_mode</code> (Literal[\"file\", \"folder\"]): Input processing mode (default: \"file\")</p>"},{"location":"rdetoolkit/models/invoice/#example_4","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import TemplateConfig\nfrom pathlib import Path\n\nconfig = TemplateConfig(\n    schema_path=\"schemas/invoice_schema.json\",\n    general_term_path=\"terms/general_terms.csv\",\n    specific_term_path=\"terms/specific_terms.csv\",\n    inputfile_mode=\"folder\"\n)\n\n# Using Path objects\nconfig = TemplateConfig(\n    schema_path=Path(\"schemas\") / \"invoice_schema.json\",\n    general_term_path=Path(\"terms\") / \"general_terms.csv\",\n    specific_term_path=Path(\"terms\") / \"specific_terms.csv\"\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#term-registry-classes","title":"Term Registry Classes","text":""},{"location":"rdetoolkit/models/invoice/#basetermregistry","title":"BaseTermRegistry","text":"<p>Abstract base class providing common functionality for term registries.</p>"},{"location":"rdetoolkit/models/invoice/#attributes","title":"Attributes","text":"<ul> <li><code>base_schema</code> (dict): Base schema definition for term data validation</li> </ul> <pre><code>base_schema = {\n    \"term_id\": pl.Utf8,\n    \"key_name\": pl.Utf8,\n    \"ja\": pl.Utf8,\n    \"en\": pl.Utf8,\n}\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#generaltermregistry","title":"GeneralTermRegistry","text":"<p>Registry for managing general terms with search capabilities.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_6","title":"Constructor","text":"<pre><code>GeneralTermRegistry(csv_path: str)\n</code></pre> <p>Parameters: - <code>csv_path</code> (str): Path to the CSV file containing general terms data</p>"},{"location":"rdetoolkit/models/invoice/#attributes_1","title":"Attributes","text":"<ul> <li><code>df</code> (pl.DataFrame): Polars DataFrame containing the loaded term data</li> </ul>"},{"location":"rdetoolkit/models/invoice/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/models/invoice/#searchcolumn-value-out_cols","title":"search(column, value, out_cols)","text":"<p>Generic search method for finding terms based on column values.</p> <pre><code>def search(column: str, value: str, out_cols: list[str]) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>column</code> (str): Column name to search in - <code>value</code> (str): Value to search for - <code>out_cols</code> (list[str]): List of columns to include in the output</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List of dictionaries containing matching rows with specified columns</p> <p>Example:</p> <pre><code>from rdetoolkit.models.invoice import GeneralTermRegistry\n\nregistry = GeneralTermRegistry(\"general_terms.csv\")\n\n# Search for terms by key_name\nresults = registry.search(\"key_name\", \"temperature\", [\"term_id\", \"ja\", \"en\"])\nprint(results)\n# [{\"term_id\": \"T001\", \"ja\": \"\u6e29\u5ea6\", \"en\": \"Temperature\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#by_term_idterm_id","title":"by_term_id(term_id)","text":"<p>Retrieve term details by term ID.</p> <pre><code>def by_term_id(term_id: str) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>term_id</code> (str): The term ID to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List of dictionaries with keys \"term_id\", \"key_name\", \"ja\", \"en\"</p> <p>Example:</p> <pre><code>registry = GeneralTermRegistry(\"general_terms.csv\")\n\nresult = registry.by_term_id(\"T001\")\nprint(result)\n# [{\"term_id\": \"T001\", \"key_name\": \"temperature\", \"ja\": \"\u6e29\u5ea6\", \"en\": \"Temperature\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#by_jaja_text","title":"by_ja(ja_text)","text":"<p>Search for terms using Japanese text.</p> <pre><code>def by_ja(ja_text: str) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>ja_text</code> (str): Japanese text to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List of dictionaries with keys \"term_id\", \"key_name\", \"en\"</p> <p>Example:</p> <pre><code>registry = GeneralTermRegistry(\"general_terms.csv\")\n\nresults = registry.by_ja(\"\u6e29\u5ea6\")\nprint(results)\n# [{\"term_id\": \"T001\", \"key_name\": \"temperature\", \"en\": \"Temperature\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#by_enen_text","title":"by_en(en_text)","text":"<p>Search for terms using English text.</p> <pre><code>def by_en(en_text: str) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>en_text</code> (str): English text to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List of dictionaries with keys \"term_id\", \"key_name\", \"ja\"</p> <p>Example:</p> <pre><code>registry = GeneralTermRegistry(\"general_terms.csv\")\n\nresults = registry.by_en(\"Temperature\")\nprint(results)\n# [{\"term_id\": \"T001\", \"key_name\": \"temperature\", \"ja\": \"\u6e29\u5ea6\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#specifictermregistry","title":"SpecificTermRegistry","text":"<p>Registry for managing specific terms with enhanced search capabilities including sample class support.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_7","title":"Constructor","text":"<pre><code>SpecificTermRegistry(csv_path: str)\n</code></pre> <p>Parameters: - <code>csv_path</code> (str): Path to the CSV file containing specific terms data</p>"},{"location":"rdetoolkit/models/invoice/#attributes_2","title":"Attributes","text":"<ul> <li><code>df</code> (pl.DataFrame): Polars DataFrame with extended schema including sample_class_id</li> </ul>"},{"location":"rdetoolkit/models/invoice/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/models/invoice/#searchcolumns-values-out_cols","title":"search(columns, values, out_cols)","text":"<p>Multi-column search method for complex term queries.</p> <pre><code>def search(columns: list[str], values: list[str], out_cols: list[str]) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>columns</code> (list[str]): List of column names to search in - <code>values</code> (list[str]): List of values to search for (must match columns length) - <code>out_cols</code> (list[str]): List of columns to include in the output</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List of dictionaries containing matching rows</p> <p>Raises: - <code>ValueError</code>: If columns and values lists have different lengths - <code>DataRetrievalError</code>: If data retrieval fails - <code>InvalidSearchParametersError</code>: If search parameters are invalid</p> <p>Example:</p> <pre><code>from rdetoolkit.models.invoice import SpecificTermRegistry\n\nregistry = SpecificTermRegistry(\"specific_terms.csv\")\n\n# Search by multiple criteria\nresults = registry.search(\n    columns=[\"sample_class_id\", \"key_name\"],\n    values=[\"C001\", \"hardness\"],\n    out_cols=[\"term_id\", \"ja\", \"en\"]\n)\nprint(results)\n# [{\"term_id\": \"S001\", \"ja\": \"\u786c\u5ea6\", \"en\": \"Hardness\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#by_term_and_class_idterm_id-sample_class_id","title":"by_term_and_class_id(term_id, sample_class_id)","text":"<p>Search for specific terms by both term ID and sample class ID.</p> <pre><code>def by_term_and_class_id(term_id: str, sample_class_id: str) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>term_id</code> (str): Term ID to search for - <code>sample_class_id</code> (str): Sample class ID to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List with keys \"sample_class_id\", \"term_id\", \"key_name\", \"ja\", \"en\"</p> <p>Example:</p> <pre><code>registry = SpecificTermRegistry(\"specific_terms.csv\")\n\nresults = registry.by_term_and_class_id(\"S001\", \"C001\")\nprint(results)\n# [{\"sample_class_id\": \"C001\", \"term_id\": \"S001\", \"key_name\": \"hardness\", \"ja\": \"\u786c\u5ea6\", \"en\": \"Hardness\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#by_key_namekey_name","title":"by_key_name(key_name)","text":"<p>Search for terms by key name.</p> <pre><code>def by_key_name(key_name: list[str]) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>key_name</code> (list[str]): List containing the key name to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List with keys \"sample_class_id\", \"term_id\", \"key_name\", \"ja\", \"en\"</p> <p>Example:</p> <pre><code>registry = SpecificTermRegistry(\"specific_terms.csv\")\n\nresults = registry.by_key_name([\"hardness\"])\nprint(results)\n# [{\"sample_class_id\": \"C001\", \"term_id\": \"S001\", \"key_name\": \"hardness\", \"ja\": \"\u786c\u5ea6\", \"en\": \"Hardness\"}]\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#by_jaja_text_1","title":"by_ja(ja_text)","text":"<p>Search for specific terms using Japanese text.</p> <pre><code>def by_ja(ja_text: list[str]) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>ja_text</code> (list[str]): List containing Japanese text to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List with keys \"sample_class_id\", \"term_id\", \"key_name\", \"en\"</p>"},{"location":"rdetoolkit/models/invoice/#by_enen_text_1","title":"by_en(en_text)","text":"<p>Search for specific terms using English text.</p> <pre><code>def by_en(en_text: list[str]) -&gt; list[dict[str, Any]]\n</code></pre> <p>Parameters: - <code>en_text</code> (list[str]): List containing English text to search for</p> <p>Returns: - <code>list[dict[str, Any]]</code>: List with keys \"sample_class_id\", \"term_id\", \"key_name\", \"ja\"</p>"},{"location":"rdetoolkit/models/invoice/#attribute-configuration-classes","title":"Attribute Configuration Classes","text":""},{"location":"rdetoolkit/models/invoice/#generalattributeconfig","title":"GeneralAttributeConfig","text":"<p>Configuration dataclass for general attribute handling.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_8","title":"Constructor","text":"<pre><code>GeneralAttributeConfig(\n    type: str,\n    registry: GeneralTermRegistry,\n    prefix: str,\n    attributes: GeneralAttribute | None,\n    requires_class_id: Literal[False]\n)\n</code></pre> <p>Parameters: - <code>type</code> (str): Attribute type identifier - <code>registry</code> (GeneralTermRegistry): General term registry instance - <code>prefix</code> (str): Column prefix for the attributes - <code>attributes</code> (GeneralAttribute | None): General attribute schema definition - <code>requires_class_id</code> (Literal[False]): Always False for general attributes</p>"},{"location":"rdetoolkit/models/invoice/#example_5","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import GeneralAttributeConfig, GeneralTermRegistry\nfrom rdetoolkit.models.invoice_schema import GeneralAttribute\n\nregistry = GeneralTermRegistry(\"general_terms.csv\")\nconfig = GeneralAttributeConfig(\n    type=\"general\",\n    registry=registry,\n    prefix=\"gen_\",\n    attributes=None,  # or GeneralAttribute instance\n    requires_class_id=False\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#specificattributeconfig","title":"SpecificAttributeConfig","text":"<p>Configuration dataclass for specific attribute handling.</p>"},{"location":"rdetoolkit/models/invoice/#constructor_9","title":"Constructor","text":"<pre><code>SpecificAttributeConfig(\n    type: str,\n    registry: SpecificTermRegistry,\n    prefix: str,\n    attributes: SpecificAttribute | None,\n    requires_class_id: Literal[True]\n)\n</code></pre> <p>Parameters: - <code>type</code> (str): Attribute type identifier - <code>registry</code> (SpecificTermRegistry): Specific term registry instance - <code>prefix</code> (str): Column prefix for the attributes - <code>attributes</code> (SpecificAttribute | None): Specific attribute schema definition - <code>requires_class_id</code> (Literal[True]): Always True for specific attributes</p>"},{"location":"rdetoolkit/models/invoice/#example_6","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice import SpecificAttributeConfig, SpecificTermRegistry\nfrom rdetoolkit.models.invoice_schema import SpecificAttribute\n\nregistry = SpecificTermRegistry(\"specific_terms.csv\")\nconfig = SpecificAttributeConfig(\n    type=\"specific\",\n    registry=registry,\n    prefix=\"spec_\",\n    attributes=None,  # or SpecificAttribute instance\n    requires_class_id=True\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/models/invoice/#creating-an-excel-invoice-template","title":"Creating an Excel Invoice Template","text":"<pre><code>from rdetoolkit.models.invoice import FixedHeaders, TemplateConfig\nfrom pathlib import Path\nimport polars as pl\n\n# Create headers\nheaders = FixedHeaders()\n\n# Generate template DataFrame\ntemplate_df = headers.to_template_dataframe()\n\n# Save to Excel\ntemplate_df.write_excel(\"invoice_template.xlsx\", position=\"A1\")\n\nprint(\"Excel template created successfully!\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#working-with-term-registries","title":"Working with Term Registries","text":"<pre><code>from rdetoolkit.models.invoice import GeneralTermRegistry, SpecificTermRegistry\n\n# Initialize registries\ngeneral_registry = GeneralTermRegistry(\"data/general_terms.csv\")\nspecific_registry = SpecificTermRegistry(\"data/specific_terms.csv\")\n\n# Search general terms\ntemp_terms = general_registry.by_en(\"Temperature\")\nprint(\"Temperature terms:\", temp_terms)\n\n# Search specific terms by class and term ID\nhardness_terms = specific_registry.by_term_and_class_id(\"S001\", \"C001\")\nprint(\"Hardness terms:\", hardness_terms)\n\n# Multi-language search\nja_results = general_registry.by_ja(\"\u6e29\u5ea6\")\nen_results = specific_registry.by_en([\"Hardness\"])\n\nprint(\"Japanese search results:\", ja_results)\nprint(\"English search results:\", en_results)\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#complete-template-generation-workflow","title":"Complete Template Generation Workflow","text":"<pre><code>from rdetoolkit.models.invoice import (\n    TemplateConfig, FixedHeaders, GeneralTermRegistry, SpecificTermRegistry,\n    GeneralAttributeConfig, SpecificAttributeConfig\n)\nfrom pathlib import Path\nimport polars as pl\n\n# Setup configuration\nconfig = TemplateConfig(\n    schema_path=\"schemas/invoice_schema.json\",\n    general_term_path=\"terms/general_terms.csv\",\n    specific_term_path=\"terms/specific_terms.csv\",\n    inputfile_mode=\"folder\"\n)\n\n# Initialize registries\ngeneral_registry = GeneralTermRegistry(str(config.general_term_path))\nspecific_registry = SpecificTermRegistry(str(config.specific_term_path))\n\n# Create attribute configurations\ngeneral_config = GeneralAttributeConfig(\n    type=\"general\",\n    registry=general_registry,\n    prefix=\"gen_\",\n    attributes=None,\n    requires_class_id=False\n)\n\nspecific_config = SpecificAttributeConfig(\n    type=\"specific\",\n    registry=specific_registry,\n    prefix=\"spec_\",\n    attributes=None,\n    requires_class_id=True\n)\n\n# Generate template\nheaders = FixedHeaders()\ntemplate_df = headers.to_template_dataframe()\n\n# Add sample data rows\nsample_data = [\n    [\"sample1.txt\", \"Dataset 1\", \"user123\", \"uuid-123\", \"Data 1\", \"exp1\", \"Description\",\n     \"Sample A\", \"sample-uuid-1\", \"owner-uuid-1\", \"H2O\", \"http://ref.url\", \"Water sample\"],\n    [\"sample2.txt\", \"Dataset 2\", \"user456\", \"uuid-456\", \"Data 2\", \"exp2\", \"Description\",\n     \"Sample B\", \"sample-uuid-2\", \"owner-uuid-2\", \"NaCl\", \"http://ref2.url\", \"Salt sample\"]\n]\n\n# Convert to DataFrame and append\nsample_df = pl.DataFrame(sample_data, schema=template_df.columns)\nfinal_df = pl.concat([template_df, sample_df])\n\n# Save complete template\nfinal_df.write_excel(\"complete_invoice_template.xlsx\", position=\"A1\")\n\nprint(f\"Complete template saved with {len(final_df)} rows\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#advanced-term-searching","title":"Advanced Term Searching","text":"<pre><code>from rdetoolkit.models.invoice import GeneralTermRegistry, SpecificTermRegistry\nfrom rdetoolkit.exceptions import DataRetrievalError, InvalidSearchParametersError\n\ndef search_terms_safely(registry, search_func, *args):\n    \"\"\"Safely search terms with error handling.\"\"\"\n    try:\n        results = search_func(*args)\n        return results\n    except (DataRetrievalError, InvalidSearchParametersError) as e:\n        print(f\"Search error: {e}\")\n        return []\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return []\n\n# Initialize registries\ngeneral_registry = GeneralTermRegistry(\"general_terms.csv\")\nspecific_registry = SpecificTermRegistry(\"specific_terms.csv\")\n\n# Safe searching\ngeneral_results = search_terms_safely(\n    general_registry,\n    general_registry.by_en,\n    \"Temperature\"\n)\n\nspecific_results = search_terms_safely(\n    specific_registry,\n    specific_registry.by_term_and_class_id,\n    \"S001\", \"C001\"\n)\n\nprint(\"General results:\", general_results)\nprint(\"Specific results:\", specific_results)\n\n# Batch term lookup\nterm_ids = [\"T001\", \"T002\", \"T003\"]\nall_general_terms = []\n\nfor term_id in term_ids:\n    terms = search_terms_safely(\n        general_registry,\n        general_registry.by_term_id,\n        term_id\n    )\n    all_general_terms.extend(terms)\n\nprint(f\"Found {len(all_general_terms)} general terms\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#custom-header-configuration","title":"Custom Header Configuration","text":"<pre><code>from rdetoolkit.models.invoice import (\n    HeaderRow1, HeaderRow2, HeaderRow3, HeaderRow4, FixedHeaders\n)\n\n# Create custom headers for a specific use case\ncustom_row2 = HeaderRow2(\n    A2=\"file_list\",\n    D2_G2=[\"metadata\"] * 4,\n    H2_M2=[\"specimen\"] * 6\n)\n\ncustom_row3 = HeaderRow3(\n    A3=\"filename\",\n    B3=\"title\",\n    # ... other customizations\n)\n\ncustom_row4 = HeaderRow4(\n    A4=\"\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5165\u529b\",\n    B4=\"\u30bf\u30a4\u30c8\u30eb\u3092\u5165\u529b\",\n    # ... other Japanese descriptions\n)\n\n# Combine into custom headers\ncustom_headers = FixedHeaders(\n    row1=HeaderRow1(),  # Use default\n    row2=custom_row2,\n    row3=custom_row3,\n    row4=custom_row4\n)\n\n# Generate custom template\ncustom_template = custom_headers.to_template_dataframe()\nprint(\"Custom template shape:\", custom_template.shape)\nprint(\"First row data:\", custom_template.row(0))\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/models/invoice/#common-exceptions","title":"Common Exceptions","text":"<p>The invoice module may raise several types of exceptions during operation:</p>"},{"location":"rdetoolkit/models/invoice/#valueerror","title":"ValueError","text":"<p>Raised when search parameters are invalid:</p> <pre><code>try:\n    registry = SpecificTermRegistry(\"terms.csv\")\n    # This will raise ValueError if columns and values have different lengths\n    results = registry.search([\"col1\", \"col2\"], [\"val1\"], [\"output\"])\nexcept ValueError as e:\n    print(f\"Parameter error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#dataretrievalerror","title":"DataRetrievalError","text":"<p>Raised when data retrieval operations fail:</p> <pre><code>from rdetoolkit.exceptions import DataRetrievalError\n\ntry:\n    registry = SpecificTermRegistry(\"terms.csv\")\n    results = registry.by_term_and_class_id(\"invalid_term\", \"invalid_class\")\nexcept DataRetrievalError as e:\n    print(f\"Data retrieval failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#invalidsearchparameterserror","title":"InvalidSearchParametersError","text":"<p>Raised when search parameters are structurally invalid:</p> <pre><code>from rdetoolkit.exceptions import InvalidSearchParametersError\n\ntry:\n    registry = SpecificTermRegistry(\"terms.csv\")\n    # Some operation that causes invalid parameters\n    results = registry.search([], [], [\"output\"])\nexcept InvalidSearchParametersError as e:\n    print(f\"Invalid search parameters: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always Validate File Paths: Ensure CSV files exist before creating registries:    <pre><code>from pathlib import Path\n\ncsv_path = Path(\"terms.csv\")\nif csv_path.exists():\n    registry = GeneralTermRegistry(str(csv_path))\nelse:\n    print(f\"File not found: {csv_path}\")\n</code></pre></p> </li> <li> <p>Handle Empty Results Gracefully: Check for empty results before processing:    <pre><code>results = registry.by_en(\"NonexistentTerm\")\nif results:\n    # Process results\n    for result in results:\n        print(result)\nelse:\n    print(\"No results found\")\n</code></pre></p> </li> <li> <p>Use Type Hints: Leverage type hints for better code clarity:    <pre><code>from typing import List, Dict, Any\n\ndef process_terms(registry: GeneralTermRegistry) -&gt; List[Dict[str, Any]]:\n    return registry.by_en(\"Temperature\")\n</code></pre></p> </li> <li> <p>Validate DataFrame Structure: Ensure DataFrames have expected columns:    <pre><code>headers = FixedHeaders()\ndf = headers.to_template_dataframe()\n\nexpected_cols = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\"]\nassert df.columns == expected_cols, \"Template structure mismatch\"\n</code></pre></p> </li> <li> <p>Use Configuration Objects: Centralize configuration for better maintainability:    <pre><code>config = TemplateConfig(\n    schema_path=\"config/schema.json\",\n    general_term_path=\"data/general.csv\",\n    specific_term_path=\"data/specific.csv\",\n    inputfile_mode=\"folder\"\n)\n\n# Pass config to other functions\ndef setup_registries(config: TemplateConfig):\n    general_reg = GeneralTermRegistry(str(config.general_term_path))\n    specific_reg = SpecificTermRegistry(str(config.specific_term_path))\n    return general_reg, specific_reg\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/models/invoice/#performance-notes","title":"Performance Notes","text":"<ul> <li>Term registries use Polars DataFrames for efficient data operations</li> <li>Search operations are optimized for large datasets with proper indexing</li> <li>DataFrame operations leverage Polars' lazy evaluation when possible</li> <li>Memory usage is optimized through schema validation and type specification</li> <li>Batch operations are recommended for processing multiple terms</li> </ul>"},{"location":"rdetoolkit/models/invoice/#integration-with-schema-models","title":"Integration with Schema Models","text":"<p>The invoice module integrates seamlessly with the <code>invoice_schema</code> module:</p> <pre><code>from rdetoolkit.models.invoice import SpecificAttributeConfig\nfrom rdetoolkit.models.invoice_schema import SpecificAttribute, SampleSpecificItems\n\n# Create schema-defined attributes\nspecific_attr = SpecificAttribute(\n    obj_type=\"array\",\n    items=SampleSpecificItems(root=[...])\n)\n\n# Use in configuration\nconfig = SpecificAttributeConfig(\n    type=\"specific\",\n    registry=specific_registry,\n    prefix=\"spec_\",\n    attributes=specific_attr,  # Schema-validated attributes\n    requires_class_id=True\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice/#see-also","title":"See Also","text":"<ul> <li>Invoice Schema Module - For schema validation and structure definitions</li> <li>Core Module - For file operations and directory management</li> <li>Polars Documentation - For DataFrame operations and optimization</li> <li>Pydantic Documentation - For data validation patterns</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/","title":"Invoice Schema Module","text":"<p>The <code>rdetoolkit.models.invoice_schema</code> module provides Pydantic models for validating and managing RDE invoice schema JSON structures. This module defines a comprehensive set of models that correspond to the JSON schema format used in RDE (Research Data Exchange) systems.</p>"},{"location":"rdetoolkit/models/invoice_schema/#overview","title":"Overview","text":"<p>The invoice schema module implements a hierarchical validation system for dataset templates using Pydantic models. The module supports:</p> <ul> <li>Multi-language Labels: Support for Japanese and English labels</li> <li>Custom Field Validation: Flexible custom property definitions with type validation</li> <li>Sample Data Management: Structured handling of general and specific sample attributes</li> <li>Schema Validation: Comprehensive validation rules for JSON schema compliance</li> <li>Type Safety: Strong typing with runtime validation</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#core-classes","title":"Core Classes","text":""},{"location":"rdetoolkit/models/invoice_schema/#invoiceschemajson","title":"InvoiceSchemaJson","text":"<p>The main class representing the complete invoice schema structure.</p>"},{"location":"rdetoolkit/models/invoice_schema/#invoiceschemajson-constructor","title":"InvoiceSchemaJson Constructor","text":"<pre><code>InvoiceSchemaJson(\n    version: str = \"https://json-schema.org/draft/2020-12/schema\",\n    schema_id: str = \"https://rde.nims.go.jp/rde/dataset-templates/\",\n    description: Optional[str] = None,\n    value_type: Literal[\"object\"] = \"object\",\n    required: Optional[list[Literal[\"custom\", \"sample\"]]] = None,\n    properties: Properties\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>version</code> (str): JSON schema version (default: \"https://json-schema.org/draft/2020-12/schema\")</li> <li><code>schema_id</code> (str): Schema identifier URL (default: \"https://rde.nims.go.jp/rde/dataset-templates/\")</li> <li><code>description</code> (Optional[str]): Description of the schema</li> <li><code>value_type</code> (Literal[\"object\"]): Schema type, must be \"object\"</li> <li><code>required</code> (Optional[list]): List of required top-level properties</li> <li><code>properties</code> (Properties): Schema properties definition</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#validation-rules-metaproperty","title":"Validation Rules (MetaProperty)","text":"<ul> <li>If <code>\"custom\"</code> is in <code>required</code>, <code>properties.custom</code> must not be None</li> <li>If <code>\"sample\"</code> is in <code>required</code>, <code>properties.sample</code> must not be None</li> <li>If <code>properties.custom</code> exists, <code>\"custom\"</code> must be in <code>required</code></li> <li>If <code>properties.sample</code> exists, <code>\"sample\"</code> must be in <code>required</code></li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#invoiceschemajson-usage-example","title":"InvoiceSchemaJson Usage Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import InvoiceSchemaJson, Properties\n\n# Create a basic invoice schema\nschema = InvoiceSchemaJson(\n    description=\"RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30c6\u30b9\u30c8\u7528\u30d5\u30a1\u30a4\u30eb\",\n    properties=Properties()\n)\n\n# Generate JSON output\njson_output = schema.model_dump_json()\nprint(json_output)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#properties","title":"Properties","text":"<p>Container for top-level schema properties.</p>"},{"location":"rdetoolkit/models/invoice_schema/#properties-constructor","title":"Properties Constructor","text":"<pre><code>Properties(\n    custom: Optional[CustomField] = None,\n    sample: Optional[SampleField] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>custom</code> (Optional[CustomField]): Custom field definitions</li> <li><code>sample</code> (Optional[SampleField]): Sample field definitions</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#properties-usage-example","title":"Properties Usage Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import Properties, CustomField, SampleField\n\nproperties = Properties(\n    custom=CustomField(...),\n    sample=SampleField(...)\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#language-support","title":"Language Support","text":""},{"location":"rdetoolkit/models/invoice_schema/#langlabels","title":"LangLabels","text":"<p>Represents labels in multiple languages (Japanese and English).</p>"},{"location":"rdetoolkit/models/invoice_schema/#langlabels-constructor","title":"LangLabels Constructor","text":"<pre><code>LangLabels(ja: str, en: str)\n</code></pre> <p>Parameters:</p> <ul> <li><code>ja</code> (str): Japanese label</li> <li><code>en</code> (str): English label</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#placeholder-example","title":"Placeholder Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import LangLabels\n\nlabel = LangLabels(\n    ja=\"\u30b5\u30f3\u30d7\u30eb\u540d\",\n    en=\"Sample Name\"\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#placeholder","title":"Placeholder","text":"<p>Represents placeholder text in multiple languages.</p>"},{"location":"rdetoolkit/models/invoice_schema/#placeholder-constructor","title":"Placeholder Constructor","text":"<pre><code>Placeholder(ja: str, en: str)\n</code></pre> <p>Parameters:</p> <ul> <li><code>ja</code> (str): Japanese placeholder text</li> <li><code>en</code> (str): English placeholder text</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#langlabels-example","title":"LangLabels Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import Placeholder\n\nplaceholder = Placeholder(\n    ja=\"\u3053\u3053\u306b\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\",\n    en=\"Please enter here\"\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#custom-fields","title":"Custom Fields","text":""},{"location":"rdetoolkit/models/invoice_schema/#customfield","title":"CustomField","text":"<p>Represents a custom field definition in the invoice schema.</p>"},{"location":"rdetoolkit/models/invoice_schema/#customfield-constructor","title":"CustomField Constructor","text":"<pre><code>CustomField(\n    obj_type: Literal[\"object\"],\n    label: LangLabels,\n    required: list[str],\n    properties: CustomItems\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>obj_type</code> (Literal[\"object\"]): Field type, must be \"object\"</li> <li><code>label</code> (LangLabels): Multi-language label</li> <li><code>required</code> (list[str]): List of required property names</li> <li><code>properties</code> (CustomItems): Custom property definitions</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#customitems","title":"CustomItems","text":"<p>A dictionary-like container for custom property definitions.</p>"},{"location":"rdetoolkit/models/invoice_schema/#customitems-constructor","title":"CustomItems Constructor","text":"<pre><code>CustomItems(root: dict[str, MetaProperty])\n</code></pre> <p>Parameters:</p> <ul> <li><code>root</code> (dict[str, MetaProperty]): Dictionary mapping property names to MetaProperty instances</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#methods","title":"Methods","text":""},{"location":"rdetoolkit/models/invoice_schema/#__iter__","title":"__iter__()","text":"<pre><code>def __iter__()\n</code></pre> <p>Returns an iterator over the custom items.</p>"},{"location":"rdetoolkit/models/invoice_schema/#__getitem__item","title":"__getitem__(item)","text":"<pre><code>def __getitem__(item: str) -&gt; MetaProperty\n</code></pre> <p>Access a custom property by name.</p>"},{"location":"rdetoolkit/models/invoice_schema/#customitems-example","title":"CustomItems Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import CustomItems, MetaProperty, LangLabels\n\n# Create custom properties\ncustom_items = CustomItems(root={\n    \"temperature\": MetaProperty(\n        label=LangLabels(ja=\"\u6e29\u5ea6\", en=\"Temperature\"),\n        value_type=\"number\",\n        description=\"Sample temperature\"\n    )\n})\n\n# Access properties\ntemp_prop = custom_items[\"temperature\"]\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#metaproperty","title":"MetaProperty","text":"<p>Defines a custom property with validation rules.</p>"},{"location":"rdetoolkit/models/invoice_schema/#metaproperty-constructor","title":"MetaProperty Constructor","text":"<pre><code>MetaProperty(\n    label: LangLabels,\n    value_type: Literal[\"boolean\", \"integer\", \"number\", \"string\"],\n    description: Optional[str] = None,\n    examples: Optional[list[Union[bool, int, float, str]]] = None,\n    default: Optional[Union[bool, int, float, str]] = None,\n    const: Optional[Union[bool, int, float, str]] = None,\n    enum: Optional[list[Union[bool, int, float, str]]] = None,\n    maximum: Optional[int] = None,\n    exclusiveMaximum: Optional[int] = None,\n    minimum: Optional[int] = None,\n    exclusiveMinimum: Optional[int] = None,\n    maxLength: Optional[int] = None,\n    minLength: Optional[int] = None,\n    pattern: Optional[str] = None,\n    format: Optional[Literal[\"date\", \"time\", \"uri\", \"uuid\", \"markdown\"]] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>label</code> (LangLabels): Multi-language property label</li> <li><code>value_type</code> (Literal): Data type of the property value</li> <li><code>description</code> (Optional[str]): Property description</li> <li><code>examples</code> (Optional[list]): Example values</li> <li><code>default</code> (Optional[Union]): Default value</li> <li><code>const</code> (Optional[Union]): Constant value constraint</li> <li><code>enum</code> (Optional[list]): Enumerated allowed values</li> <li><code>maximum</code> (Optional[int]): Maximum value (for numeric types)</li> <li><code>exclusiveMaximum</code> (Optional[int]): Exclusive maximum value</li> <li><code>minimum</code> (Optional[int]): Minimum value (for numeric types)</li> <li><code>exclusiveMinimum</code> (Optional[int]): Exclusive minimum value</li> <li><code>maxLength</code> (Optional[int]): Maximum string length</li> <li><code>minLength</code> (Optional[int]): Minimum string length</li> <li><code>pattern</code> (Optional[str]): Regular expression pattern</li> <li><code>format</code> (Optional[Literal]): String format constraint</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#customfield-validation-rules","title":"CustomField Validation Rules","text":"<ul> <li>Numeric constraints (<code>maximum</code>, <code>minimum</code>, etc.) only apply to <code>\"integer\"</code> or <code>\"number\"</code> types</li> <li>String length constraints (<code>maxLength</code>, <code>minLength</code>) only apply to <code>\"string\"</code> type</li> <li>If <code>const</code> is specified, its type must match <code>value_type</code></li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#metaproperty-example","title":"MetaProperty Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import MetaProperty, LangLabels\n\n# Numeric property with constraints\ntemperature = MetaProperty(\n    label=LangLabels(ja=\"\u6e29\u5ea6\", en=\"Temperature\"),\n    value_type=\"number\",\n    description=\"Sample temperature in Celsius\",\n    minimum=-273,\n    maximum=1000,\n    examples=[20.5, 25.0, 30.2]\n)\n\n# String property with pattern\nsample_id = MetaProperty(\n    label=LangLabels(ja=\"\u30b5\u30f3\u30d7\u30ebID\", en=\"Sample ID\"),\n    value_type=\"string\",\n    pattern=\"^[A-Z]{2}[0-9]{4}$\",\n    examples=[\"AB1234\", \"CD5678\"]\n)\n\n# Enumerated property\nstatus = MetaProperty(\n    label=LangLabels(ja=\"\u30b9\u30c6\u30fc\u30bf\u30b9\", en=\"Status\"),\n    value_type=\"string\",\n    enum=[\"active\", \"inactive\", \"pending\"]\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#options","title":"Options","text":"<p>Represents widget options for custom properties.</p>"},{"location":"rdetoolkit/models/invoice_schema/#options-constructor","title":"Options Constructor","text":"<pre><code>Options(\n    widget: Optional[Literal[\"textarea\"]] = None,\n    rows: Optional[int] = None,\n    unit: Optional[str] = None,\n    placeholder: Optional[Placeholder] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>widget</code> (Optional[Literal[\"textarea\"]]): Widget type</li> <li><code>rows</code> (Optional[int]): Number of rows for textarea widget</li> <li><code>unit</code> (Optional[str]): Unit of measurement</li> <li><code>placeholder</code> (Optional[Placeholder]): Placeholder text</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#validation-rules","title":"Validation Rules","text":"<ul> <li>If <code>widget</code> is set to <code>\"textarea\"</code>, <code>rows</code> must be specified</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#example","title":"Example","text":"<pre><code>from rdetoolkit.models.invoice_schema import Options, Placeholder\n\n# Textarea widget options\ntextarea_options = Options(\n    widget=\"textarea\",\n    rows=5,\n    placeholder=Placeholder(\n        ja=\"\u8a73\u7d30\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\",\n        en=\"Please enter details\"\n    )\n)\n\n# Numeric input with unit\nnumeric_options = Options(\n    unit=\"\u00b0C\",\n    placeholder=Placeholder(\n        ja=\"\u6e29\u5ea6\u3092\u5165\u529b\",\n        en=\"Enter temperature\"\n    )\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#sample-fields","title":"Sample Fields","text":""},{"location":"rdetoolkit/models/invoice_schema/#samplefield","title":"SampleField","text":"<p>Represents the sample field definition in the invoice schema.</p>"},{"location":"rdetoolkit/models/invoice_schema/#samplefield-constructor","title":"SampleField Constructor","text":"<pre><code>SampleField(\n    obj_type: Literal[\"object\"],\n    label: LangLabels,\n    required: list[Literal[\"names\", \"sampleId\"]] = [\"names\", \"sampleId\"],\n    properties: SampleProperties\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>obj_type</code> (Literal[\"object\"]): Field type, must be \"object\"</li> <li><code>label</code> (LangLabels): Multi-language label</li> <li><code>required</code> (list): Required property names (default: [\"names\", \"sampleId\"])</li> <li><code>properties</code> (SampleProperties): Sample property definitions</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#sampleproperties","title":"SampleProperties","text":"<p>Contains the properties for sample data.</p>"},{"location":"rdetoolkit/models/invoice_schema/#sampleproperties-constructor","title":"SampleProperties Constructor","text":"<pre><code>SampleProperties(\n    generalAttributes: Optional[GeneralAttribute] = None,\n    specificAttributes: Optional[SpecificAttribute] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>generalAttributes</code> (Optional[GeneralAttribute]): General sample attributes</li> <li><code>specificAttributes</code> (Optional[SpecificAttribute]): Specific sample attributes</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#samplepropertieswhenadding","title":"SamplePropertiesWhenAdding","text":"<p>Extended sample properties used when adding new samples.</p>"},{"location":"rdetoolkit/models/invoice_schema/#samplepropertieswhenadding-constructor","title":"SamplePropertiesWhenAdding Constructor","text":"<pre><code>SamplePropertiesWhenAdding(\n    sample_id: Optional[str] = None,\n    ownerId: str,\n    composition: Optional[str] = None,\n    referenceUrl: Optional[str] = None,\n    description: Optional[str] = None,\n    generalAttributes: Optional[GeneralAttribute] = None,\n    specificAttributes: Optional[SpecificAttribute] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>sample_id</code> (Optional[str]): Sample identifier</li> <li><code>ownerId</code> (str): Owner ID (must match pattern <code>^([0-9a-zA-Z]{56})$</code>)</li> <li><code>composition</code> (Optional[str]): Sample composition</li> <li><code>referenceUrl</code> (Optional[str]): Reference URL</li> <li><code>description</code> (Optional[str]): Sample description</li> <li><code>generalAttributes</code> (Optional[GeneralAttribute]): General attributes</li> <li><code>specificAttributes</code> (Optional[SpecificAttribute]): Specific attributes</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#sample-attributes","title":"Sample Attributes","text":""},{"location":"rdetoolkit/models/invoice_schema/#generalattribute","title":"GeneralAttribute","text":"<p>Represents general attributes for samples.</p>"},{"location":"rdetoolkit/models/invoice_schema/#generalattribute-constructor","title":"GeneralAttribute Constructor","text":"<pre><code>GeneralAttribute(\n    obj_type: Literal[\"array\"],\n    items: SampleGeneralItems\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>obj_type</code> (Literal[\"array\"]): Attribute type, must be \"array\"</li> <li><code>items</code> (SampleGeneralItems): General attribute items</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#samplegeneralitems","title":"SampleGeneralItems","text":"<p>Container for general attribute items.</p>"},{"location":"rdetoolkit/models/invoice_schema/#samplegeneralitems-constructor","title":"SampleGeneralItems Constructor","text":"<pre><code>SampleGeneralItems(root: Optional[list[GeneralProperty]] = None)\n</code></pre> <p>Parameters:</p> <ul> <li><code>root</code> (Optional[list[GeneralProperty]]): List of general properties</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#generalproperty","title":"GeneralProperty","text":"<p>Defines a general property structure.</p>"},{"location":"rdetoolkit/models/invoice_schema/#generalproperty-constructor","title":"GeneralProperty Constructor","text":"<pre><code>GeneralProperty(\n    object_type: Literal[\"object\"],\n    required: list[Literal[\"termId\", \"value\"]],\n    properties: GeneralChildProperty\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>object_type</code> (Literal[\"object\"]): Property type, must be \"object\"</li> <li><code>required</code> (list): Required fields (must include \"termId\" and \"value\")</li> <li><code>properties</code> (GeneralChildProperty): Child properties</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#generalchildproperty","title":"GeneralChildProperty","text":"<p>Contains child properties for general attributes.</p>"},{"location":"rdetoolkit/models/invoice_schema/#generalchildproperty-constructor","title":"GeneralChildProperty Constructor","text":"<pre><code>GeneralChildProperty(term_id: TermId)\n</code></pre> <p>Parameters:</p> <ul> <li><code>term_id</code> (TermId): Term identifier</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#specificattribute","title":"SpecificAttribute","text":"<p>Represents specific attributes for samples.</p>"},{"location":"rdetoolkit/models/invoice_schema/#specificattribute-constructor","title":"SpecificAttribute Constructor","text":"<pre><code>SpecificAttribute(\n    obj_type: Literal[\"array\"],\n    items: SampleSpecificItems\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>obj_type</code> (Literal[\"array\"]): Attribute type, must be \"array\"</li> <li><code>items</code> (SampleSpecificItems): Specific attribute items</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#samplespecificitems","title":"SampleSpecificItems","text":"<p>Container for specific attribute items.</p>"},{"location":"rdetoolkit/models/invoice_schema/#samplespecificitems-constructor","title":"SampleSpecificItems Constructor","text":"<pre><code>SampleSpecificItems(root: list[SpecificProperty])\n</code></pre> <p>Parameters:</p> <ul> <li><code>root</code> (list[SpecificProperty]): List of specific properties</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#specificproperty","title":"SpecificProperty","text":"<p>Defines a specific property structure.</p>"},{"location":"rdetoolkit/models/invoice_schema/#specificproperty-constructor","title":"SpecificProperty Constructor","text":"<pre><code>SpecificProperty(\n    object_type: Literal[\"object\"],\n    required: list[Literal[\"classId\", \"termId\", \"value\"]],\n    properties: SpecificChildProperty\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>object_type</code> (Literal[\"object\"]): Property type, must be \"object\"</li> <li><code>required</code> (list): Required fields (must include \"classId\", \"termId\", and \"value\")</li> <li><code>properties</code> (SpecificChildProperty): Child properties</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#specificchildproperty","title":"SpecificChildProperty","text":"<p>Contains child properties for specific attributes.</p>"},{"location":"rdetoolkit/models/invoice_schema/#specificchildproperty-constructor","title":"SpecificChildProperty Constructor","text":"<pre><code>SpecificChildProperty(\n    term_id: TermId,\n    class_id: ClassId\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>term_id</code> (TermId): Term identifier</li> <li><code>class_id</code> (ClassId): Class identifier</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#identifier-classes","title":"Identifier Classes","text":""},{"location":"rdetoolkit/models/invoice_schema/#termid","title":"TermId","text":"<p>Represents a term identifier with a constant value.</p>"},{"location":"rdetoolkit/models/invoice_schema/#termid-constructor","title":"TermId Constructor","text":"<pre><code>TermId(const: str)\n</code></pre> <p>Parameters:</p> <ul> <li><code>const</code> (str): Constant term identifier value</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#classid","title":"ClassId","text":"<p>Represents a class identifier with a constant value.</p>"},{"location":"rdetoolkit/models/invoice_schema/#classid-constructor","title":"ClassId Constructor","text":"<pre><code>ClassId(const: str)\n</code></pre> <p>Parameters:</p> <ul> <li><code>const</code> (str): Constant class identifier value</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#basic-data-types","title":"Basic Data Types","text":""},{"location":"rdetoolkit/models/invoice_schema/#datasetid","title":"DatasetId","text":"<p>Represents a dataset identifier.</p>"},{"location":"rdetoolkit/models/invoice_schema/#datasetid-constructor","title":"DatasetId Constructor","text":"<pre><code>DatasetId(value_type: str = \"string\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>value_type</code> (str): Data type, defaults to \"string\"</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#basicitems","title":"BasicItems","text":"<p>Contains basic invoice items with predefined structures.</p>"},{"location":"rdetoolkit/models/invoice_schema/#basicitems-constructor","title":"BasicItems Constructor","text":"<pre><code>BasicItems(\n    dateSubmitted: BasicItemsValue = BasicItemsValue(type=\"string\", format=\"date\"),\n    dataOwnerId: BasicItemsValue = BasicItemsValue(type=\"string\", pattern=\"^([0-9a-zA-Z]{56})$\"),\n    dateName: BasicItemsValue = BasicItemsValue(type=\"string\", pattern=\"^([0-9a-zA-Z]{56})$\"),\n    instrumentId: Optional[BasicItemsValue] = BasicItemsValue(type=\"string\", pattern=\"^$|^([0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12})$\"),\n    experimentId: Optional[BasicItemsValue] = None,\n    description: Optional[BasicItemsValue] = None\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#basicitemsvalue","title":"BasicItemsValue","text":"<p>Represents a basic value type with validation.</p>"},{"location":"rdetoolkit/models/invoice_schema/#basicitemsvalue-constructor","title":"BasicItemsValue Constructor","text":"<pre><code>BasicItemsValue(\n    value_type: Union[str, list, None] = None,\n    format: Optional[Literal[\"date\"]] = None,\n    pattern: Optional[str] = None,\n    description: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>value_type</code> (Union[str, list, None]): Value type specification</li> <li><code>format</code> (Optional[Literal[\"date\"]]): Date format constraint</li> <li><code>pattern</code> (Optional[str]): Regular expression pattern</li> <li><code>description</code> (Optional[str]): Value description</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/models/invoice_schema/#creating-a-custom-field-schema","title":"Creating a Custom Field Schema","text":"<pre><code>from rdetoolkit.models.invoice_schema import (\n    InvoiceSchemaJson, Properties, CustomField, CustomItems,\n    MetaProperty, LangLabels, Options, Placeholder\n)\n\n# Define custom properties\ntemperature_prop = MetaProperty(\n    label=LangLabels(ja=\"\u6e29\u5ea6\", en=\"Temperature\"),\n    value_type=\"number\",\n    description=\"Sample temperature in Celsius\",\n    minimum=-273,\n    maximum=1000,\n    unit=\"\u00b0C\"\n)\n\ndescription_prop = MetaProperty(\n    label=LangLabels(ja=\"\u8aac\u660e\", en=\"Description\"),\n    value_type=\"string\",\n    description=\"Detailed sample description\",\n    maxLength=1000,\n    options=Options(\n        widget=\"textarea\",\n        rows=5,\n        placeholder=Placeholder(\n            ja=\"\u30b5\u30f3\u30d7\u30eb\u306e\u8a73\u7d30\u8aac\u660e\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\",\n            en=\"Please enter detailed sample description\"\n        )\n    )\n)\n\n# Create custom field\ncustom_field = CustomField(\n    obj_type=\"object\",\n    label=LangLabels(ja=\"\u30ab\u30b9\u30bf\u30e0\u30d5\u30a3\u30fc\u30eb\u30c9\", en=\"Custom Fields\"),\n    required=[\"temperature\"],\n    properties=CustomItems(root={\n        \"temperature\": temperature_prop,\n        \"description\": description_prop\n    })\n)\n\n# Create complete schema\nschema = InvoiceSchemaJson(\n    description=\"Custom sample analysis schema\",\n    required=[\"custom\"],\n    properties=Properties(custom=custom_field)\n)\n\n# Generate JSON\njson_output = schema.model_dump_json(indent=2)\nprint(json_output)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#creating-a-sample-field-schema","title":"Creating a Sample Field Schema","text":"<pre><code>from rdetoolkit.models.invoice_schema import (\n    SampleField, SampleProperties, GeneralAttribute, SpecificAttribute,\n    SampleGeneralItems, SampleSpecificItems, GeneralProperty, SpecificProperty,\n    GeneralChildProperty, SpecificChildProperty, TermId, ClassId, LangLabels\n)\n\n# Define general attribute\ngeneral_prop = GeneralProperty(\n    object_type=\"object\",\n    required=[\"termId\", \"value\"],\n    properties=GeneralChildProperty(\n        term_id=TermId(const=\"general_term_001\")\n    )\n)\n\ngeneral_attr = GeneralAttribute(\n    obj_type=\"array\",\n    items=SampleGeneralItems(root=[general_prop])\n)\n\n# Define specific attribute\nspecific_prop = SpecificProperty(\n    object_type=\"object\",\n    required=[\"classId\", \"termId\", \"value\"],\n    properties=SpecificChildProperty(\n        term_id=TermId(const=\"specific_term_001\"),\n        class_id=ClassId(const=\"class_001\")\n    )\n)\n\nspecific_attr = SpecificAttribute(\n    obj_type=\"array\",\n    items=SampleSpecificItems(root=[specific_prop])\n)\n\n# Create sample field\nsample_field = SampleField(\n    obj_type=\"object\",\n    label=LangLabels(ja=\"\u30b5\u30f3\u30d7\u30eb\", en=\"Sample\"),\n    properties=SampleProperties(\n        generalAttributes=general_attr,\n        specificAttributes=specific_attr\n    )\n)\n\n# Create complete schema\nschema = InvoiceSchemaJson(\n    description=\"Sample data schema\",\n    required=[\"sample\"],\n    properties=Properties(sample=sample_field)\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#validation-and-error-handling","title":"Validation and Error Handling","text":"<pre><code>from rdetoolkit.models.invoice_schema import MetaProperty, LangLabels\nfrom pydantic import ValidationError\n\ntry:\n    # This will raise a validation error\n    invalid_prop = MetaProperty(\n        label=LangLabels(ja=\"\u6e29\u5ea6\", en=\"Temperature\"),\n        value_type=\"string\",  # String type\n        minimum=0,           # But using numeric constraint\n        maximum=100\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n\ntry:\n    # This will also raise a validation error\n    invalid_options = Options(\n        widget=\"textarea\",  # Textarea widget\n        # rows not specified - validation error\n    )\nexcept ValidationError as e:\n    print(f\"Options validation error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#working-with-enum-and-const-values","title":"Working with Enum and Const Values","text":"<pre><code>from rdetoolkit.models.invoice_schema import MetaProperty, LangLabels\n\n# Enumerated values\nstatus_prop = MetaProperty(\n    label=LangLabels(ja=\"\u30b9\u30c6\u30fc\u30bf\u30b9\", en=\"Status\"),\n    value_type=\"string\",\n    enum=[\"active\", \"inactive\", \"pending\", \"completed\"],\n    description=\"Current sample status\"\n)\n\n# Constant value\nversion_prop = MetaProperty(\n    label=LangLabels(ja=\"\u30d0\u30fc\u30b8\u30e7\u30f3\", en=\"Version\"),\n    value_type=\"string\",\n    const=\"1.0.0\",\n    description=\"Schema version\"\n)\n\n# Boolean with default\npublished_prop = MetaProperty(\n    label=LangLabels(ja=\"\u516c\u958b\u6e08\u307f\", en=\"Published\"),\n    value_type=\"boolean\",\n    default=False,\n    description=\"Whether the sample data is published\"\n)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/models/invoice_schema/#common-validation-errors","title":"Common Validation Errors","text":"<p>The invoice schema models may raise <code>ValidationError</code> exceptions in the following cases:</p>"},{"location":"rdetoolkit/models/invoice_schema/#type-mismatch-errors","title":"Type Mismatch Errors","text":"<pre><code>try:\n    MetaProperty(\n        label=LangLabels(ja=\"\u30c6\u30b9\u30c8\", en=\"Test\"),\n        value_type=\"string\",\n        minimum=0  # Invalid: minimum only applies to numeric types\n    )\nexcept ValidationError as e:\n    print(f\"Type mismatch: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#required-field-errors","title":"Required Field Errors","text":"<pre><code>try:\n    Options(\n        widget=\"textarea\"\n        # Missing required 'rows' field\n    )\nexcept ValidationError as e:\n    print(f\"Missing required field: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#pattern-validation-errors","title":"Pattern Validation Errors","text":"<pre><code>try:\n    SamplePropertiesWhenAdding(\n        ownerId=\"invalid_id\"  # Must match specific pattern\n    )\nexcept ValidationError as e:\n    print(f\"Pattern validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#best-practices","title":"Best Practices","text":"<ol> <li>Use Type Hints: Always use proper type hints for better IDE support and validation:</li> </ol> <pre><code>from typing import Optional\nfrom rdetoolkit.models.invoice_schema import MetaProperty, LangLabels\n\ndef create_property(name: str, prop_type: str) -&gt; MetaProperty:\n    return MetaProperty(\n        label=LangLabels(ja=f\"{name}_ja\", en=f\"{name}_en\"),\n        value_type=prop_type\n    )\n</code></pre> <ol> <li>Validate Early: Validate models as soon as they're created:</li> </ol> <pre><code>try:\n    prop = MetaProperty(...)\n    # Model is automatically validated on creation\nexcept ValidationError as e:\n    # Handle validation errors immediately\n    print(f\"Invalid property: {e}\")\n</code></pre> <ol> <li>Use Model Validation: Leverage Pydantic's validation features:</li> </ol> <pre><code># Use model validation to ensure consistency\nschema = InvoiceSchemaJson(\n    required=[\"custom\"],\n    properties=Properties(\n        custom=custom_field  # This will be validated\n    )\n)\n</code></pre> <ol> <li>Handle Serialization: Use proper serialization methods:</li> </ol> <pre><code># Generate JSON with proper formatting\njson_str = schema.model_dump_json(indent=2, exclude_none=True)\n\n# Parse from JSON\nschema_dict = schema.model_dump()\nreconstructed = InvoiceSchemaJson(**schema_dict)\n</code></pre>"},{"location":"rdetoolkit/models/invoice_schema/#performance-notes","title":"Performance Notes","text":"<ul> <li>All models use Pydantic v2 for optimal performance and validation</li> <li>Models support both dictionary and JSON serialization/deserialization</li> <li>Validation is performed at object creation time, not during access</li> <li>Use <code>model_dump()</code> for dictionary representation and <code>model_dump_json()</code> for JSON strings</li> <li>Large schemas with many custom properties are efficiently handled through lazy validation</li> </ul>"},{"location":"rdetoolkit/models/invoice_schema/#see-also","title":"See Also","text":"<ul> <li>Pydantic Documentation - For detailed information about Pydantic features</li> <li>JSON Schema Specification - For understanding JSON schema standards</li> <li>RDE Documentation - For RDE-specific schema requirements</li> </ul>"},{"location":"rdetoolkit/models/metadata/","title":"Metadata Module","text":"<p>The <code>rdetoolkit.models.metadata</code> module provides Pydantic models for handling metadata extracted during RDE data structuring processes. This module implements validation rules for metadata values and provides a structured approach to managing both constant and variable metadata attributes.</p>"},{"location":"rdetoolkit/models/metadata/#overview","title":"Overview","text":"<p>The metadata module implements a comprehensive validation system for metadata handling with the following capabilities:</p> <ul> <li>Size Validation: Automatic validation of metadata value sizes with configurable limits</li> <li>Type Safety: Strong typing with runtime validation using Pydantic models</li> <li>Structured Storage: Separation of constant and variable metadata types</li> <li>UTF-8 Encoding Support: Proper handling of multi-byte character encodings</li> <li>Flexible Value Types: Support for various data types while maintaining validation</li> </ul>"},{"location":"rdetoolkit/models/metadata/#constants","title":"Constants","text":""},{"location":"rdetoolkit/models/metadata/#max_value_size","title":"MAX_VALUE_SIZE","text":"<p>The maximum allowed size for metadata values in bytes.</p> <pre><code>MAX_VALUE_SIZE: Final[int] = 1024\n</code></pre> <p>This constant defines the size limit for individual metadata values. Values exceeding this limit will raise a validation error during model creation.</p>"},{"location":"rdetoolkit/models/metadata/#core-classes","title":"Core Classes","text":""},{"location":"rdetoolkit/models/metadata/#variable","title":"Variable","text":"<p>Model for handling variable-type metadata attributes.</p>"},{"location":"rdetoolkit/models/metadata/#constructor-for-variable","title":"Constructor for <code>Variable</code>","text":"<pre><code>Variable(variable: dict[str, Any])\n</code></pre> <p>Parameters:</p> <ul> <li><code>variable</code> (dict[str, Any]): Dictionary containing variable metadata with string keys and any value types</li> </ul>"},{"location":"rdetoolkit/models/metadata/#validation-rules-for-variable","title":"Validation Rules for <code>Variable</code>","text":"<ul> <li>All string values in the dictionary must not exceed <code>MAX_VALUE_SIZE</code> bytes when UTF-8 encoded</li> <li>Non-string values are not subject to size validation</li> <li>Empty dictionaries are allowed</li> </ul>"},{"location":"rdetoolkit/models/metadata/#example-variable","title":"Example: Variable","text":"<pre><code>from rdetoolkit.models.metadata import Variable\n\n# Valid variable metadata\nvalid_variable = Variable(variable={\n    \"temperature\": \"25.5\",\n    \"pressure\": \"1013.25\",\n    \"humidity\": \"60%\",\n    \"operator\": \"John Doe\"\n})\n\n# Invalid - value too large (will raise ValueError)\ntry:\n    large_value = \"x\" * 2000  # 2000 bytes\n    invalid_variable = Variable(variable={\"description\": large_value})\nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#metavalue","title":"MetaValue","text":"<p>Model for handling individual metadata values with optional units.</p>"},{"location":"rdetoolkit/models/metadata/#constructor-for-metavalue","title":"Constructor for <code>MetaValue</code>","text":"<pre><code>MetaValue(value: Any, unit: str | None = None)\n</code></pre> <p>Parameters:</p> <ul> <li><code>value</code> (Any): The metadata value (can be any type)</li> <li><code>unit</code> (str | None): Optional unit description (default: None)</li> </ul>"},{"location":"rdetoolkit/models/metadata/#validation-rules-for-metavalue","title":"Validation Rules for <code>MetaValue</code>","text":"<ul> <li>If <code>value</code> is a string, it must not exceed <code>MAX_VALUE_SIZE</code> bytes when UTF-8 encoded</li> <li>Non-string values are not subject to size validation</li> <li>The <code>unit</code> field is optional and can be None</li> </ul>"},{"location":"rdetoolkit/models/metadata/#example-metavalue","title":"Example: MetaValue","text":"<pre><code>from rdetoolkit.models.metadata import MetaValue\n\n# Numeric value with unit\ntemperature = MetaValue(value=25.5, unit=\"\u00b0C\")\n\n# String value without unit\nsample_id = MetaValue(value=\"SAMPLE_001\")\n\n# Complex value (not size-validated)\ndata_array = MetaValue(value=[1, 2, 3, 4, 5], unit=\"counts\")\n\n# Boolean value\nis_calibrated = MetaValue(value=True)\n\nprint(temperature.value)  # 25.5\nprint(temperature.unit)   # \"\u00b0C\"\nprint(sample_id.unit)     # None\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#validableitems","title":"ValidableItems","text":"<p>Container for collections of validatable metadata items.</p>"},{"location":"rdetoolkit/models/metadata/#constructor-for-validableitems","title":"Constructor for <code>ValidableItems</code>","text":"<pre><code>ValidableItems(root: list[dict[str, MetaValue]])\n</code></pre> <p>Parameters:</p> <ul> <li><code>root</code> (list[dict[str, MetaValue]]): List of dictionaries mapping string keys to MetaValue instances</li> </ul>"},{"location":"rdetoolkit/models/metadata/#methods","title":"Methods","text":"<p>The class inherits from <code>RootModel</code> and provides standard list-like access to the underlying data.</p>"},{"location":"rdetoolkit/models/metadata/#example-validableitems","title":"Example: ValidableItems","text":"<pre><code>from rdetoolkit.models.metadata import ValidableItems, MetaValue\n\n# Create metadata items\nitems = ValidableItems(root=[\n    {\n        \"measurement_1\": MetaValue(value=25.0, unit=\"\u00b0C\"),\n        \"timestamp_1\": MetaValue(value=\"2025-01-15T10:00:00Z\")\n    },\n    {\n        \"measurement_2\": MetaValue(value=26.5, unit=\"\u00b0C\"),\n        \"timestamp_2\": MetaValue(value=\"2025-01-15T10:05:00Z\")\n    }\n])\n\n# Access items\nprint(len(items.root))  # 2\nprint(items.root[0][\"measurement_1\"].value)  # 25.0\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#metadataitem","title":"MetadataItem","text":"<p>Main metadata container representing the complete metadata structure for a dataset.</p>"},{"location":"rdetoolkit/models/metadata/#constructor","title":"Constructor","text":"<pre><code>MetadataItem(\n    constant: dict[str, MetaValue],\n    variable: ValidableItems\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>constant</code> (dict[str, MetaValue]): Dictionary of constant metadata that applies to all measurements</li> <li><code>variable</code> (ValidableItems): Collection of variable metadata that changes between measurements</li> </ul>"},{"location":"rdetoolkit/models/metadata/#example","title":"Example","text":"<pre><code>from rdetoolkit.models.metadata import MetadataItem, MetaValue, ValidableItems\n\n# Define constant metadata\nconstant_meta = {\n    \"instrument\": MetaValue(value=\"XRD_Analyzer_v2\", unit=None),\n    \"operator\": MetaValue(value=\"Dr. Smith\"),\n    \"calibration_date\": MetaValue(value=\"2025-01-01\"),\n    \"lab_temperature\": MetaValue(value=22.0, unit=\"\u00b0C\")\n}\n\n# Define variable metadata\nvariable_meta = ValidableItems(root=[\n    {\n        \"sample_temp\": MetaValue(value=100.0, unit=\"\u00b0C\"),\n        \"measurement_time\": MetaValue(value=\"2025-01-15T10:00:00Z\"),\n        \"scan_rate\": MetaValue(value=0.5, unit=\"\u00b0/min\")\n    },\n    {\n        \"sample_temp\": MetaValue(value=150.0, unit=\"\u00b0C\"),\n        \"measurement_time\": MetaValue(value=\"2025-01-15T11:00:00Z\"),\n        \"scan_rate\": MetaValue(value=1.0, unit=\"\u00b0/min\")\n    }\n])\n\n# Create complete metadata\nmetadata = MetadataItem(\n    constant=constant_meta,\n    variable=variable_meta\n)\n\n# Access metadata\nprint(metadata.constant[\"instrument\"].value)  # \"XRD_Analyzer_v2\"\nprint(metadata.variable.root[0][\"sample_temp\"].value)  # 100.0\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/models/metadata/#creating-metadata-for-scientific-measurements","title":"Creating Metadata for Scientific Measurements","text":"<pre><code>from rdetoolkit.models.metadata import MetadataItem, MetaValue, ValidableItems\n\ndef create_xrd_metadata(measurements: list[dict]) -&gt; MetadataItem:\n    \"\"\"Create metadata for XRD measurement series.\"\"\"\n\n    # Constant metadata - same for all measurements\n    constant = {\n        \"instrument_model\": MetaValue(value=\"Rigaku MiniFlex\", unit=None),\n        \"x_ray_source\": MetaValue(value=\"Cu K\u03b1\", unit=None),\n        \"wavelength\": MetaValue(value=1.5406, unit=\"\u00c5\"),\n        \"detector_type\": MetaValue(value=\"NaI scintillation\"),\n        \"operator\": MetaValue(value=\"Lab Technician A\"),\n        \"facility\": MetaValue(value=\"Materials Analysis Lab\"),\n        \"calibration_standard\": MetaValue(value=\"Si powder\")\n    }\n\n    # Variable metadata - changes per measurement\n    variable_data = []\n    for i, measurement in enumerate(measurements):\n        variable_data.append({\n            \"measurement_id\": MetaValue(value=f\"XRD_{i+1:03d}\"),\n            \"sample_temperature\": MetaValue(\n                value=measurement.get(\"temperature\", 25.0),\n                unit=\"\u00b0C\"\n            ),\n            \"scan_range_start\": MetaValue(\n                value=measurement.get(\"start_angle\", 10.0),\n                unit=\"\u00b0\"\n            ),\n            \"scan_range_end\": MetaValue(\n                value=measurement.get(\"end_angle\", 80.0),\n                unit=\"\u00b0\"\n            ),\n            \"step_size\": MetaValue(\n                value=measurement.get(\"step_size\", 0.02),\n                unit=\"\u00b0\"\n            ),\n            \"scan_speed\": MetaValue(\n                value=measurement.get(\"scan_speed\", 1.0),\n                unit=\"\u00b0/min\"\n            ),\n            \"measurement_time\": MetaValue(\n                value=measurement.get(\"timestamp\", \"2025-01-15T10:00:00Z\")\n            )\n        })\n\n    return MetadataItem(\n        constant=constant,\n        variable=ValidableItems(root=variable_data)\n    )\n\n# Usage example\nmeasurements = [\n    {\n        \"temperature\": 25.0,\n        \"start_angle\": 10.0,\n        \"end_angle\": 80.0,\n        \"step_size\": 0.02,\n        \"scan_speed\": 1.0,\n        \"timestamp\": \"2025-01-15T10:00:00Z\"\n    },\n    {\n        \"temperature\": 100.0,\n        \"start_angle\": 15.0,\n        \"end_angle\": 75.0,\n        \"step_size\": 0.01,\n        \"scan_speed\": 0.5,\n        \"timestamp\": \"2025-01-15T11:30:00Z\"\n    }\n]\n\nxrd_metadata = create_xrd_metadata(measurements)\nprint(f\"Created metadata for {len(xrd_metadata.variable.root)} measurements\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#handling-large-text-metadata-with-validation","title":"Handling Large Text Metadata with Validation","text":"<pre><code>from rdetoolkit.models.metadata import MetaValue, Variable\nfrom pydantic import ValidationError\n\ndef safe_create_metadata(value: str, unit: str = None) -&gt; MetaValue | None:\n    \"\"\"Safely create metadata with size validation.\"\"\"\n    try:\n        return MetaValue(value=value, unit=unit)\n    except ValidationError as e:\n        print(f\"Validation failed for value: {e}\")\n        return None\n\n# Test with various value sizes\ntest_values = [\n    \"Short description\",\n    \"Medium length description that might be acceptable\",\n    \"Very long description that exceeds the maximum allowed size limit\" * 50,  # Too long\n    \"\u65e5\u672c\u8a9e\u306e\u8aac\u660e\u6587\u3067\u3059\u3002UTF-8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067\u30d0\u30a4\u30c8\u6570\u3092\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3059\u3002\",\n    \"\ud83c\udf1f Unicode emoji and special characters \ud83d\udd2c\",\n]\n\nmetadata_items = []\nfor i, value in enumerate(test_values):\n    meta = safe_create_metadata(value, \"description\")\n    if meta:\n        metadata_items.append({f\"item_{i}\": meta})\n        print(f\"\u2713 Created metadata for item {i}\")\n    else:\n        print(f\"\u2717 Failed to create metadata for item {i}\")\n\nprint(f\"Successfully created {len(metadata_items)} metadata items\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#working-with-complex-data-types","title":"Working with Complex Data Types","text":"<pre><code>from rdetoolkit.models.metadata import MetaValue, MetadataItem, ValidableItems\nimport json\nfrom datetime import datetime\n\n# Handling various data types\nmetadata_examples = {\n    # Numeric values\n    \"temperature\": MetaValue(value=25.5, unit=\"\u00b0C\"),\n    \"pressure\": MetaValue(value=1013.25, unit=\"hPa\"),\n    \"count\": MetaValue(value=12345, unit=\"counts\"),\n\n    # String values\n    \"sample_id\": MetaValue(value=\"SAMPLE_2025_001\"),\n    \"batch_number\": MetaValue(value=\"BATCH_A001\"),\n\n    # Boolean values\n    \"is_calibrated\": MetaValue(value=True),\n    \"quality_check_passed\": MetaValue(value=False),\n\n    # List/Array values (not size validated)\n    \"wavelengths\": MetaValue(value=[400, 500, 600, 700], unit=\"nm\"),\n    \"coordinates\": MetaValue(value=[10.5, 20.3, 30.1], unit=\"mm\"),\n\n    # Dictionary values (not size validated)\n    \"settings\": MetaValue(value={\n        \"gain\": 1.5,\n        \"offset\": 0.1,\n        \"mode\": \"auto\"\n    }),\n\n    # None values\n    \"optional_field\": MetaValue(value=None),\n}\n\n# Create metadata with mixed types\nmixed_metadata = MetadataItem(\n    constant=metadata_examples,\n    variable=ValidableItems(root=[])\n)\n\n# Access and display metadata\nfor key, meta_value in mixed_metadata.constant.items():\n    print(f\"{key}: {meta_value.value} {meta_value.unit or ''}\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#serialization-and-deserialization","title":"Serialization and Deserialization","text":"<pre><code>from rdetoolkit.models.metadata import MetadataItem, MetaValue, ValidableItems\nimport json\n\n# Create metadata\nmetadata = MetadataItem(\n    constant={\n        \"instrument\": MetaValue(value=\"Spectrometer X1\", unit=None),\n        \"wavelength\": MetaValue(value=632.8, unit=\"nm\")\n    },\n    variable=ValidableItems(root=[\n        {\n            \"power\": MetaValue(value=10.0, unit=\"mW\"),\n            \"exposure\": MetaValue(value=1.0, unit=\"s\")\n        },\n        {\n            \"power\": MetaValue(value=20.0, unit=\"mW\"),\n            \"exposure\": MetaValue(value=0.5, unit=\"s\")\n        }\n    ])\n)\n\n# Serialize to dictionary\nmetadata_dict = metadata.model_dump()\nprint(\"Serialized metadata:\")\nprint(json.dumps(metadata_dict, indent=2))\n\n# Serialize to JSON string\nmetadata_json = metadata.model_dump_json(indent=2)\nprint(\"\\nJSON representation:\")\nprint(metadata_json)\n\n# Deserialize from dictionary\nreconstructed = MetadataItem(**metadata_dict)\nprint(f\"\\nReconstructed metadata has {len(reconstructed.variable.root)} variable entries\")\n\n# Verify data integrity\noriginal_instrument = metadata.constant[\"instrument\"].value\nreconstructed_instrument = reconstructed.constant[\"instrument\"].value\nassert original_instrument == reconstructed_instrument\nprint(\"\u2713 Data integrity verified\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#batch-processing-metadata","title":"Batch Processing Metadata","text":"<pre><code>from rdetoolkit.models.metadata import MetadataItem, MetaValue, ValidableItems\nfrom typing import List, Dict, Any\n\ndef process_measurement_batch(\n    constant_data: Dict[str, Any],\n    measurements: List[Dict[str, Any]]\n) -&gt; MetadataItem:\n    \"\"\"Process a batch of measurements into structured metadata.\"\"\"\n\n    # Convert constant data\n    constant_meta = {}\n    for key, value in constant_data.items():\n        if isinstance(value, dict) and \"value\" in value:\n            # Handle pre-structured data\n            constant_meta[key] = MetaValue(\n                value=value[\"value\"],\n                unit=value.get(\"unit\")\n            )\n        else:\n            # Handle simple values\n            constant_meta[key] = MetaValue(value=value)\n\n    # Convert variable data\n    variable_data = []\n    for measurement in measurements:\n        measurement_meta = {}\n        for key, value in measurement.items():\n            if isinstance(value, dict) and \"value\" in value:\n                measurement_meta[key] = MetaValue(\n                    value=value[\"value\"],\n                    unit=value.get(\"unit\")\n                )\n            else:\n                measurement_meta[key] = MetaValue(value=value)\n        variable_data.append(measurement_meta)\n\n    return MetadataItem(\n        constant=constant_meta,\n        variable=ValidableItems(root=variable_data)\n    )\n\n# Example usage\nbatch_constant = {\n    \"experiment_id\": \"EXP_2025_001\",\n    \"instrument\": \"High-res Spectrometer\",\n    \"operator\": \"Dr. Johnson\",\n    \"lab_conditions\": {\n        \"value\": \"Standard atmosphere\",\n        \"unit\": None\n    }\n}\n\nbatch_measurements = [\n    {\n        \"sample_id\": \"S001\",\n        \"temperature\": {\"value\": 20.0, \"unit\": \"\u00b0C\"},\n        \"intensity\": {\"value\": 1500, \"unit\": \"counts\"},\n        \"timestamp\": \"2025-01-15T09:00:00Z\"\n    },\n    {\n        \"sample_id\": \"S002\",\n        \"temperature\": {\"value\": 25.0, \"unit\": \"\u00b0C\"},\n        \"intensity\": {\"value\": 1750, \"unit\": \"counts\"},\n        \"timestamp\": \"2025-01-15T09:15:00Z\"\n    }\n]\n\nbatch_metadata = process_measurement_batch(batch_constant, batch_measurements)\nprint(f\"Processed batch with {len(batch_metadata.variable.root)} measurements\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/models/metadata/#validation-errors","title":"Validation Errors","text":"<p>The metadata module raises <code>ValueError</code> exceptions when validation fails:</p>"},{"location":"rdetoolkit/models/metadata/#size-limit-exceeded","title":"Size Limit Exceeded","text":"<pre><code>from rdetoolkit.models.metadata import MetaValue, Variable\nfrom pydantic import ValidationError\n\n# Test size limit for MetaValue\ntry:\n    large_value = \"x\" * 2000  # Exceeds 1024 byte limit\n    invalid_meta = MetaValue(value=large_value)\nexcept ValidationError as e:\n    print(f\"MetaValue validation failed: {e}\")\n\n# Test size limit for Variable\ntry:\n    large_description = \"y\" * 2000  # Exceeds 1024 byte limit\n    invalid_variable = Variable(variable={\"description\": large_description})\nexcept ValidationError as e:\n    print(f\"Variable validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#utf-8-encoding-considerations","title":"UTF-8 Encoding Considerations","text":"<pre><code>from rdetoolkit.models.metadata import MetaValue\n\n# Multi-byte characters count as multiple bytes\njapanese_text = \"\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u30c6\u30b9\u30c8\u3067\u3059\u3002\" * 50  # May exceed byte limit\ntry:\n    meta = MetaValue(value=japanese_text)\n    print(\"\u2713 Japanese text accepted\")\nexcept ValidationError as e:\n    print(f\"\u2717 Japanese text too large: {e}\")\n\n# Emoji and special characters\nemoji_text = \"\ud83d\udd2c\" * 300  # Each emoji is 4 bytes in UTF-8\ntry:\n    meta = MetaValue(value=emoji_text)\n    print(\"\u2713 Emoji text accepted\")\nexcept ValidationError as e:\n    print(f\"\u2717 Emoji text too large: {e}\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#best-practices","title":"Best Practices","text":"<ol> <li>Validate Input Sizes: Check string sizes before creating metadata:</li> </ol> <pre><code>def safe_create_meta_value(value: Any, unit: str = None) -&gt; MetaValue | None:\n    if isinstance(value, str):\n        byte_size = len(value.encode('utf-8'))\n        if byte_size &gt; 1024:\n            print(f\"Warning: Value size {byte_size} bytes exceeds limit\")\n            return None\n    return MetaValue(value=value, unit=unit)\n</code></pre> <ol> <li>Handle Large Text Data: Truncate or summarize large text values:</li> </ol> <pre><code>def truncate_large_text(text: str, max_bytes: int = 1000) -&gt; str:\n    if len(text.encode('utf-8')) &lt;= max_bytes:\n        return text\n\n    # Truncate while respecting UTF-8 boundaries\n    encoded = text.encode('utf-8')\n    truncated = encoded[:max_bytes]\n\n    # Ensure we don't break in the middle of a character\n    try:\n        return truncated.decode('utf-8') + \"...\"\n    except UnicodeDecodeError:\n        # Back off until we find a valid boundary\n        for i in range(max_bytes - 1, max_bytes - 4, -1):\n            try:\n                return encoded[:i].decode('utf-8') + \"...\"\n            except UnicodeDecodeError:\n                continue\n        return \"...\"\n</code></pre> <ol> <li>Use Type Hints: Improve code clarity and IDE support:</li> </ol> <pre><code>from typing import Dict, List, Any, Optional\n\ndef create_metadata_from_dict(\n    data: Dict[str, Any]\n) -&gt; Optional[MetadataItem]:\n    try:\n        # Process data...\n        return MetadataItem(constant=constant, variable=variable)\n    except ValidationError:\n        return None\n</code></pre> <ol> <li>Separate Large Data: Store large data separately and reference it:</li> </ol> <pre><code># Instead of storing large data directly\nlarge_data = [1] * 10000  # Large array\n\n# Store a reference or summary\nmetadata = MetaValue(\n    value=\"large_dataset_ref_001\",\n    unit=\"dataset_id\"\n)\n</code></pre> <ol> <li>Validate Before Batch Operations: Check data validity before processing:</li> </ol> <pre><code>def validate_measurement_data(measurements: List[Dict]) -&gt; bool:\n    for measurement in measurements:\n        for key, value in measurement.items():\n            if isinstance(value, str):\n                if len(value.encode('utf-8')) &gt; 1024:\n                    print(f\"Invalid measurement {key}: value too large\")\n                    return False\n    return True\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#performance-notes","title":"Performance Notes","text":"<ul> <li>Validation occurs at object creation time, not during access</li> <li>String size validation uses UTF-8 encoding for accurate byte counting</li> <li>Non-string values bypass size validation for optimal performance</li> <li>Pydantic models provide efficient serialization and deserialization</li> <li>Large datasets should be referenced rather than embedded directly</li> </ul>"},{"location":"rdetoolkit/models/metadata/#integration-examples","title":"Integration Examples","text":""},{"location":"rdetoolkit/models/metadata/#working-with-file-metadata","title":"Working with File Metadata","text":"<pre><code>from rdetoolkit.models.metadata import MetadataItem, MetaValue, ValidableItems\nfrom pathlib import Path\nimport json\n\ndef extract_file_metadata(file_path: Path) -&gt; MetadataItem:\n    \"\"\"Extract metadata from a file and create structured metadata.\"\"\"\n\n    stat = file_path.stat()\n\n    constant = {\n        \"file_name\": MetaValue(value=file_path.name),\n        \"file_extension\": MetaValue(value=file_path.suffix),\n        \"file_size\": MetaValue(value=stat.st_size, unit=\"bytes\"),\n        \"creation_time\": MetaValue(value=stat.st_ctime),\n        \"modification_time\": MetaValue(value=stat.st_mtime),\n    }\n\n    # Variable data might come from file contents\n    variable = ValidableItems(root=[])\n\n    return MetadataItem(constant=constant, variable=variable)\n\n# Usage\nfile_metadata = extract_file_metadata(Path(\"data.txt\"))\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#database-storage","title":"Database Storage","text":"<pre><code>from rdetoolkit.models.metadata import MetadataItem\nimport sqlite3\nimport json\n\ndef store_metadata_in_db(metadata: MetadataItem, db_path: str, record_id: str):\n    \"\"\"Store metadata in a database.\"\"\"\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    # Create table if it doesn't exist\n    cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS metadata (\n            id TEXT PRIMARY KEY,\n            constant_data TEXT,\n            variable_data TEXT\n        )\n    \"\"\")\n\n    # Serialize metadata\n    constant_json = json.dumps(metadata.model_dump()[\"constant\"])\n    variable_json = json.dumps(metadata.model_dump()[\"variable\"])\n\n    # Insert data\n    cursor.execute(\"\"\"\n        INSERT OR REPLACE INTO metadata (id, constant_data, variable_data)\n        VALUES (?, ?, ?)\n    \"\"\", (record_id, constant_json, variable_json))\n\n    conn.commit()\n    conn.close()\n\ndef load_metadata_from_db(db_path: str, record_id: str) -&gt; MetadataItem:\n    \"\"\"Load metadata from database.\"\"\"\n\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n        SELECT constant_data, variable_data FROM metadata WHERE id = ?\n    \"\"\", (record_id,))\n\n    row = cursor.fetchone()\n    conn.close()\n\n    if row:\n        constant_data = json.loads(row[0])\n        variable_data = json.loads(row[1])\n\n        return MetadataItem(\n            constant=constant_data,\n            variable=variable_data\n        )\n    else:\n        raise ValueError(f\"No metadata found for ID: {record_id}\")\n</code></pre>"},{"location":"rdetoolkit/models/metadata/#see-also","title":"See Also","text":"<ul> <li>Invoice Schema Module - For schema validation and structure definitions</li> <li>Invoice Module - For invoice template and term management</li> <li>Core Module - For file operations and directory management</li> <li>Pydantic Documentation - For advanced validation patterns</li> </ul>"},{"location":"rdetoolkit/models/rde2types/","title":"RDE2 Types Module","text":"<p>The <code>rdetoolkit.models.rde2types</code> module provides comprehensive type definitions, data structures, and utility classes for RDE (Research Data Exchange) version 2 systems. This module defines essential types, path management utilities, and configuration structures used throughout the RDE processing pipeline.</p>"},{"location":"rdetoolkit/models/rde2types/#overview","title":"Overview","text":"<p>The RDE2 types module implements a complete type system for managing research data workflows with the following capabilities:</p> <ul> <li>Type Aliases: Comprehensive type definitions for file paths, metadata, and data structures</li> <li>Path Management: Structured handling of input and output directory paths</li> <li>Configuration Management: Default configuration creation and management</li> <li>Data Structures: TypedDict definitions for JSON schema validation</li> <li>Legacy Support: Deprecated features with migration warnings</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#type-aliases","title":"Type Aliases","text":"<p>The module defines several type aliases for improved code clarity and type safety.</p>"},{"location":"rdetoolkit/models/rde2types/#file-path-types","title":"File Path Types","text":"<pre><code>ZipFilesPathList = Sequence[Path]\nUnZipFilesPathList = Sequence[Path]\nExcelInvoicePathList = Sequence[Path]\nOtherFilesPathList = Sequence[Path]\nPathTuple = tuple[Path, ...]\nRdeFsPath = Union[str, Path]\n</code></pre> <ul> <li>ZipFilesPathList: Sequence of ZIP file paths</li> <li>UnZipFilesPathList: Sequence of extracted file paths</li> <li>ExcelInvoicePathList: Sequence of Excel invoice file paths</li> <li>OtherFilesPathList: Sequence of other file paths</li> <li>PathTuple: Tuple of Path objects</li> <li>RdeFsPath: Union type for filesystem paths (string or Path)</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#data-structure-types","title":"Data Structure Types","text":"<pre><code>InputFilesGroup = tuple[ZipFilesPathList, ExcelInvoicePathList, OtherFilesPathList]\nRawFiles = Sequence[PathTuple]\nMetaType = dict[str, Union[str, int, float, list, bool]]\nRepeatedMetaType = dict[str, list[Union[str, int, float]]]\nMetaItem = dict[str, Union[str, int, float, bool]]\n</code></pre> <ul> <li>InputFilesGroup: Grouped input files by type</li> <li>RawFiles: Sequence of raw file tuples</li> <li>MetaType: Flexible metadata dictionary type</li> <li>RepeatedMetaType: Metadata with repeated values</li> <li>MetaItem: Individual metadata item type</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#example-usage","title":"Example Usage","text":"<pre><code>from rdetoolkit.models.rde2types import (\n    ZipFilesPathList, InputFilesGroup, MetaType, RdeFsPath\n)\nfrom pathlib import Path\n\n# File path collections\nzip_files: ZipFilesPathList = [\n    Path(\"data1.zip\"),\n    Path(\"data2.zip\"),\n    Path(\"data3.zip\")\n]\n\nexcel_files: ExcelInvoicePathList = [\n    Path(\"invoice1.xlsx\"),\n    Path(\"invoice2.xlsx\")\n]\n\nother_files: OtherFilesPathList = [\n    Path(\"readme.txt\"),\n    Path(\"config.json\")\n]\n\n# Group files\ninput_group: InputFilesGroup = (zip_files, excel_files, other_files)\n\n# Metadata example\nmetadata: MetaType = {\n    \"temperature\": 25.5,\n    \"pressure\": 1013.25,\n    \"sample_id\": \"SAMPLE_001\",\n    \"measurements\": [1.1, 2.2, 3.3],\n    \"is_calibrated\": True\n}\n\n# Flexible path handling\ndef process_path(path: RdeFsPath) -&gt; Path:\n    return Path(path) if isinstance(path, str) else path\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#configuration-functions","title":"Configuration Functions","text":""},{"location":"rdetoolkit/models/rde2types/#create_default_config","title":"create_default_config()","text":"<p>Creates and returns a default configuration object for RDE processing.</p> <pre><code>def create_default_config() -&gt; Config\n</code></pre> <p>Returns:</p> <ul> <li><code>Config</code>: A default configuration object with standard settings</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.models.rde2types import create_default_config\n\n# Create default configuration\nconfig = create_default_config()\n\nprint(config.system.extended_mode)  # None\nprint(config.system.save_raw)       # True\nprint(config.system.save_thumbnail_image)  # False\nprint(config.system.magic_variable)  # False\nprint(config.multidata_tile.ignore_errors)  # False\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#path-management-classes","title":"Path Management Classes","text":""},{"location":"rdetoolkit/models/rde2types/#rdeinputdirpaths","title":"RdeInputDirPaths","text":"<p>Data class for managing input directory paths in RDE processing.</p>"},{"location":"rdetoolkit/models/rde2types/#rdeinputdirpaths-constructor","title":"RdeInputDirPaths Constructor","text":"<pre><code>RdeInputDirPaths(\n    inputdata: Path,\n    invoice: Path,\n    tasksupport: Path,\n    config: Config = field(default_factory=create_default_config)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>inputdata</code> (Path): Path to the input data directory</li> <li><code>invoice</code> (Path): Path to the invoice directory containing invoice.json</li> <li><code>tasksupport</code> (Path): Path to the task support data directory</li> <li><code>config</code> (Config): Configuration object (defaults to <code>create_default_config()</code>)</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#properties","title":"Properties","text":""},{"location":"rdetoolkit/models/rde2types/#default_csv","title":"default_csv","text":"<p>Returns the path to the 'default_value.csv' file.</p> <pre><code>@property\ndef default_csv(self) -&gt; Path\n</code></pre> <p>Returns:</p> <ul> <li><code>Path</code>: Path to the default_value.csv file</li> </ul> <p>Behavior:</p> <ul> <li>If <code>tasksupport</code> is set, uses that path</li> <li>Otherwise, uses the default path under 'data/tasksupport'</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#usage-example","title":"Usage Example","text":"<pre><code>from rdetoolkit.models.rde2types import RdeInputDirPaths, create_default_config\nfrom pathlib import Path\n\n# Create input paths configuration\ninput_paths = RdeInputDirPaths(\n    inputdata=Path(\"input/data\"),\n    invoice=Path(\"input/invoices\"),\n    tasksupport=Path(\"support\"),\n    config=create_default_config()\n)\n\n# Access paths\nprint(input_paths.inputdata)     # input/data\nprint(input_paths.invoice)       # input/invoices\nprint(input_paths.tasksupport)   # support\nprint(input_paths.default_csv)   # support/default_value.csv\n\n# With None tasksupport (uses default)\ninput_paths_default = RdeInputDirPaths(\n    inputdata=Path(\"input/data\"),\n    invoice=Path(\"input/invoices\"),\n    tasksupport=None\n)\nprint(input_paths_default.default_csv)  # data/tasksupport/default_value.csv\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#rdeoutputresourcepath","title":"RdeOutputResourcePath","text":"<p>Data class for managing output resource paths in RDE processing.</p>"},{"location":"rdetoolkit/models/rde2types/#valueunitpair-initialization","title":"ValueUnitPair Initialization","text":"<pre><code>RdeOutputResourcePath(\n    raw: Path,\n    nonshared_raw: Path,\n    rawfiles: tuple[Path, ...],\n    struct: Path,\n    main_image: Path,\n    other_image: Path,\n    meta: Path,\n    thumbnail: Path,\n    logs: Path,\n    invoice: Path,\n    invoice_schema_json: Path,\n    invoice_org: Path,\n    temp: Path | None = None,\n    invoice_patch: Path | None = None,\n    attachment: Path | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>raw</code> (Path): Path for raw data storage</li> <li><code>nonshared_raw</code> (Path): Path for non-shared raw data</li> <li><code>rawfiles</code> (tuple[Path, ...]): Tuple of input file paths for single data tile</li> <li><code>struct</code> (Path): Path for structured data storage</li> <li><code>main_image</code> (Path): Path for main image files</li> <li><code>other_image</code> (Path): Path for other image files</li> <li><code>meta</code> (Path): Path for metadata files</li> <li><code>thumbnail</code> (Path): Path for thumbnail images</li> <li><code>logs</code> (Path): Path for log files</li> <li><code>invoice</code> (Path): Path for invoice files</li> <li><code>invoice_schema_json</code> (Path): Path for invoice.schema.json</li> <li><code>invoice_org</code> (Path): Path for original invoice.json backup</li> <li><code>temp</code> (Path | None): Optional path for temporary files</li> <li><code>invoice_patch</code> (Path | None): Optional path for modified invoices</li> <li><code>attachment</code> (Path | None): Optional path for attachment files</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#example-usage-name","title":"Example Usage: Name","text":"<pre><code>from rdetoolkit.models.rde2types import RdeOutputResourcePath\nfrom pathlib import Path\n\n# Create output resource paths\noutput_paths = RdeOutputResourcePath(\n    raw=Path(\"output/raw\"),\n    nonshared_raw=Path(\"output/nonshared_raw\"),\n    rawfiles=(Path(\"file1.txt\"), Path(\"file2.txt\"), Path(\"file3.txt\")),\n    struct=Path(\"output/structured\"),\n    main_image=Path(\"output/images/main\"),\n    other_image=Path(\"output/images/other\"),\n    meta=Path(\"output/metadata\"),\n    thumbnail=Path(\"output/thumbnails\"),\n    logs=Path(\"output/logs\"),\n    invoice=Path(\"output/invoices\"),\n    invoice_schema_json=Path(\"output/schema/invoice.schema.json\"),\n    invoice_org=Path(\"output/backup/invoice_original.json\"),\n    temp=Path(\"output/temp\"),\n    invoice_patch=Path(\"output/patches\"),\n    attachment=Path(\"output/attachments\")\n)\n\n# Access paths\nprint(output_paths.raw)           # output/raw\nprint(output_paths.struct)        # output/structured\nprint(output_paths.main_image)    # output/images/main\nprint(len(output_paths.rawfiles)) # 3\n\n# Optional paths\nif output_paths.temp:\n    print(f\"Temp directory: {output_paths.temp}\")\n\nif output_paths.attachment:\n    print(f\"Attachment directory: {output_paths.attachment}\")\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#typeddict-definitions","title":"TypedDict Definitions","text":""},{"location":"rdetoolkit/models/rde2types/#name","title":"Name","text":"<p>TypedDict for representing multilingual names.</p> <pre><code>class Name(TypedDict):\n    ja: str  # Japanese name\n    en: str  # English name\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#example-name","title":"Example: Name","text":"<pre><code>from rdetoolkit.models.rde2types import Name\n\n# Create multilingual name\nsample_name: Name = {\n    \"ja\": \"\u30b5\u30f3\u30d7\u30eb\u6750\u6599\",\n    \"en\": \"Sample Material\"\n}\n\nprint(sample_name[\"ja\"])  # \u30b5\u30f3\u30d7\u30eb\u6750\u6599\nprint(sample_name[\"en\"])  # Sample Material\n\n# Type checking\ndef process_name(name: Name) -&gt; str:\n    return f\"{name['en']} ({name['ja']})\"\n\nresult = process_name(sample_name)\nprint(result)  # Sample Material (\u30b5\u30f3\u30d7\u30eb\u6750\u6599)\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#schema","title":"Schema","text":"<p>TypedDict for schema definitions with optional fields.</p> <pre><code>class Schema(TypedDict, total=False):\n    type: str    # Schema type\n    format: str  # Schema format\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#example-schema","title":"Example: Schema","text":"<pre><code>from rdetoolkit.models.rde2types import Schema\n\n# Complete schema\nfull_schema: Schema = {\n    \"type\": \"string\",\n    \"format\": \"date-time\"\n}\n\n# Partial schema (total=False allows this)\npartial_schema: Schema = {\n    \"type\": \"number\"\n    # format is optional\n}\n\n# Type checking function\ndef validate_schema(schema: Schema) -&gt; bool:\n    return \"type\" in schema\n\nprint(validate_schema(full_schema))    # True\nprint(validate_schema(partial_schema)) # True\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#metadatadefjson","title":"MetadataDefJson","text":"<p>Comprehensive TypedDict for metadata definition structure.</p> <pre><code>class MetadataDefJson(TypedDict):\n    name: Name           # Multilingual name\n    schema: Schema       # Schema definition\n    unit: str           # Unit of measurement\n    description: str     # Description\n    uri: str            # URI reference\n    originalName: str    # Original name\n    originalType: str    # Original type\n    mode: str           # Processing mode\n    order: str          # Order specification\n    valiable: int       # Variable identifier\n    _feature: bool      # Feature flag\n    action: str         # Action specification\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#example-metadatadefjson","title":"Example: MetadataDefJson","text":"<pre><code>from rdetoolkit.models.rde2types import MetadataDefJson, Name, Schema\n\n# Create complete metadata definition\nmetadata_def: MetadataDefJson = {\n    \"name\": {\n        \"ja\": \"\u6e29\u5ea6\",\n        \"en\": \"Temperature\"\n    },\n    \"schema\": {\n        \"type\": \"number\",\n        \"format\": \"float\"\n    },\n    \"unit\": \"\u00b0C\",\n    \"description\": \"Sample temperature measurement\",\n    \"uri\": \"http://example.com/temperature\",\n    \"originalName\": \"temp_sensor_1\",\n    \"originalType\": \"float64\",\n    \"mode\": \"measurement\",\n    \"order\": \"ascending\",\n    \"valiable\": 1,\n    \"_feature\": True,\n    \"action\": \"record\"\n}\n\n# Access metadata fields\nprint(metadata_def[\"name\"][\"en\"])  # Temperature\nprint(metadata_def[\"unit\"])        # \u00b0C\nprint(metadata_def[\"schema\"][\"type\"])  # number\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#data-classes","title":"Data Classes","text":""},{"location":"rdetoolkit/models/rde2types/#valueunitpair","title":"ValueUnitPair","text":"<p>Simple dataclass for value-unit pairs.</p>"},{"location":"rdetoolkit/models/rde2types/#valueunitpair-constructor","title":"ValueUnitPair Constructor","text":"<pre><code>ValueUnitPair(value: str, unit: str)\n</code></pre> <p>Parameters:</p> <ul> <li><code>value</code> (str): The value component</li> <li><code>unit</code> (str): The unit component</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#example","title":"Example","text":"<pre><code>from rdetoolkit.models.rde2types import ValueUnitPair\n\n# Create value-unit pairs\ntemperature = ValueUnitPair(value=\"25.5\", unit=\"\u00b0C\")\npressure = ValueUnitPair(value=\"1013.25\", unit=\"hPa\")\ndistance = ValueUnitPair(value=\"10.0\", unit=\"mm\")\n\nprint(f\"Temperature: {temperature.value} {temperature.unit}\")  # Temperature: 25.5 \u00b0C\nprint(f\"Pressure: {pressure.value} {pressure.unit}\")          # Pressure: 1013.25 hPa\n\n# Use in collections\nmeasurements = [\n    ValueUnitPair(\"25.5\", \"\u00b0C\"),\n    ValueUnitPair(\"1013.25\", \"hPa\"),\n    ValueUnitPair(\"60\", \"%RH\")\n]\n\nfor measurement in measurements:\n    print(f\"{measurement.value} {measurement.unit}\")\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#legacy-classes-deprecated","title":"Legacy Classes (Deprecated)","text":""},{"location":"rdetoolkit/models/rde2types/#rdeformatflags","title":"RdeFormatFlags","text":"<p>\u26a0\ufe0f Warning: This class is deprecated and scheduled for removal.</p> <p>Legacy class for managing RDE format flags. This class is no longer used and will be removed in future versions.</p>"},{"location":"rdetoolkit/models/rde2types/#constructor","title":"Constructor","text":"<pre><code>RdeFormatFlags()\n</code></pre> <p>Warning Behavior:</p> <ul> <li>Emits a <code>FutureWarning</code> when instantiated</li> <li>Functionality is preserved for backward compatibility</li> </ul>"},{"location":"rdetoolkit/models/rde2types/#example-not-recommended-rdeformatflags","title":"Example (Not Recommended): RdeFormatFlags","text":"<pre><code>from rdetoolkit.models.rde2types import RdeFormatFlags\nimport warnings\n\n# This will emit a deprecation warning\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", FutureWarning)\n    flags = RdeFormatFlags()\n\n# Migration: Remove usage of RdeFormatFlags\n# Use configuration objects instead\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/models/rde2types/#setting-up-rde-processing-pipeline","title":"Setting Up RDE Processing Pipeline","text":"<pre><code>from rdetoolkit.models.rde2types import (\n    RdeInputDirPaths, RdeOutputResourcePath, create_default_config,\n    ZipFilesPathList, ExcelInvoicePathList, OtherFilesPathList, InputFilesGroup\n)\nfrom pathlib import Path\n\ndef setup_rde_pipeline(base_input: Path, base_output: Path):\n    \"\"\"Set up complete RDE processing pipeline paths.\"\"\"\n\n    # Configure input paths\n    input_paths = RdeInputDirPaths(\n        inputdata=base_input / \"data\",\n        invoice=base_input / \"invoices\",\n        tasksupport=base_input / \"support\",\n        config=create_default_config()\n    )\n\n    # Configure output paths\n    output_paths = RdeOutputResourcePath(\n        raw=base_output / \"raw\",\n        nonshared_raw=base_output / \"nonshared_raw\",\n        rawfiles=tuple(),  # Will be populated during processing\n        struct=base_output / \"structured\",\n        main_image=base_output / \"images\" / \"main\",\n        other_image=base_output / \"images\" / \"other\",\n        meta=base_output / \"metadata\",\n        thumbnail=base_output / \"thumbnails\",\n        logs=base_output / \"logs\",\n        invoice=base_output / \"invoices\",\n        invoice_schema_json=base_output / \"schema\" / \"invoice.schema.json\",\n        invoice_org=base_output / \"backup\" / \"invoice_original.json\",\n        temp=base_output / \"temp\",\n        invoice_patch=base_output / \"patches\",\n        attachment=base_output / \"attachments\"\n    )\n\n    return input_paths, output_paths\n\n# Usage\ninput_paths, output_paths = setup_rde_pipeline(\n    Path(\"project/input\"),\n    Path(\"project/output\")\n)\n\nprint(f\"Input data: {input_paths.inputdata}\")\nprint(f\"Output structure: {output_paths.struct}\")\nprint(f\"Default CSV: {input_paths.default_csv}\")\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#file-organization-and-processing","title":"File Organization and Processing","text":"<pre><code>from rdetoolkit.models.rde2types import (\n    ZipFilesPathList, ExcelInvoicePathList, OtherFilesPathList,\n    InputFilesGroup, RawFiles, PathTuple\n)\nfrom pathlib import Path\n\ndef organize_input_files(input_dir: Path) -&gt; InputFilesGroup:\n    \"\"\"Organize input files by type.\"\"\"\n\n    all_files = list(input_dir.rglob(\"*\"))\n\n    # Categorize files\n    zip_files: ZipFilesPathList = [f for f in all_files if f.suffix == \".zip\"]\n    excel_files: ExcelInvoicePathList = [f for f in all_files if f.suffix in [\".xlsx\", \".xls\"]]\n    other_files: OtherFilesPathList = [f for f in all_files if f.suffix not in [\".zip\", \".xlsx\", \".xls\"]]\n\n    return (zip_files, excel_files, other_files)\n\ndef process_raw_files(input_group: InputFilesGroup) -&gt; RawFiles:\n    \"\"\"Process input files into raw file groups.\"\"\"\n\n    zip_files, excel_files, other_files = input_group\n\n    raw_groups = []\n\n    # Group ZIP files with related files\n    for zip_file in zip_files:\n        related_files = [f for f in other_files if f.stem == zip_file.stem]\n        file_group: PathTuple = (zip_file, *related_files)\n        raw_groups.append(file_group)\n\n    # Handle standalone Excel files\n    for excel_file in excel_files:\n        file_group: PathTuple = (excel_file,)\n        raw_groups.append(file_group)\n\n    return raw_groups\n\n# Usage example\ninput_dir = Path(\"input/experiment_data\")\nfile_groups = organize_input_files(input_dir)\nraw_files = process_raw_files(file_groups)\n\nprint(f\"Found {len(file_groups[0])} ZIP files\")\nprint(f\"Found {len(file_groups[1])} Excel files\")\nprint(f\"Created {len(raw_files)} raw file groups\")\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#metadata-handling","title":"Metadata Handling","text":"<pre><code>from rdetoolkit.models.rde2types import (\n    MetaType, RepeatedMetaType, MetaItem, MetadataDefJson,\n    Name, Schema, ValueUnitPair\n)\n\ndef create_measurement_metadata() -&gt; MetaType:\n    \"\"\"Create measurement metadata.\"\"\"\n\n    metadata: MetaType = {\n        \"experiment_id\": \"EXP_2025_001\",\n        \"temperature\": 25.5,\n        \"pressure\": 1013.25,\n        \"humidity\": 60,\n        \"measurements\": [1.1, 2.2, 3.3, 4.4],\n        \"is_calibrated\": True,\n        \"sample_count\": 100\n    }\n\n    return metadata\n\ndef create_repeated_metadata() -&gt; RepeatedMetaType:\n    \"\"\"Create metadata with repeated measurements.\"\"\"\n\n    repeated_meta: RepeatedMetaType = {\n        \"temperatures\": [20.0, 21.5, 23.0, 24.5, 26.0],\n        \"pressures\": [1010.0, 1011.5, 1013.0, 1014.5, 1016.0],\n        \"humidity_values\": [58, 59, 60, 61, 62]\n    }\n\n    return repeated_meta\n\ndef create_metadata_definition() -&gt; MetadataDefJson:\n    \"\"\"Create a complete metadata definition.\"\"\"\n\n    metadata_def: MetadataDefJson = {\n        \"name\": {\n            \"ja\": \"\u6e29\u5ea6\u6e2c\u5b9a\",\n            \"en\": \"Temperature Measurement\"\n        },\n        \"schema\": {\n            \"type\": \"number\",\n            \"format\": \"float\"\n        },\n        \"unit\": \"\u00b0C\",\n        \"description\": \"Sample temperature during measurement\",\n        \"uri\": \"http://example.com/schema/temperature\",\n        \"originalName\": \"temp_sensor_reading\",\n        \"originalType\": \"float64\",\n        \"mode\": \"continuous\",\n        \"order\": \"chronological\",\n        \"valiable\": 1,\n        \"_feature\": True,\n        \"action\": \"measure\"\n    }\n\n    return metadata_def\n\n# Usage\nmeasurement_meta = create_measurement_metadata()\nrepeated_meta = create_repeated_metadata()\nmeta_def = create_metadata_definition()\n\n# Create value-unit pairs from metadata\nvalue_units = [\n    ValueUnitPair(str(measurement_meta[\"temperature\"]), \"\u00b0C\"),\n    ValueUnitPair(str(measurement_meta[\"pressure\"]), \"hPa\"),\n    ValueUnitPair(str(measurement_meta[\"humidity\"]), \"%RH\")\n]\n\nfor vu in value_units:\n    print(f\"{vu.value} {vu.unit}\")\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#configuration-and-path-management","title":"Configuration and Path Management","text":"<pre><code>from rdetoolkit.models.rde2types import (\n    RdeInputDirPaths, RdeOutputResourcePath, create_default_config,\n    RdeFsPath\n)\nfrom pathlib import Path\n\nclass RdeProcessor:\n    \"\"\"Example RDE processor using type-safe paths.\"\"\"\n\n    def __init__(self, input_base: RdeFsPath, output_base: RdeFsPath):\n        self.input_base = Path(input_base)\n        self.output_base = Path(output_base)\n\n        # Initialize paths\n        self.input_paths = RdeInputDirPaths(\n            inputdata=self.input_base / \"data\",\n            invoice=self.input_base / \"invoices\",\n            tasksupport=self.input_base / \"support\",\n            config=create_default_config()\n        )\n\n        self.output_paths = None\n\n    def setup_output_structure(self, raw_files: tuple[Path, ...]):\n        \"\"\"Set up output directory structure.\"\"\"\n\n        self.output_paths = RdeOutputResourcePath(\n            raw=self.output_base / \"raw\",\n            nonshared_raw=self.output_base / \"nonshared\",\n            rawfiles=raw_files,\n            struct=self.output_base / \"structured\",\n            main_image=self.output_base / \"images\" / \"main\",\n            other_image=self.output_base / \"images\" / \"other\",\n            meta=self.output_base / \"metadata\",\n            thumbnail=self.output_base / \"thumbnails\",\n            logs=self.output_base / \"logs\",\n            invoice=self.output_base / \"invoices\",\n            invoice_schema_json=self.output_base / \"schema\" / \"invoice.schema.json\",\n            invoice_org=self.output_base / \"backup\" / \"invoice.json\",\n            temp=self.output_base / \"temp\",\n            invoice_patch=self.output_base / \"patches\",\n            attachment=self.output_base / \"attachments\"\n        )\n\n        # Create directories\n        self._create_directories()\n\n    def _create_directories(self):\n        \"\"\"Create all necessary output directories.\"\"\"\n\n        if not self.output_paths:\n            return\n\n        directories = [\n            self.output_paths.raw,\n            self.output_paths.nonshared_raw,\n            self.output_paths.struct,\n            self.output_paths.main_image,\n            self.output_paths.other_image,\n            self.output_paths.meta,\n            self.output_paths.thumbnail,\n            self.output_paths.logs,\n            self.output_paths.invoice,\n            self.output_paths.invoice_schema_json.parent,\n            self.output_paths.invoice_org.parent,\n        ]\n\n        # Add optional directories if they exist\n        if self.output_paths.temp:\n            directories.append(self.output_paths.temp)\n        if self.output_paths.invoice_patch:\n            directories.append(self.output_paths.invoice_patch)\n        if self.output_paths.attachment:\n            directories.append(self.output_paths.attachment)\n\n        for directory in directories:\n            directory.mkdir(parents=True, exist_ok=True)\n\n    def get_config(self):\n        \"\"\"Get the current configuration.\"\"\"\n        return self.input_paths.config\n\n# Usage\nprocessor = RdeProcessor(\"project/input\", \"project/output\")\n\n# Setup with sample raw files\nraw_files = (\n    Path(\"data1.txt\"),\n    Path(\"data2.txt\"),\n    Path(\"image1.png\")\n)\n\nprocessor.setup_output_structure(raw_files)\n\nconfig = processor.get_config()\nprint(f\"Save raw: {config.system.save_raw}\")\nprint(f\"Extended mode: {config.system.extended_mode}\")\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#type-safety-best-practices","title":"Type Safety Best Practices","text":"<ol> <li>Use Type Hints: Always use the provided type aliases for better code clarity:</li> </ol> <pre><code>from rdetoolkit.models.rde2types import ZipFilesPathList, MetaType\n\ndef process_zip_files(files: ZipFilesPathList) -&gt; MetaType:\n    return {\"file_count\": len(files)}\n</code></pre> <ol> <li>Path Type Consistency: Use <code>RdeFsPath</code> for functions that accept both strings and Path objects:</li> </ol> <pre><code>from rdetoolkit.models.rde2types import RdeFsPath\nfrom pathlib import Path\n\ndef normalize_path(path: RdeFsPath) -&gt; Path:\n    return Path(path)\n</code></pre> <ol> <li>TypedDict Validation: Use TypedDict definitions for structured data:</li> </ol> <pre><code>from rdetoolkit.models.rde2types import Name, MetadataDefJson\n\ndef validate_name(name: Name) -&gt; bool:\n    return \"ja\" in name and \"en\" in name\n</code></pre> <ol> <li>Configuration Consistency: Always use the default configuration factory:</li> </ol> <pre><code>from rdetoolkit.models.rde2types import create_default_config\n\n# Preferred\nconfig = create_default_config()\n\n# Modify as needed\nconfig.system.save_raw = False\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#migration-from-legacy-features","title":"Migration from Legacy Features","text":"<p>If you're using deprecated features like <code>RdeFormatFlags</code>, migrate to the new configuration system:</p> <pre><code># Old (deprecated)\nfrom rdetoolkit.models.rde2types import RdeFormatFlags\nflags = RdeFormatFlags()  # Emits deprecation warning\n\n# New (recommended)\nfrom rdetoolkit.models.rde2types import create_default_config\nconfig = create_default_config()\n# Configure as needed through the Config object\n</code></pre>"},{"location":"rdetoolkit/models/rde2types/#see-also","title":"See Also","text":"<ul> <li>Config Module - For detailed configuration management</li> <li>Core Module - For directory operations and file handling</li> <li>Metadata Module - For metadata validation and processing</li> <li>Python Typing Documentation - For advanced type hints</li> </ul>"},{"location":"rdetoolkit/models/report/","title":"Reports Module","text":"<p>The <code>rdetoolkit.models.reports</code> module provides Pydantic models for generating and managing security scan reports and code analysis results. This module implements structured data models for documenting security vulnerabilities, external request activities, and code quality assessments in RDE systems.</p>"},{"location":"rdetoolkit/models/report/#overview","title":"Overview","text":"<p>The reports module implements a comprehensive reporting system with the following capabilities:</p> <ul> <li>Security Scan Reports: Structured reporting of code security vulnerabilities</li> <li>External Request Tracking: Documentation of external API calls and network requests</li> <li>Code Quality Analysis: Integration with code scanning and analysis tools</li> <li>Standardized Documentation: Consistent format for audit trails and compliance reporting</li> <li>Flexible Description Fields: Optional detailed descriptions for findings</li> </ul>"},{"location":"rdetoolkit/models/report/#core-classes","title":"Core Classes","text":""},{"location":"rdetoolkit/models/report/#codesnippet","title":"CodeSnippet","text":"<p>Model for representing code snippets found during security or quality scans.</p>"},{"location":"rdetoolkit/models/report/#codesnippet-constructor","title":"CodeSnippet Constructor","text":"<pre><code>CodeSnippet(\n    file_path: str,\n    snippet: str,\n    description: str | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code> (str): Path to the file containing the code snippet</li> <li><code>snippet</code> (str): The actual code snippet that was identified</li> <li><code>description</code> (str | None): Optional description explaining the significance of the snippet</li> </ul>"},{"location":"rdetoolkit/models/report/#codesnippet-example","title":"CodeSnippet Example","text":"<pre><code>from rdetoolkit.models.reports import CodeSnippet\n\n# Security vulnerability example\nsecurity_issue = CodeSnippet(\n    file_path=\"src/auth/login.py\",\n    snippet=\"password = request.form['password']  # Plain text password\",\n    description=\"Password handled in plain text without encryption\"\n)\n\n# External request example\nexternal_call = CodeSnippet(\n    file_path=\"src/api/data_fetcher.py\",\n    snippet=\"response = requests.get('https://api.external.com/data')\",\n    description=\"External API call to third-party service\"\n)\n\n# Code quality issue\nquality_issue = CodeSnippet(\n    file_path=\"src/utils/helpers.py\",\n    snippet=\"def calculate(x, y): return x/y\",\n    description=\"Division by zero not handled\"\n)\n\nprint(security_issue.file_path)    # src/auth/login.py\nprint(external_call.snippet)      # response = requests.get('https://api.external.com/data')\nprint(quality_issue.description)  # Division by zero not handled\n</code></pre>"},{"location":"rdetoolkit/models/report/#reportitem","title":"ReportItem","text":"<p>Comprehensive model for security and code analysis reports.</p>"},{"location":"rdetoolkit/models/report/#constructor","title":"Constructor","text":"<pre><code>ReportItem(\n    exec_date: str,\n    dockerfile_path: str,\n    requirements_path: str,\n    include_dirs: list[str],\n    code_security_scan_results: list[CodeSnippet],\n    code_ext_requests_scan_results: list[CodeSnippet]\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>exec_date</code> (str): Date when the scan was executed (ISO format recommended)</li> <li><code>dockerfile_path</code> (str): Path to the Dockerfile used for the environment</li> <li><code>requirements_path</code> (str): Path to the requirements file (e.g., requirements.txt)</li> <li><code>include_dirs</code> (list[str]): List of directories included in the scan</li> <li><code>code_security_scan_results</code> (list[CodeSnippet]): Security vulnerability findings</li> <li><code>code_ext_requests_scan_results</code> (list[CodeSnippet]): External request findings</li> </ul>"},{"location":"rdetoolkit/models/report/#example","title":"Example","text":"<pre><code>from rdetoolkit.models.reports import ReportItem, CodeSnippet\nfrom datetime import datetime\n\n# Create security findings\nsecurity_findings = [\n    CodeSnippet(\n        file_path=\"src/auth/session.py\",\n        snippet=\"session['user_id'] = user.id\",\n        description=\"Session data stored without encryption\"\n    ),\n    CodeSnippet(\n        file_path=\"src/database/connection.py\",\n        snippet=\"cursor.execute(f'SELECT * FROM users WHERE id = {user_id}')\",\n        description=\"SQL injection vulnerability - unparameterized query\"\n    )\n]\n\n# Create external request findings\nexternal_requests = [\n    CodeSnippet(\n        file_path=\"src/services/weather.py\",\n        snippet=\"requests.get('http://api.openweathermap.org/data/2.5/weather')\",\n        description=\"External weather API call\"\n    ),\n    CodeSnippet(\n        file_path=\"src/integrations/analytics.py\",\n        snippet=\"requests.post('https://analytics.service.com/track')\",\n        description=\"Analytics tracking service call\"\n    )\n]\n\n# Create comprehensive report\nreport = ReportItem(\n    exec_date=\"2025-01-15T10:30:00Z\",\n    dockerfile_path=\"docker/Dockerfile.prod\",\n    requirements_path=\"requirements/production.txt\",\n    include_dirs=[\"src/\", \"tests/\", \"scripts/\"],\n    code_security_scan_results=security_findings,\n    code_ext_requests_scan_results=external_requests\n)\n\n# Access report data\nprint(f\"Scan executed on: {report.exec_date}\")\nprint(f\"Found {len(report.code_security_scan_results)} security issues\")\nprint(f\"Found {len(report.code_ext_requests_scan_results)} external requests\")\nprint(f\"Scanned directories: {', '.join(report.include_dirs)}\")\n</code></pre>"},{"location":"rdetoolkit/models/report/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/models/report/#security-scan-report-generation","title":"Security Scan Report Generation","text":"<pre><code>from rdetoolkit.models.reports import ReportItem, CodeSnippet\nfrom datetime import datetime\nfrom pathlib import Path\nimport json\n\nclass SecurityScanner:\n    \"\"\"Example security scanner that generates reports.\"\"\"\n\n    def __init__(self, project_root: Path):\n        self.project_root = project_root\n        self.security_patterns = [\n            (\"password\", \"Potential password in plain text\"),\n            (\"SECRET\", \"Hardcoded secret detected\"),\n            (\"eval(\", \"Dangerous eval() usage\"),\n            (\"exec(\", \"Dangerous exec() usage\"),\n            (\"subprocess.call\", \"Subprocess call - review for command injection\")\n        ]\n        self.external_patterns = [\n            (\"requests.get\", \"HTTP GET request\"),\n            (\"requests.post\", \"HTTP POST request\"),\n            (\"urllib.request\", \"urllib request\"),\n            (\"httpx.\", \"HTTPX client request\"),\n            (\"aiohttp.\", \"Aiohttp request\")\n        ]\n\n    def scan_file(self, file_path: Path) -&gt; tuple[list[CodeSnippet], list[CodeSnippet]]:\n        \"\"\"Scan a single file for security issues and external requests.\"\"\"\n\n        security_findings = []\n        external_findings = []\n\n        try:\n            content = file_path.read_text(encoding='utf-8')\n            lines = content.splitlines()\n\n            for line_num, line in enumerate(lines, 1):\n                line_stripped = line.strip()\n\n                # Check for security patterns\n                for pattern, description in self.security_patterns:\n                    if pattern in line_stripped:\n                        security_findings.append(CodeSnippet(\n                            file_path=str(file_path.relative_to(self.project_root)),\n                            snippet=f\"Line {line_num}: {line_stripped}\",\n                            description=f\"{description} (Line {line_num})\"\n                        ))\n\n                # Check for external request patterns\n                for pattern, description in self.external_patterns:\n                    if pattern in line_stripped:\n                        external_findings.append(CodeSnippet(\n                            file_path=str(file_path.relative_to(self.project_root)),\n                            snippet=f\"Line {line_num}: {line_stripped}\",\n                            description=f\"{description} (Line {line_num})\"\n                        ))\n\n        except Exception as e:\n            # Handle encoding or other errors\n            print(f\"Error scanning {file_path}: {e}\")\n\n        return security_findings, external_findings\n\n    def scan_directory(self, scan_dirs: list[str]) -&gt; ReportItem:\n        \"\"\"Scan multiple directories and generate a report.\"\"\"\n\n        all_security_findings = []\n        all_external_findings = []\n\n        for scan_dir in scan_dirs:\n            dir_path = self.project_root / scan_dir\n            if not dir_path.exists():\n                continue\n\n            # Scan Python files\n            for py_file in dir_path.rglob(\"*.py\"):\n                security, external = self.scan_file(py_file)\n                all_security_findings.extend(security)\n                all_external_findings.extend(external)\n\n        # Create report\n        report = ReportItem(\n            exec_date=datetime.now().isoformat(),\n            dockerfile_path=\"Dockerfile\",\n            requirements_path=\"requirements.txt\",\n            include_dirs=scan_dirs,\n            code_security_scan_results=all_security_findings,\n            code_ext_requests_scan_results=all_external_findings\n        )\n\n        return report\n\n# Usage example\nscanner = SecurityScanner(Path(\"project_root\"))\nreport = scanner.scan_directory([\"src/\", \"tests/\", \"scripts/\"])\n\nprint(f\"Security scan completed at {report.exec_date}\")\nprint(f\"Scanned directories: {report.include_dirs}\")\nprint(f\"Security issues found: {len(report.code_security_scan_results)}\")\nprint(f\"External requests found: {len(report.code_ext_requests_scan_results)}\")\n</code></pre>"},{"location":"rdetoolkit/models/report/#report-serialization-and-storage","title":"Report Serialization and Storage","text":"<pre><code>from rdetoolkit.models.reports import ReportItem, CodeSnippet\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\ndef save_report_to_json(report: ReportItem, output_path: Path) -&gt; None:\n    \"\"\"Save a report to a JSON file.\"\"\"\n\n    # Serialize to dictionary\n    report_dict = report.model_dump()\n\n    # Save to file\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(report_dict, f, indent=2, ensure_ascii=False)\n\n    print(f\"Report saved to {output_path}\")\n\ndef load_report_from_json(file_path: Path) -&gt; ReportItem:\n    \"\"\"Load a report from a JSON file.\"\"\"\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        report_dict = json.load(f)\n\n    # Reconstruct the report\n    return ReportItem(**report_dict)\n\ndef generate_html_report(report: ReportItem, output_path: Path) -&gt; None:\n    \"\"\"Generate an HTML report from a ReportItem.\"\"\"\n\n    html_template = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Security Scan Report&lt;/title&gt;\n        &lt;style&gt;\n            body { font-family: Arial, sans-serif; margin: 20px; }\n            .header { background-color: #f0f0f0; padding: 10px; border-radius: 5px; }\n            .section { margin: 20px 0; }\n            .finding { background-color: #fff3cd; padding: 10px; margin: 5px 0; border-left: 4px solid #ffc107; }\n            .external { background-color: #d1ecf1; padding: 10px; margin: 5px 0; border-left: 4px solid #17a2b8; }\n            .code { background-color: #f8f9fa; padding: 5px; font-family: monospace; border-radius: 3px; }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"header\"&gt;\n            &lt;h1&gt;Security Scan Report&lt;/h1&gt;\n            &lt;p&gt;&lt;strong&gt;Execution Date:&lt;/strong&gt; {exec_date}&lt;/p&gt;\n            &lt;p&gt;&lt;strong&gt;Dockerfile:&lt;/strong&gt; {dockerfile_path}&lt;/p&gt;\n            &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; {requirements_path}&lt;/p&gt;\n            &lt;p&gt;&lt;strong&gt;Scanned Directories:&lt;/strong&gt; {include_dirs}&lt;/p&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"section\"&gt;\n            &lt;h2&gt;Security Issues ({security_count})&lt;/h2&gt;\n            {security_findings}\n        &lt;/div&gt;\n\n        &lt;div class=\"section\"&gt;\n            &lt;h2&gt;External Requests ({external_count})&lt;/h2&gt;\n            {external_findings}\n        &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    # Generate security findings HTML\n    security_html = \"\"\n    for finding in report.code_security_scan_results:\n        security_html += f\"\"\"\n        &lt;div class=\"finding\"&gt;\n            &lt;strong&gt;File:&lt;/strong&gt; {finding.file_path}&lt;br&gt;\n            &lt;strong&gt;Code:&lt;/strong&gt; &lt;code class=\"code\"&gt;{finding.snippet}&lt;/code&gt;&lt;br&gt;\n            &lt;strong&gt;Description:&lt;/strong&gt; {finding.description or 'No description'}\n        &lt;/div&gt;\n        \"\"\"\n\n    # Generate external request findings HTML\n    external_html = \"\"\n    for finding in report.code_ext_requests_scan_results:\n        external_html += f\"\"\"\n        &lt;div class=\"external\"&gt;\n            &lt;strong&gt;File:&lt;/strong&gt; {finding.file_path}&lt;br&gt;\n            &lt;strong&gt;Code:&lt;/strong&gt; &lt;code class=\"code\"&gt;{finding.snippet}&lt;/code&gt;&lt;br&gt;\n            &lt;strong&gt;Description:&lt;/strong&gt; {finding.description or 'No description'}\n        &lt;/div&gt;\n        \"\"\"\n\n    # Fill template\n    html_content = html_template.format(\n        exec_date=report.exec_date,\n        dockerfile_path=report.dockerfile_path,\n        requirements_path=report.requirements_path,\n        include_dirs=\", \".join(report.include_dirs),\n        security_count=len(report.code_security_scan_results),\n        external_count=len(report.code_ext_requests_scan_results),\n        security_findings=security_html,\n        external_findings=external_html\n    )\n\n    # Save HTML file\n    with open(output_path, 'w', encoding='utf-8') as f:\n        f.write(html_content)\n\n    print(f\"HTML report saved to {output_path}\")\n\n# Example usage\nreport = ReportItem(\n    exec_date=\"2025-01-15T14:30:00Z\",\n    dockerfile_path=\"docker/Dockerfile.prod\",\n    requirements_path=\"requirements/production.txt\",\n    include_dirs=[\"src/\", \"tests/\"],\n    code_security_scan_results=[\n        CodeSnippet(\n            file_path=\"src/auth.py\",\n            snippet=\"password = 'hardcoded_secret'\",\n            description=\"Hardcoded password detected\"\n        )\n    ],\n    code_ext_requests_scan_results=[\n        CodeSnippet(\n            file_path=\"src/api.py\",\n            snippet=\"requests.get('https://api.example.com')\",\n            description=\"External API call\"\n        )\n    ]\n)\n\n# Save in different formats\nsave_report_to_json(report, Path(\"security_report.json\"))\ngenerate_html_report(report, Path(\"security_report.html\"))\n\n# Load and verify\nloaded_report = load_report_from_json(Path(\"security_report.json\"))\nassert loaded_report.exec_date == report.exec_date\nprint(\"Report successfully saved and loaded!\")\n</code></pre>"},{"location":"rdetoolkit/models/report/#integration-with-cicd-pipeline","title":"Integration with CI/CD Pipeline","text":"<pre><code>from rdetoolkit.models.reports import ReportItem, CodeSnippet\nimport subprocess\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass CICDSecurityIntegration:\n    \"\"\"Integration class for CI/CD security scanning.\"\"\"\n\n    def __init__(self, project_root: Path):\n        self.project_root = project_root\n        self.report_dir = project_root / \"security_reports\"\n        self.report_dir.mkdir(exist_ok=True)\n\n    def run_bandit_scan(self, scan_dirs: list[str]) -&gt; list[CodeSnippet]:\n        \"\"\"Run Bandit security scanner and parse results.\"\"\"\n\n        findings = []\n\n        try:\n            # Run bandit with JSON output\n            cmd = [\"bandit\", \"-r\", \"-f\", \"json\"] + scan_dirs\n            result = subprocess.run(cmd, capture_output=True, text=True, cwd=self.project_root)\n\n            if result.returncode == 0 or result.stdout:\n                bandit_data = json.loads(result.stdout)\n\n                for issue in bandit_data.get(\"results\", []):\n                    findings.append(CodeSnippet(\n                        file_path=issue[\"filename\"],\n                        snippet=issue[\"code\"],\n                        description=f\"{issue['test_name']}: {issue['issue_text']}\"\n                    ))\n\n        except Exception as e:\n            print(f\"Error running Bandit: {e}\")\n\n        return findings\n\n    def scan_for_external_requests(self, scan_dirs: list[str]) -&gt; list[CodeSnippet]:\n        \"\"\"Scan for external HTTP requests in code.\"\"\"\n\n        findings = []\n        request_patterns = [\n            \"requests.\",\n            \"urllib.request\",\n            \"httpx.\",\n            \"aiohttp.\",\n            \"fetch(\",\n            \"axios.\"\n        ]\n\n        for scan_dir in scan_dirs:\n            dir_path = self.project_root / scan_dir\n            if not dir_path.exists():\n                continue\n\n            for file_path in dir_path.rglob(\"*\"):\n                if file_path.suffix in [\".py\", \".js\", \".ts\"]:\n                    try:\n                        content = file_path.read_text(encoding='utf-8')\n                        lines = content.splitlines()\n\n                        for line_num, line in enumerate(lines, 1):\n                            for pattern in request_patterns:\n                                if pattern in line:\n                                    findings.append(CodeSnippet(\n                                        file_path=str(file_path.relative_to(self.project_root)),\n                                        snippet=line.strip(),\n                                        description=f\"External request detected at line {line_num}\"\n                                    ))\n                    except Exception as e:\n                        print(f\"Error scanning {file_path}: {e}\")\n\n        return findings\n\n    def generate_comprehensive_report(self, scan_dirs: list[str]) -&gt; ReportItem:\n        \"\"\"Generate comprehensive security and external request report.\"\"\"\n\n        # Run security scan\n        security_findings = self.run_bandit_scan(scan_dirs)\n\n        # Scan for external requests\n        external_findings = self.scan_for_external_requests(scan_dirs)\n\n        # Create report\n        report = ReportItem(\n            exec_date=datetime.now().isoformat(),\n            dockerfile_path=str(self.project_root / \"Dockerfile\"),\n            requirements_path=str(self.project_root / \"requirements.txt\"),\n            include_dirs=scan_dirs,\n            code_security_scan_results=security_findings,\n            code_ext_requests_scan_results=external_findings\n        )\n\n        return report\n\n    def save_report_with_timestamp(self, report: ReportItem) -&gt; Path:\n        \"\"\"Save report with timestamp in filename.\"\"\"\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        report_path = self.report_dir / f\"security_report_{timestamp}.json\"\n\n        with open(report_path, 'w', encoding='utf-8') as f:\n            f.write(report.model_dump_json(indent=2))\n\n        return report_path\n\n    def check_security_thresholds(self, report: ReportItem) -&gt; bool:\n        \"\"\"Check if security findings exceed acceptable thresholds.\"\"\"\n\n        # Define thresholds\n        max_high_severity = 0\n        max_medium_severity = 5\n        max_total_findings = 10\n\n        # Count severity levels (simplified - in real scenario, parse from descriptions)\n        high_severity = sum(1 for finding in report.code_security_scan_results\n                          if \"high\" in finding.description.lower())\n        medium_severity = sum(1 for finding in report.code_security_scan_results\n                            if \"medium\" in finding.description.lower())\n        total_findings = len(report.code_security_scan_results)\n\n        # Check thresholds\n        if high_severity &gt; max_high_severity:\n            print(f\"\u274c High severity issues: {high_severity} (max: {max_high_severity})\")\n            return False\n\n        if medium_severity &gt; max_medium_severity:\n            print(f\"\u274c Medium severity issues: {medium_severity} (max: {max_medium_severity})\")\n            return False\n\n        if total_findings &gt; max_total_findings:\n            print(f\"\u274c Total security issues: {total_findings} (max: {max_total_findings})\")\n            return False\n\n        print(\"\u2705 Security scan passed all thresholds\")\n        return True\n\n# Usage in CI/CD\ndef ci_security_check():\n    \"\"\"Main function for CI/CD security checking.\"\"\"\n\n    scanner = CICDSecurityIntegration(Path(\".\"))\n\n    # Generate report\n    report = scanner.generate_comprehensive_report([\"src/\", \"tests/\"])\n\n    # Save report\n    report_path = scanner.save_report_with_timestamp(report)\n    print(f\"Report saved: {report_path}\")\n\n    # Check thresholds\n    passed = scanner.check_security_thresholds(report)\n\n    # Print summary\n    print(f\"\"\"\nSecurity Scan Summary:\n- Execution Date: {report.exec_date}\n- Security Issues: {len(report.code_security_scan_results)}\n- External Requests: {len(report.code_ext_requests_scan_results)}\n- Directories Scanned: {', '.join(report.include_dirs)}\n- Threshold Check: {'PASSED' if passed else 'FAILED'}\n    \"\"\")\n\n    # Exit with appropriate code for CI/CD\n    return 0 if passed else 1\n\n# Example usage\nif __name__ == \"__main__\":\n    exit_code = ci_security_check()\n    exit(exit_code)\n</code></pre>"},{"location":"rdetoolkit/models/report/#report-analysis-and-aggregation","title":"Report Analysis and Aggregation","text":"<pre><code>from rdetoolkit.models.reports import ReportItem, CodeSnippet\nfrom pathlib import Path\nimport json\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\n\nclass ReportAnalyzer:\n    \"\"\"Analyzer for aggregating and analyzing multiple security reports.\"\"\"\n\n    def __init__(self, reports_dir: Path):\n        self.reports_dir = reports_dir\n\n    def load_all_reports(self, days_back: int = 30) -&gt; list[ReportItem]:\n        \"\"\"Load all reports from the last N days.\"\"\"\n\n        cutoff_date = datetime.now() - timedelta(days=days_back)\n        reports = []\n\n        for report_file in self.reports_dir.glob(\"*.json\"):\n            try:\n                with open(report_file, 'r', encoding='utf-8') as f:\n                    report_data = json.load(f)\n\n                report = ReportItem(**report_data)\n                report_date = datetime.fromisoformat(report.exec_date.replace('Z', '+00:00'))\n\n                if report_date &gt;= cutoff_date:\n                    reports.append(report)\n\n            except Exception as e:\n                print(f\"Error loading report {report_file}: {e}\")\n\n        return reports\n\n    def analyze_security_trends(self, reports: list[ReportItem]) -&gt; dict:\n        \"\"\"Analyze security vulnerability trends over time.\"\"\"\n\n        trends = {\n            \"total_scans\": len(reports),\n            \"average_security_issues\": 0,\n            \"average_external_requests\": 0,\n            \"most_common_security_patterns\": defaultdict(int),\n            \"most_affected_files\": defaultdict(int),\n            \"severity_distribution\": defaultdict(int)\n        }\n\n        if not reports:\n            return trends\n\n        total_security = 0\n        total_external = 0\n\n        for report in reports:\n            total_security += len(report.code_security_scan_results)\n            total_external += len(report.code_ext_requests_scan_results)\n\n            # Analyze security patterns\n            for finding in report.code_security_scan_results:\n                if finding.description:\n                    # Extract pattern (simplified)\n                    pattern = finding.description.split(':')[0] if ':' in finding.description else finding.description\n                    trends[\"most_common_security_patterns\"][pattern] += 1\n\n                # Track affected files\n                trends[\"most_affected_files\"][finding.file_path] += 1\n\n                # Analyze severity (simplified)\n                if finding.description:\n                    desc_lower = finding.description.lower()\n                    if \"high\" in desc_lower:\n                        trends[\"severity_distribution\"][\"high\"] += 1\n                    elif \"medium\" in desc_lower:\n                        trends[\"severity_distribution\"][\"medium\"] += 1\n                    elif \"low\" in desc_lower:\n                        trends[\"severity_distribution\"][\"low\"] += 1\n                    else:\n                        trends[\"severity_distribution\"][\"unknown\"] += 1\n\n        trends[\"average_security_issues\"] = total_security / len(reports)\n        trends[\"average_external_requests\"] = total_external / len(reports)\n\n        return trends\n\n    def generate_executive_summary(self, trends: dict) -&gt; str:\n        \"\"\"Generate executive summary from trends analysis.\"\"\"\n\n        summary = f\"\"\"\nSECURITY ANALYSIS EXECUTIVE SUMMARY\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nOVERVIEW:\n- Total scans analyzed: {trends['total_scans']}\n- Average security issues per scan: {trends['average_security_issues']:.1f}\n- Average external requests per scan: {trends['average_external_requests']:.1f}\n\nSEVERITY DISTRIBUTION:\n\"\"\"\n\n        for severity, count in trends['severity_distribution'].items():\n            summary += f\"- {severity.title()}: {count} issues\\n\"\n\n        summary += \"\\nMOST COMMON SECURITY PATTERNS:\\n\"\n        for pattern, count in sorted(trends['most_common_security_patterns'].items(),\n                                   key=lambda x: x[1], reverse=True)[:5]:\n            summary += f\"- {pattern}: {count} occurrences\\n\"\n\n        summary += \"\\nMOST AFFECTED FILES:\\n\"\n        for file_path, count in sorted(trends['most_affected_files'].items(),\n                                     key=lambda x: x[1], reverse=True)[:5]:\n            summary += f\"- {file_path}: {count} issues\\n\"\n\n        return summary\n\n# Usage example\nanalyzer = ReportAnalyzer(Path(\"security_reports\"))\nreports = analyzer.load_all_reports(days_back=30)\ntrends = analyzer.analyze_security_trends(reports)\nsummary = analyzer.generate_executive_summary(trends)\n\nprint(summary)\n\n# Save summary to file\nwith open(\"security_summary.txt\", \"w\") as f:\n    f.write(summary)\n</code></pre>"},{"location":"rdetoolkit/models/report/#error-handling-and-validation","title":"Error Handling and Validation","text":""},{"location":"rdetoolkit/models/report/#input-validation","title":"Input Validation","text":"<pre><code>from rdetoolkit.models.reports import ReportItem, CodeSnippet\nfrom pydantic import ValidationError\nfrom datetime import datetime\n\ndef create_safe_code_snippet(file_path: str, snippet: str, description: str = None) -&gt; CodeSnippet | None:\n    \"\"\"Safely create a CodeSnippet with validation.\"\"\"\n\n    try:\n        return CodeSnippet(\n            file_path=file_path,\n            snippet=snippet,\n            description=description\n        )\n    except ValidationError as e:\n        print(f\"Invalid CodeSnippet data: {e}\")\n        return None\n\ndef create_safe_report(\n    exec_date: str,\n    dockerfile_path: str,\n    requirements_path: str,\n    include_dirs: list[str],\n    security_findings: list[CodeSnippet],\n    external_findings: list[CodeSnippet]\n) -&gt; ReportItem | None:\n    \"\"\"Safely create a ReportItem with validation.\"\"\"\n\n    try:\n        return ReportItem(\n            exec_date=exec_date,\n            dockerfile_path=dockerfile_path,\n            requirements_path=requirements_path,\n            include_dirs=include_dirs,\n            code_security_scan_results=security_findings,\n            code_ext_requests_scan_results=external_findings\n        )\n    except ValidationError as e:\n        print(f\"Invalid ReportItem data: {e}\")\n        return None\n\n# Example with validation\nvalid_snippet = create_safe_code_snippet(\n    \"src/main.py\",\n    \"print('Hello World')\",\n    \"Simple print statement\"\n)\n\ninvalid_snippet = create_safe_code_snippet(\n    None,  # Invalid: missing file_path\n    \"code\",\n    \"description\"\n)\n\nprint(f\"Valid snippet created: {valid_snippet is not None}\")    # True\nprint(f\"Invalid snippet created: {invalid_snippet is not None}\") # False\n</code></pre>"},{"location":"rdetoolkit/models/report/#best-practices","title":"Best Practices","text":"<ol> <li>Use ISO Date Formats: Always use ISO 8601 format for dates:</li> </ol> <pre><code>exec_date = datetime.now().isoformat()  # \"2025-01-15T10:30:00.123456\"\n</code></pre> <ol> <li>Provide Meaningful Descriptions: Include context in code snippet descriptions:</li> </ol> <pre><code>CodeSnippet(\n    file_path=\"auth.py\",\n    snippet=\"password = input('Enter password: ')\",\n    description=\"Password input without encryption - Line 45\"\n)\n</code></pre> <ol> <li>Use Relative Paths: Store relative paths for portability:</li> </ol> <pre><code># Good\nfile_path = str(Path(absolute_path).relative_to(project_root))\n\n# Avoi\nfile_path = \"/home/user/project/src/file.py\"\n</code></pre> <ol> <li>Validate Input Data: Always validate data before creating reports:</li> </ol> <pre><code>if not Path(dockerfile_path).exists():\n    dockerfile_path = \"Dockerfile.default\"\n</code></pre> <ol> <li>Handle Encoding Issues: Use proper encoding when reading files:</li> </ol> <pre><code>try:\n    content = file_path.read_text(encoding='utf-8')\nexcept UnicodeDecodeError:\n    content = file_path.read_text(encoding='latin-1')\n</code></pre>"},{"location":"rdetoolkit/models/report/#integration-examples","title":"Integration Examples","text":""},{"location":"rdetoolkit/models/report/#integration-with-popular-security-tools","title":"Integration with Popular Security Tools","text":"<pre><code># Integration with different security scanners\ndef integrate_with_semgrep(project_path: Path) -&gt; list[CodeSnippet]:\n    \"\"\"Integrate with Semgrep security scanner.\"\"\"\n\n    cmd = [\"semgrep\", \"--config=auto\", \"--json\", str(project_path)]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    findings = []\n    if result.returncode == 0:\n        data = json.loads(result.stdout)\n        for finding in data.get(\"results\", []):\n            findings.append(CodeSnippet(\n                file_path=finding[\"path\"],\n                snippet=finding[\"extra\"][\"lines\"],\n                description=f\"{finding['check_id']}: {finding['message']}\"\n            ))\n\n    return findings\n\ndef integrate_with_safety(requirements_path: Path) -&gt; list[CodeSnippet]:\n    \"\"\"Integrate with Safety package vulnerability scanner.\"\"\"\n\n    cmd = [\"safety\", \"check\", \"--json\", \"-r\", str(requirements_path)]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    findings = []\n    if result.stdout:\n        try:\n            data = json.loads(result.stdout)\n            for vuln in data:\n                findings.append(CodeSnippet(\n                    file_path=str(requirements_path),\n                    snippet=f\"{vuln['package']}=={vuln['installed_version']}\",\n                    description=f\"Vulnerability: {vuln['vulnerability_id']} - {vuln['advisory']}\"\n                ))\n        except json.JSONDecodeError:\n            pass\n\n    return findings\n</code></pre>"},{"location":"rdetoolkit/models/report/#see-also","title":"See Also","text":"<ul> <li>Pydantic Documentation - For model validation and serialization</li> <li>Security Scanning Tools - For integration with security scanners</li> <li>CI/CD Integration - For automated security scanning in pipelines</li> <li>JSON Schema - For report format validation</li> </ul>"},{"location":"rdetoolkit/models/result/","title":"Result Module","text":"<p>The <code>rdetoolkit.models.result</code> module provides comprehensive models and utilities for managing workflow execution results and status tracking in RDE systems. This module implements structured handling of workflow execution data, error tracking, and result aggregation with JSON serialization support.</p>"},{"location":"rdetoolkit/models/result/#overview","title":"Overview","text":"<p>The result module implements a complete workflow execution tracking system with the following capabilities:</p> <ul> <li>Status Tracking: Comprehensive tracking of workflow execution states and progress</li> <li>Error Management: Detailed error code, message, and stack trace capture</li> <li>Result Aggregation: Collection and management of multiple workflow execution results</li> <li>JSON Serialization: Built-in support for JSON export and import of execution data</li> <li>Iteration Support: Pythonic iteration and indexing over workflow results</li> </ul>"},{"location":"rdetoolkit/models/result/#core-classes","title":"Core Classes","text":""},{"location":"rdetoolkit/models/result/#workflowexecutionstatus","title":"WorkflowExecutionStatus","text":"<p>Model representing the execution status of a single workflow run.</p>"},{"location":"rdetoolkit/models/result/#workflowexecutionstatus-constructor","title":"WorkflowExecutionStatus Constructor","text":"<pre><code>WorkflowExecutionStatus(\n    run_id: str,\n    title: str,\n    status: str,\n    mode: str,\n    error_code: int | None = None,\n    error_message: str | None = None,\n    target: str | None = None,\n    stacktrace: str | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>run_id</code> (str): Unique identifier for the workflow run (automatically formatted to 4 digits)</li> <li><code>title</code> (str): Descriptive title of the workflow execution</li> <li><code>status</code> (str): Current execution status (e.g., \"success\", \"failed\", \"running\")</li> <li><code>mode</code> (str): Processing mode used for the workflow</li> <li><code>error_code</code> (int | None): Optional error code for failed executions</li> <li><code>error_message</code> (str | None): Optional error message describing failures</li> <li><code>target</code> (str | None): Optional target directory or resource path</li> <li><code>stacktrace</code> (str | None): Optional detailed stack trace for debugging</li> </ul>"},{"location":"rdetoolkit/models/result/#validation","title":"Validation","text":"<ul> <li><code>run_id</code> is automatically formatted to a 4-digit zero-padded string (e.g., \"1\" becomes \"0001\")</li> </ul>"},{"location":"rdetoolkit/models/result/#usage-example","title":"Usage Example","text":"<pre><code>from rdetoolkit.models.result import WorkflowExecutionStatus\n\n# Successful workflow execution\nsuccess_status = WorkflowExecutionStatus(\n    run_id=\"1\",\n    title=\"Data Processing Pipeline\",\n    status=\"success\",\n    mode=\"batch\",\n    target=\"/output/data/processed\"\n)\n\n# Failed workflow execution with error details\nfailed_status = WorkflowExecutionStatus(\n    run_id=\"2\",\n    title=\"Image Analysis Workflow\",\n    status=\"failed\",\n    mode=\"interactive\",\n    error_code=500,\n    error_message=\"Failed to process image data\",\n    target=\"/output/images\",\n    stacktrace=\"Traceback (most recent call last):\\n  File 'image_processor.py', line 45...\"\n)\n\n# Running workflow\nrunning_status = WorkflowExecutionStatus(\n    run_id=\"3\",\n    title=\"Large Dataset Analysis\",\n    status=\"running\",\n    mode=\"background\"\n)\n\nprint(success_status.run_id)  # \"0001\"\nprint(failed_status.error_code)  # 500\nprint(running_status.status)  # \"running\"\n</code></pre>"},{"location":"rdetoolkit/models/result/#workflowexecutionresults","title":"WorkflowExecutionResults","text":"<p>Container model for managing collections of workflow execution statuses.</p>"},{"location":"rdetoolkit/models/result/#workflowexecutionresults-constructor","title":"WorkflowExecutionResults Constructor","text":"<pre><code>WorkflowExecutionResults(statuses: list[WorkflowExecutionStatus])\n</code></pre> <p>Parameters:</p> <ul> <li><code>statuses</code> (list[WorkflowExecutionStatus]): List of workflow execution status objects</li> </ul>"},{"location":"rdetoolkit/models/result/#example","title":"Example","text":"<pre><code>from rdetoolkit.models.result import WorkflowExecutionResults, WorkflowExecutionStatus\n\n# Create multiple statuses\nstatuses = [\n    WorkflowExecutionStatus(\n        run_id=\"1\",\n        title=\"First Workflow\",\n        status=\"success\",\n        mode=\"batch\"\n    ),\n    WorkflowExecutionStatus(\n        run_id=\"2\",\n        title=\"Second Workflow\",\n        status=\"failed\",\n        mode=\"interactive\",\n        error_code=404,\n        error_message=\"Resource not found\"\n    )\n]\n\n# Create results container\nresults = WorkflowExecutionResults(statuses=statuses)\n\nprint(len(results.statuses))  # 2\nprint(results.statuses[0].title)  # \"First Workflow\"\n</code></pre>"},{"location":"rdetoolkit/models/result/#workflowresultmanager","title":"WorkflowResultManager","text":"<p>High-level manager class for workflow execution result tracking and manipulation.</p>"},{"location":"rdetoolkit/models/result/#constructor","title":"Constructor","text":"<pre><code>WorkflowResultManager()\n</code></pre> <p>Initializes an empty workflow result manager with no execution statuses.</p>"},{"location":"rdetoolkit/models/result/#methods","title":"Methods","text":""},{"location":"rdetoolkit/models/result/#addrun_id-title-status-mode-error_codenone-error_messagenone-targetnone-stacktracenone","title":"add(run_id, title, status, mode, error_code=None, error_message=None, target=None, stacktrace=None)","text":"<p>Add a new workflow execution status with individual parameters.</p> <pre><code>def add(\n    run_id: str,\n    title: str,\n    status: str,\n    mode: str,\n    error_code: int | None = None,\n    error_message: str | None = None,\n    target: str | None = None,\n    stacktrace: str | None = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>run_id</code> (str): Unique identifier for the workflow run</li> <li><code>title</code> (str): Descriptive title of the workflow</li> <li><code>status</code> (str): Execution status</li> <li><code>mode</code> (str): Processing mode</li> <li><code>error_code</code> (int | None): Optional error code</li> <li><code>error_message</code> (str | None): Optional error message</li> <li><code>target</code> (str | None): Optional target path</li> <li><code>stacktrace</code> (str | None): Optional stack trace</li> </ul> <p>Example:</p> <pre><code>from rdetoolkit.models.result import WorkflowResultManager\n\nmanager = WorkflowResultManager()\n\n# Add successful execution\nmanager.add(\n    run_id=\"1\",\n    title=\"Data Validation\",\n    status=\"success\",\n    mode=\"validation\",\n    target=\"/data/validated\"\n)\n\n# Add failed execution\nmanager.add(\n    run_id=\"2\",\n    title=\"File Processing\",\n    status=\"failed\",\n    mode=\"processing\",\n    error_code=500,\n    error_message=\"File not found: input.csv\",\n    stacktrace=\"FileNotFoundError: [Errno 2] No such file or directory: 'input.csv'\"\n)\n\nprint(len(manager))  # 2\n</code></pre>"},{"location":"rdetoolkit/models/result/#add_statusstatus","title":"add_status(status)","text":"<p>Add an existing WorkflowExecutionStatus object.</p> <pre><code>def add_status(status: WorkflowExecutionStatus) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>status</code> (WorkflowExecutionStatus): Pre-created status object to add</li> </ul> <p>Example:</p> <pre><code>manager = WorkflowResultManager()\n\n# Create status separately\nstatus = WorkflowExecutionStatus(\n    run_id=\"5\",\n    title=\"Custom Workflow\",\n    status=\"completed\",\n    mode=\"custom\"\n)\n\n# Add the status object\nmanager.add_status(status)\n\nprint(manager[0].title)  # \"Custom Workflow\"\n</code></pre>"},{"location":"rdetoolkit/models/result/#to_json","title":"to_json()","text":"<p>Export the workflow execution results as JSON string.</p> <pre><code>def to_json() -&gt; str\n</code></pre> <p>Returns:</p> <ul> <li><code>str</code>: JSON representation of all workflow execution results</li> </ul> <p>Example:</p> <pre><code>manager = WorkflowResultManager()\nmanager.add(\"1\", \"Test Workflow\", \"success\", \"test\")\n\njson_output = manager.to_json()\nprint(json_output)\n# {\n#   \"statuses\": [\n#     {\n#       \"run_id\": \"0001\",\n#       \"title\": \"Test Workflow\",\n#       \"status\": \"success\",\n#       \"mode\": \"test\",\n#       \"error_code\": null,\n#       \"error_message\": null,\n#       \"target\": null,\n#       \"stacktrace\": null\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"rdetoolkit/models/result/#magic-methods","title":"Magic Methods","text":"<p>The WorkflowResultManager supports standard Python container operations:</p>"},{"location":"rdetoolkit/models/result/#__iter__","title":"__iter__()","text":"<pre><code>def __iter__() -&gt; Iterator[WorkflowExecutionStatus]\n</code></pre> <p>Returns:</p> <ul> <li><code>Iterator[WorkflowExecutionStatus]</code>: Iterator over workflow execution statuses</li> </ul>"},{"location":"rdetoolkit/models/result/#__len__","title":"__len__()","text":"<pre><code>def __len__() -&gt; int\n</code></pre> <p>Returns:</p> <ul> <li><code>int</code>: Number of workflow execution statuses</li> </ul>"},{"location":"rdetoolkit/models/result/#__getitem__index","title":"__getitem__(index)","text":"<pre><code>def __getitem__(index: int) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters:</p> <ul> <li><code>index</code> (int): Index of the status to retrieve</li> </ul> <p>Returns:</p> <ul> <li><code>WorkflowExecutionStatus</code>: Status at the specified index</li> </ul>"},{"location":"rdetoolkit/models/result/#__repr__","title":"__repr__()","text":"<pre><code>def __repr__() -&gt; str\n</code></pre> <p>Returns:</p> <ul> <li><code>str</code>: String representation of the manager</li> </ul>"},{"location":"rdetoolkit/models/result/#example-of-magic-methods","title":"Example of Magic Methods","text":"<pre><code>manager = WorkflowResultManager()\nmanager.add(\"1\", \"First\", \"success\", \"mode1\")\nmanager.add(\"2\", \"Second\", \"failed\", \"mode2\", error_code=404)\n\n# Iteration\nfor status in manager:\n    print(f\"{status.run_id}: {status.status}\")\n# Output:\n# 0001: success\n# 0002: failed\n\n# Length\nprint(len(manager))  # 2\n\n# Indexing\nfirst_status = manager[0]\nprint(first_status.title)  # \"First\"\n\n# String representation\nprint(repr(manager))  # WorkflowResultManager(WorkflowExecutionResults(...))\n</code></pre>"},{"location":"rdetoolkit/models/result/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/models/result/#basic-workflow-tracking","title":"Basic Workflow Tracking","text":"<pre><code>from rdetoolkit.models.result import WorkflowResultManager\nimport traceback\n\ndef run_data_processing_pipeline():\n    \"\"\"Example of comprehensive workflow tracking.\"\"\"\n\n    manager = WorkflowResultManager()\n\n    # Track data validation step\n    try:\n        # Simulate data validation\n        print(\"Validating input data...\")\n        # validation_logic()\n\n        manager.add(\n            run_id=\"1\",\n            title=\"Data Validation\",\n            status=\"success\",\n            mode=\"validation\",\n            target=\"/data/validated\"\n        )\n\n    except Exception as e:\n        manager.add(\n            run_id=\"1\",\n            title=\"Data Validation\",\n            status=\"failed\",\n            mode=\"validation\",\n            error_code=400,\n            error_message=str(e),\n            stacktrace=traceback.format_exc()\n        )\n\n    # Track data transformation step\n    try:\n        print(\"Transforming data...\")\n        # transformation_logic()\n\n        manager.add(\n            run_id=\"2\",\n            title=\"Data Transformation\",\n            status=\"success\",\n            mode=\"transformation\",\n            target=\"/data/transformed\"\n        )\n\n    except Exception as e:\n        manager.add(\n            run_id=\"2\",\n            title=\"Data Transformation\",\n            status=\"failed\",\n            mode=\"transformation\",\n            error_code=500,\n            error_message=str(e),\n            stacktrace=traceback.format_exc()\n        )\n\n    # Track analysis step\n    try:\n        print(\"Running analysis...\")\n        # analysis_logic()\n\n        manager.add(\n            run_id=\"3\",\n            title=\"Data Analysis\",\n            status=\"success\",\n            mode=\"analysis\",\n            target=\"/data/results\"\n        )\n\n    except Exception as e:\n        manager.add(\n            run_id=\"3\",\n            title=\"Data Analysis\",\n            status=\"failed\",\n            mode=\"analysis\",\n            error_code=500,\n            error_message=str(e),\n            stacktrace=traceback.format_exc()\n        )\n\n    return manager\n\n# Execute pipeline and get results\nresults = run_data_processing_pipeline()\n\n# Display summary\nprint(f\"\\nPipeline completed with {len(results)} steps:\")\nfor status in results:\n    print(f\"  {status.run_id}: {status.title} - {status.status}\")\n\n# Save results\nwith open(\"pipeline_results.json\", \"w\") as f:\n    f.write(results.to_json())\n</code></pre>"},{"location":"rdetoolkit/models/result/#advanced-error-tracking-and-recovery","title":"Advanced Error Tracking and Recovery","text":"<pre><code>from rdetoolkit.models.result import WorkflowResultManager, WorkflowExecutionStatus\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass AdvancedWorkflowTracker:\n    \"\"\"Advanced workflow tracker with error recovery and retries.\"\"\"\n\n    def __init__(self, workflow_name: str):\n        self.workflow_name = workflow_name\n        self.manager = WorkflowResultManager()\n        self.start_time = datetime.now()\n\n    def execute_step(self, step_id: str, step_name: str, step_function, mode: str = \"default\", max_retries: int = 3):\n        \"\"\"Execute a workflow step with retry logic and comprehensive tracking.\"\"\"\n\n        for attempt in range(max_retries + 1):\n            try:\n                print(f\"Executing {step_name} (attempt {attempt + 1}/{max_retries + 1})\")\n\n                # Execute the step function\n                result = step_function()\n\n                # Success - record and return\n                self.manager.add(\n                    run_id=step_id,\n                    title=step_name,\n                    status=\"success\",\n                    mode=mode,\n                    target=str(result) if result else None\n                )\n\n                print(f\"\u2705 {step_name} completed successfully\")\n                return result\n\n            except Exception as e:\n                error_msg = f\"Attempt {attempt + 1} failed: {str(e)}\"\n                print(f\"\u274c {error_msg}\")\n\n                if attempt == max_retries:\n                    # Final failure - record and raise\n                    self.manager.add(\n                        run_id=step_id,\n                        title=step_name,\n                        status=\"failed\",\n                        mode=mode,\n                        error_code=getattr(e, 'errno', 500),\n                        error_message=str(e),\n                        stacktrace=traceback.format_exc()\n                    )\n                    raise\n                else:\n                    # Retry - record attempt\n                    self.manager.add(\n                        run_id=f\"{step_id}_retry_{attempt}\",\n                        title=f\"{step_name} (Retry {attempt + 1})\",\n                        status=\"retry\",\n                        mode=mode,\n                        error_code=getattr(e, 'errno', 500),\n                        error_message=error_msg\n                    )\n                    time.sleep(2 ** attempt)  # Exponential backoff\n\n    def get_summary(self) -&gt; dict:\n        \"\"\"Get comprehensive workflow execution summary.\"\"\"\n\n        total_steps = len(self.manager)\n        successful_steps = sum(1 for status in self.manager if status.status == \"success\")\n        failed_steps = sum(1 for status in self.manager if status.status == \"failed\")\n        retry_attempts = sum(1 for status in self.manager if status.status == \"retry\")\n\n        return {\n            \"workflow_name\": self.workflow_name,\n            \"start_time\": self.start_time.isoformat(),\n            \"total_steps\": total_steps,\n            \"successful_steps\": successful_steps,\n            \"failed_steps\": failed_steps,\n            \"retry_attempts\": retry_attempts,\n            \"success_rate\": (successful_steps / max(successful_steps + failed_steps, 1)) * 100,\n            \"execution_details\": [\n                {\n                    \"run_id\": status.run_id,\n                    \"title\": status.title,\n                    \"status\": status.status,\n                    \"mode\": status.mode,\n                    \"has_errors\": status.error_code is not None\n                }\n                for status in self.manager\n            ]\n        }\n\n    def save_detailed_report(self, output_path: Path):\n        \"\"\"Save detailed execution report.\"\"\"\n\n        summary = self.get_summary()\n\n        report = {\n            \"summary\": summary,\n            \"detailed_results\": self.manager.to_json()\n        }\n\n        with open(output_path, \"w\") as f:\n            import json\n            json.dump(report, f, indent=2, default=str)\n\n# Example usage with retry logic\ndef unreliable_data_processing():\n    \"\"\"Simulate unreliable data processing that might fail.\"\"\"\n    import random\n\n    if random.random() &lt; 0.3:  # 30% chance of failure\n        raise ValueError(\"Random processing error\")\n\n    return \"/data/processed/output.json\"\n\ndef reliable_validation():\n    \"\"\"Simulate reliable validation step.\"\"\"\n    return \"/data/validated/schema.json\"\n\n# Execute advanced workflow\ntracker = AdvancedWorkflowTracker(\"Data Processing Pipeline v2\")\n\ntry:\n    # Execute steps with different retry policies\n    tracker.execute_step(\"001\", \"Data Validation\", reliable_validation, \"validation\", max_retries=1)\n    tracker.execute_step(\"002\", \"Data Processing\", unreliable_data_processing, \"processing\", max_retries=3)\n\n    print(\"\\n\u2705 Workflow completed successfully!\")\n\nexcept Exception as e:\n    print(f\"\\n\u274c Workflow failed: {e}\")\n\nfinally:\n    # Generate and save report\n    summary = tracker.get_summary()\n    print(f\"\\nWorkflow Summary:\")\n    print(f\"  Total steps: {summary['total_steps']}\")\n    print(f\"  Successful: {summary['successful_steps']}\")\n    print(f\"  Failed: {summary['failed_steps']}\")\n    print(f\"  Retries: {summary['retry_attempts']}\")\n    print(f\"  Success rate: {summary['success_rate']:.1f}%\")\n\n    tracker.save_detailed_report(Path(\"detailed_workflow_report.json\"))\n</code></pre>"},{"location":"rdetoolkit/models/result/#workflow-result-analysis-and-reporting","title":"Workflow Result Analysis and Reporting","text":"<pre><code>from rdetoolkit.models.result import WorkflowResultManager, WorkflowExecutionStatus\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom datetime import datetime\n\nclass WorkflowAnalyzer:\n    \"\"\"Analyzer for workflow execution results and patterns.\"\"\"\n\n    def __init__(self):\n        self.results_cache = {}\n\n    def load_results_from_json(self, file_path: Path) -&gt; WorkflowResultManager:\n        \"\"\"Load workflow results from JSON file.\"\"\"\n\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n\n        # Handle both direct WorkflowExecutionResults and nested structures\n        if \"statuses\" in data:\n            statuses_data = data[\"statuses\"]\n        elif \"detailed_results\" in data:\n            detailed_data = json.loads(data[\"detailed_results\"])\n            statuses_data = detailed_data[\"statuses\"]\n        else:\n            statuses_data = data\n\n        manager = WorkflowResultManager()\n\n        for status_data in statuses_data:\n            status = WorkflowExecutionStatus(**status_data)\n            manager.add_status(status)\n\n        return manager\n\n    def analyze_failure_patterns(self, manager: WorkflowResultManager) -&gt; dict:\n        \"\"\"Analyze common failure patterns in workflow results.\"\"\"\n\n        analysis = {\n            \"total_executions\": len(manager),\n            \"failure_count\": 0,\n            \"success_count\": 0,\n            \"error_codes\": defaultdict(int),\n            \"failure_by_mode\": defaultdict(int),\n            \"failure_by_title\": defaultdict(int),\n            \"common_error_messages\": defaultdict(int),\n            \"steps_with_stacktraces\": []\n        }\n\n        for status in manager:\n            if status.status == \"failed\":\n                analysis[\"failure_count\"] += 1\n\n                if status.error_code:\n                    analysis[\"error_codes\"][status.error_code] += 1\n\n                analysis[\"failure_by_mode\"][status.mode] += 1\n                analysis[\"failure_by_title\"][status.title] += 1\n\n                if status.error_message:\n                    # Simplify error message for pattern matching\n                    simplified_msg = status.error_message.split(':')[0] if ':' in status.error_message else status.error_message\n                    analysis[\"common_error_messages\"][simplified_msg] += 1\n\n                if status.stacktrace:\n                    analysis[\"steps_with_stacktraces\"].append({\n                        \"run_id\": status.run_id,\n                        \"title\": status.title,\n                        \"error\": status.error_message\n                    })\n\n            elif status.status == \"success\":\n                analysis[\"success_count\"] += 1\n\n        # Calculate success rate\n        total_completed = analysis[\"failure_count\"] + analysis[\"success_count\"]\n        analysis[\"success_rate\"] = (analysis[\"success_count\"] / max(total_completed, 1)) * 100\n\n        return analysis\n\n    def generate_failure_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate human-readable failure analysis report.\"\"\"\n\n        report = f\"\"\"\nWORKFLOW FAILURE ANALYSIS REPORT\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nOVERVIEW:\n- Total executions: {analysis['total_executions']}\n- Successful executions: {analysis['success_count']}\n- Failed executions: {analysis['failure_count']}\n- Success rate: {analysis['success_rate']:.1f}%\n\nERROR CODE DISTRIBUTION:\n\"\"\"\n\n        for error_code, count in sorted(analysis['error_codes'].items()):\n            report += f\"- Error {error_code}: {count} occurrences\\n\"\n\n        report += \"\\nFAILURES BY MODE:\\n\"\n        for mode, count in sorted(analysis['failure_by_mode'].items(), key=lambda x: x[1], reverse=True):\n            report += f\"- {mode}: {count} failures\\n\"\n\n        report += \"\\nFAILURES BY STEP:\\n\"\n        for title, count in sorted(analysis['failure_by_title'].items(), key=lambda x: x[1], reverse=True):\n            report += f\"- {title}: {count} failures\\n\"\n\n        report += \"\\nCOMMON ERROR PATTERNS:\\n\"\n        for error_msg, count in sorted(analysis['common_error_messages'].items(), key=lambda x: x[1], reverse=True)[:5]:\n            report += f\"- {error_msg}: {count} occurrences\\n\"\n\n        if analysis['steps_with_stacktraces']:\n            report += f\"\\nSTEPS WITH DETAILED STACKTRACES: {len(analysis['steps_with_stacktraces'])}\\n\"\n            for step in analysis['steps_with_stacktraces'][:3]:  # Show first 3\n                report += f\"- {step['run_id']}: {step['title']} - {step['error']}\\n\"\n\n        return report\n\n    def compare_workflow_runs(self, managers: list[WorkflowResultManager]) -&gt; dict:\n        \"\"\"Compare multiple workflow runs to identify trends.\"\"\"\n\n        comparison = {\n            \"run_count\": len(managers),\n            \"average_steps\": sum(len(m) for m in managers) / len(managers),\n            \"success_rates\": [],\n            \"most_reliable_steps\": defaultdict(int),\n            \"least_reliable_steps\": defaultdict(int),\n            \"improvement_trend\": None\n        }\n\n        for manager in managers:\n            analysis = self.analyze_failure_patterns(manager)\n            comparison[\"success_rates\"].append(analysis[\"success_rate\"])\n\n            # Track step reliability\n            for status in manager:\n                if status.status == \"success\":\n                    comparison[\"most_reliable_steps\"][status.title] += 1\n                elif status.status == \"failed\":\n                    comparison[\"least_reliable_steps\"][status.title] += 1\n\n        # Calculate improvement trend\n        if len(comparison[\"success_rates\"]) &gt;= 2:\n            recent_rate = sum(comparison[\"success_rates\"][-3:]) / min(3, len(comparison[\"success_rates\"]))\n            earlier_rate = sum(comparison[\"success_rates\"][:-3]) / max(1, len(comparison[\"success_rates\"]) - 3)\n            comparison[\"improvement_trend\"] = recent_rate - earlier_rate\n\n        return comparison\n\n# Example usage\nanalyzer = WorkflowAnalyzer()\n\n# Create sample workflow results for analysis\ndef create_sample_results():\n    \"\"\"Create sample workflow results for testing.\"\"\"\n\n    managers = []\n\n    for run in range(3):\n        manager = WorkflowResultManager()\n\n        # Add various execution results\n        manager.add(f\"{run*10 + 1}\", \"Data Validation\", \"success\", \"validation\")\n        manager.add(f\"{run*10 + 2}\", \"Data Processing\", \"failed\" if run == 1 else \"success\", \"processing\",\n                   error_code=500 if run == 1 else None,\n                   error_message=\"Memory allocation failed\" if run == 1 else None)\n        manager.add(f\"{run*10 + 3}\", \"Result Generation\", \"success\", \"output\")\n\n        managers.append(manager)\n\n    return managers\n\n# Analyze sample results\nsample_managers = create_sample_results()\n\n# Individual analysis\nfor i, manager in enumerate(sample_managers):\n    analysis = analyzer.analyze_failure_patterns(manager)\n    print(f\"Run {i+1} Analysis:\")\n    print(f\"  Success rate: {analysis['success_rate']:.1f}%\")\n    print(f\"  Failures: {analysis['failure_count']}\")\n\n# Comparative analysis\ncomparison = analyzer.compare_workflow_runs(sample_managers)\nprint(f\"\\nComparative Analysis:\")\nprint(f\"  Average steps per run: {comparison['average_steps']:.1f}\")\nprint(f\"  Success rates: {[f'{rate:.1f}%' for rate in comparison['success_rates']]}\")\n\nif comparison[\"improvement_trend\"]:\n    trend_word = \"improving\" if comparison[\"improvement_trend\"] &gt; 0 else \"declining\"\n    print(f\"  Trend: {trend_word} ({comparison['improvement_trend']:+.1f}%)\")\n</code></pre>"},{"location":"rdetoolkit/models/result/#integration-with-monitoring-systems","title":"Integration with Monitoring Systems","text":"<pre><code>from rdetoolkit.models.result import WorkflowResultManager, WorkflowExecutionStatus\nimport requests\nimport json\nfrom datetime import datetime\n\nclass WorkflowMonitoringIntegration:\n    \"\"\"Integration with external monitoring and alerting systems.\"\"\"\n\n    def __init__(self, webhook_url: str = None, slack_webhook: str = None):\n        self.webhook_url = webhook_url\n        self.slack_webhook = slack_webhook\n        self.manager = WorkflowResultManager()\n\n    def add_with_monitoring(self, run_id: str, title: str, status: str, mode: str, **kwargs):\n        \"\"\"Add workflow status with automatic monitoring notifications.\"\"\"\n\n        # Add to manager\n        self.manager.add(run_id, title, status, mode, **kwargs)\n\n        # Send notifications based on status\n        if status == \"failed\":\n            self._send_failure_alert(run_id, title, kwargs.get('error_message'))\n        elif status == \"success\":\n            self._send_success_notification(run_id, title)\n\n    def _send_failure_alert(self, run_id: str, title: str, error_message: str = None):\n        \"\"\"Send failure alert to monitoring systems.\"\"\"\n\n        alert_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"severity\": \"error\",\n            \"service\": \"workflow_execution\",\n            \"run_id\": run_id,\n            \"title\": title,\n            \"message\": error_message or \"Workflow execution failed\",\n            \"alert_type\": \"workflow_failure\"\n        }\n\n        # Send to generic webhook\n        if self.webhook_url:\n            try:\n                requests.post(self.webhook_url, json=alert_data, timeout=5)\n            except Exception as e:\n                print(f\"Failed to send webhook alert: {e}\")\n\n        # Send to Slack\n        if self.slack_webhook:\n            slack_message = {\n                \"text\": f\"\ud83d\udea8 Workflow Failure Alert\",\n                \"attachments\": [\n                    {\n                        \"color\": \"danger\",\n                        \"fields\": [\n                            {\"title\": \"Run ID\", \"value\": run_id, \"short\": True},\n                            {\"title\": \"Title\", \"value\": title, \"short\": True},\n                            {\"title\": \"Error\", \"value\": error_message or \"Unknown error\", \"short\": False}\n                        ]\n                    }\n                ]\n            }\n\n            try:\n                requests.post(self.slack_webhook, json=slack_message, timeout=5)\n            except Exception as e:\n                print(f\"Failed to send Slack alert: {e}\")\n\n    def _send_success_notification(self, run_id: str, title: str):\n        \"\"\"Send success notification (if configured for verbose monitoring).\"\"\"\n\n        # Typically only send success notifications for important workflows\n        if \"critical\" in title.lower() or \"production\" in title.lower():\n            success_data = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"severity\": \"info\",\n                \"service\": \"workflow_execution\",\n                \"run_id\": run_id,\n                \"title\": title,\n                \"message\": \"Critical workflow completed successfully\",\n                \"alert_type\": \"workflow_success\"\n            }\n\n            if self.webhook_url:\n                try:\n                    requests.post(self.webhook_url, json=success_data, timeout=5)\n                except Exception as e:\n                    print(f\"Failed to send success notification: {e}\")\n\n    def generate_health_check_report(self) -&gt; dict:\n        \"\"\"Generate health check report for monitoring systems.\"\"\"\n\n        now = datetime.now()\n        recent_failures = sum(1 for status in self.manager\n                            if status.status == \"failed\")\n\n        total_executions = len(self.manager)\n        success_rate = 0 if total_executions == 0 else \\\n                      (total_executions - recent_failures) / total_executions * 100\n\n        health_status = \"healthy\"\n        if success_rate &lt; 50:\n            health_status = \"critical\"\n        elif success_rate &lt; 80:\n            health_status = \"warning\"\n\n        return {\n            \"timestamp\": now.isoformat(),\n            \"service\": \"workflow_execution\",\n            \"status\": health_status,\n            \"metrics\": {\n                \"total_executions\": total_executions,\n                \"recent_failures\": recent_failures,\n                \"success_rate\": success_rate\n            },\n            \"recent_executions\": [\n                {\n                    \"run_id\": status.run_id,\n                    \"title\": status.title,\n                    \"status\": status.status,\n                    \"mode\": status.mode\n                }\n                for status in list(self.manager)[-5:]  # Last 5 executions\n            ]\n        }\n\n# Example usage with monitoring\nmonitor = WorkflowMonitoringIntegration(\n    webhook_url=\"https://monitoring.example.com/webhook\",\n    slack_webhook=\"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\"\n)\n\n# Simulate workflow executions with monitoring\nmonitor.add_with_monitoring(\"001\", \"Critical Data Backup\", \"success\", \"backup\")\nmonitor.add_with_monitoring(\"002\", \"User Data Processing\", \"failed\", \"processing\",\n                          error_code=500, error_message=\"Database connection timeout\")\nmonitor.add_with_monitoring(\"003\", \"Report Generation\", \"success\", \"reporting\")\n\n# Generate health check\nhealth_report = monitor.generate_health_check_report()\nprint(\"Health Check Report:\")\nprint(json.dumps(health_report, indent=2))\n</code></pre>"},{"location":"rdetoolkit/models/result/#best-practices","title":"Best Practices","text":"<ol> <li>Use Meaningful Run IDs: Create systematic run ID schemes:</li> </ol> <pre><code># Good - includes date and sequence\nrun_id = f\"{datetime.now().strftime('%Y%m%d')}_{sequence:03d}\"\n\n# Good - includes workflow type\nrun_id = f\"validation_{run_number}\"\n</code></pre> <ol> <li>Provide Descriptive Titles: Include context in workflow titles:</li> </ol> <pre><code># Good\ntitle = \"Customer Data ETL Pipeline - Weekly Batch\"\n\n# Avoid\ntitle = \"Pipeline\"\n</code></pre> <ol> <li>Standardize Status Values: Use consistent status terminology:</li> </ol> <pre><code>VALID_STATUSES = [\"success\", \"failed\", \"running\", \"queued\", \"cancelled\"]\n</code></pre> <ol> <li>Include Relevant Target Information: Store output paths and resources:</li> </ol> <pre><code>manager.add(\n    run_id=\"001\",\n    title=\"Data Export\",\n    status=\"success\",\n    mode=\"export\",\n    target=\"/exports/customer_data_2025_01_15.csv\"\n)\n</code></pre> <ol> <li>Capture Complete Error Information: Include both error messages and stack traces:</li> </ol> <pre><code>try:\n    risky_operation()\nexcept Exception as e:\n    manager.add(\n        run_id=\"002\",\n        title=\"Risky Operation\",\n        status=\"failed\",\n        mode=\"processing\",\n        error_code=getattr(e, 'errno', 500),\n        error_message=str(e),\n        stacktrace=traceback.format_exc()\n    )\n</code></pre>"},{"location":"rdetoolkit/models/result/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/models/result/#validation-errors","title":"Validation Errors","text":"<pre><code>from rdetoolkit.models.result import WorkflowExecutionStatus\nfrom pydantic import ValidationError\n\ndef safe_create_status(run_id, title, status, mode, **kwargs):\n    \"\"\"Safely create workflow status with validation.\"\"\"\n\n    try:\n        return WorkflowExecutionStatus(\n            run_id=run_id,\n            title=title,\n            status=status,\n            mode=mode,\n            **kwargs\n        )\n    except ValidationError as e:\n        print(f\"Validation error creating status: {e}\")\n        return None\n\n# Example with validation\nvalid_status = safe_create_status(\"1\", \"Test\", \"success\", \"test\")\ninvalid_status = safe_create_status(None, \"Test\", \"success\", \"test\")  # Invalid run_id\n\nprint(f\"Valid status created: {valid_status is not None}\")    # True\nprint(f\"Invalid status created: {invalid_status is not None}\") # False\n</code></pre>"},{"location":"rdetoolkit/models/result/#see-also","title":"See Also","text":"<ul> <li>Pydantic Documentation - For model validation and serialization</li> <li>Workflow Management - For workflow orchestration systems</li> <li>Monitoring Integration - For metrics and monitoring systems</li> <li>JSON Schema - For result format validation</li> </ul>"},{"location":"rdetoolkit/processing/","title":"Processing Module","text":"<p>The <code>rdetoolkit.processing</code> module provides a modern, pipeline-based architecture for data processing operations in the RDE toolkit. This module replaces the traditional mode-based processing approach with a more flexible and extensible pipeline system.</p>"},{"location":"rdetoolkit/processing/#overview","title":"Overview","text":"<p>The processing module introduces a clean separation of concerns through:</p> <ul> <li>Pipeline Architecture: Sequential execution of processing steps</li> <li>Processor Components: Individual, reusable processing units</li> <li>Context Management: Centralized state and resource management</li> <li>Factory Pattern: Automated pipeline construction for different modes</li> <li>Extensibility: Easy addition of new processors and pipelines</li> </ul>"},{"location":"rdetoolkit/processing/#architecture-components","title":"Architecture Components","text":""},{"location":"rdetoolkit/processing/#core-classes","title":"Core Classes","text":"<ul> <li>Pipeline: Orchestrates the execution of multiple processors</li> <li>Processor: Abstract base class for all processing operations</li> <li>ProcessingContext: Encapsulates all processing state and resources</li> <li>PipelineFactory: Creates pre-configured pipelines for different modes</li> </ul>"},{"location":"rdetoolkit/processing/#supported-processing-modes","title":"Supported Processing Modes","text":"<ol> <li>RDEFormat Mode: Processes RDE format files with structured data</li> <li>MultiDataTile Mode: Handles multiple data files in flat structure</li> <li>ExcelInvoice Mode: Processes Excel-based invoice data</li> <li>Invoice Mode: Standard invoice processing</li> <li>SmartTableInvoice Mode: Processes SmartTable-generated data</li> </ol>"},{"location":"rdetoolkit/processing/#quick-start","title":"Quick Start","text":""},{"location":"rdetoolkit/processing/#basic-usage","title":"Basic Usage","text":"<pre><code>from rdetoolkit.processing import PipelineFactory, ProcessingContext\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\n\n# Create processing context\ncontext = ProcessingContext(\n    index=\"0\",\n    srcpaths=input_paths,\n    resource_paths=output_paths,\n    datasets_function=None,\n    mode_name=\"Invoice\"\n)\n\n# Create and execute pipeline\npipeline = PipelineFactory.create_pipeline(\"invoice\")\nresult = pipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/#custom-pipeline-creation","title":"Custom Pipeline Creation","text":"<pre><code>from rdetoolkit.processing import Pipeline\nfrom rdetoolkit.processing.processors import FileCopier, DatasetRunner, ThumbnailGenerator\n\n# Build custom pipeline\npipeline = (Pipeline()\n    .add(FileCopier())\n    .add(DatasetRunner())\n    .add(ThumbnailGenerator()))\n\n# Execute pipeline\nresult = pipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/#module-structure","title":"Module Structure","text":"<pre><code>processing/\n\u251c\u2500\u2500 __init__.py          # Main module exports\n\u251c\u2500\u2500 context.py           # Processing context management\n\u251c\u2500\u2500 factories.py         # Pipeline factory and builders\n\u251c\u2500\u2500 pipeline.py          # Core pipeline and processor classes\n\u2514\u2500\u2500 processors/          # Individual processor implementations\n    \u251c\u2500\u2500 __init__.py      # Processor exports\n    \u251c\u2500\u2500 datasets.py      # Custom dataset processing\n    \u251c\u2500\u2500 descriptions.py  # Description updates\n    \u251c\u2500\u2500 files.py         # File operations\n    \u251c\u2500\u2500 invoice.py       # Invoice initialization\n    \u251c\u2500\u2500 thumbnails.py    # Thumbnail generation\n    \u251c\u2500\u2500 validation.py    # Data validation\n    \u2514\u2500\u2500 variables.py     # Variable replacement\n</code></pre>"},{"location":"rdetoolkit/processing/#key-features","title":"Key Features","text":""},{"location":"rdetoolkit/processing/#pipeline-execution","title":"Pipeline Execution","text":"<ul> <li>Sequential Processing: Processors execute in order</li> <li>Context Sharing: All processors share the same context</li> <li>Error Handling: Comprehensive error handling and logging</li> <li>Status Reporting: Detailed execution status and results</li> </ul>"},{"location":"rdetoolkit/processing/#processor-design","title":"Processor Design","text":"<ul> <li>Stateless: Processors maintain no internal state</li> <li>Idempotent: Can be safely re-executed</li> <li>Configurable: Behavior controlled by context configuration</li> <li>Extensible: Easy to add new processing capabilities</li> </ul>"},{"location":"rdetoolkit/processing/#factory-pattern","title":"Factory Pattern","text":"<ul> <li>Mode-based Creation: Automatic pipeline setup for different modes</li> <li>Backward Compatibility: Support for legacy mode names</li> <li>Extensible: Easy addition of new processing modes</li> </ul>"},{"location":"rdetoolkit/processing/#error-handling","title":"Error Handling","text":"<p>The processing module provides comprehensive error handling:</p> <pre><code>try:\n    result = pipeline.execute(context)\n    if result.status == \"success\":\n        print(\"Processing completed successfully\")\n    else:\n        print(f\"Processing failed: {result.error_message}\")\nexcept Exception as e:\n    print(f\"Pipeline execution failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory Efficient: Streaming processing where possible</li> <li>Parallel Processing: Processor design supports parallel execution</li> <li>Resource Management: Proper cleanup and resource management</li> <li>Logging: Comprehensive logging for debugging and monitoring</li> </ul>"},{"location":"rdetoolkit/processing/#migration-from-legacy-code","title":"Migration from Legacy Code","text":"<p>The processing module is designed to be backward compatible with existing mode-based processing:</p> <pre><code># Legacy approach\nfrom rdetoolkit.modeproc import invoice_mode_process\n\n# New approach\nfrom rdetoolkit.processing import PipelineFactory\n\npipeline = PipelineFactory.create_pipeline(\"invoice\")\nresult = pipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/#see-also","title":"See Also","text":"<ul> <li>Context Management - Processing context and state management</li> <li>Pipeline Architecture - Core pipeline and processor classes</li> <li>Factory Pattern - Pipeline creation and builders</li> <li>Processors - Individual processor implementations</li> </ul>"},{"location":"rdetoolkit/processing/context/","title":"Processing Context","text":"<p>The <code>rdetoolkit.processing.context</code> module provides the <code>ProcessingContext</code> class, which encapsulates all the information needed for processing operations across different modes.</p>"},{"location":"rdetoolkit/processing/context/#overview","title":"Overview","text":"<p>The <code>ProcessingContext</code> serves as a centralized container for all processing-related data, configuration, and resources. It provides a consistent interface for processors to access input paths, output resources, and mode-specific information.</p>"},{"location":"rdetoolkit/processing/context/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/context/#processingcontext","title":"ProcessingContext","text":"<p>A dataclass that encapsulates all information needed for processing operations in different modes (RDEFormat, MultiFile, ExcelInvoice, etc.).</p>"},{"location":"rdetoolkit/processing/context/#constructor","title":"Constructor","text":"<pre><code>@dataclass\nclass ProcessingContext:\n    index: str\n    srcpaths: RdeInputDirPaths\n    resource_paths: RdeOutputResourcePath\n    datasets_function: _CallbackType | None\n    mode_name: str\n    excel_file: Path | None = None\n    excel_index: int | None = None\n    smarttable_file: Path | None = None\n</code></pre> <p>Parameters: - <code>index</code> (str): Processing index identifier - <code>srcpaths</code> (RdeInputDirPaths): Input directory paths configuration - <code>resource_paths</code> (RdeOutputResourcePath): Output resource paths - <code>datasets_function</code> (Optional[_CallbackType]): Custom dataset processing function - <code>mode_name</code> (str): Processing mode name - <code>excel_file</code> (Optional[Path]): Excel invoice file path (for Excel mode) - <code>excel_index</code> (Optional[int]): Excel processing index (for Excel mode) - <code>smarttable_file</code> (Optional[Path]): SmartTable file path (for SmartTable mode)</p>"},{"location":"rdetoolkit/processing/context/#properties","title":"Properties","text":""},{"location":"rdetoolkit/processing/context/#basedir","title":"basedir","text":"<p>Get the base directory for the processing operation.</p> <pre><code>@property\ndef basedir(self) -&gt; str\n</code></pre> <p>Returns: - <code>str</code>: Base directory path from the first raw file, or empty string if no raw files</p> <p>Example: <pre><code>context = ProcessingContext(...)\nprint(f\"Base directory: {context.basedir}\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#invoice_dst_filepath","title":"invoice_dst_filepath","text":"<p>Get the destination invoice file path.</p> <pre><code>@property\ndef invoice_dst_filepath(self) -&gt; Path\n</code></pre> <p>Returns: - <code>Path</code>: Path to the output invoice.json file</p> <p>Example: <pre><code>invoice_path = context.invoice_dst_filepath\n# Returns: Path(\"data/divided/0001/invoice/invoice.json\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#schema_path","title":"schema_path","text":"<p>Get the invoice schema file path.</p> <pre><code>@property\ndef schema_path(self) -&gt; Path\n</code></pre> <p>Returns: - <code>Path</code>: Path to the invoice.schema.json file in tasksupport directory</p> <p>Example: <pre><code>schema_path = context.schema_path\n# Returns: Path(\"data/tasksupport/invoice.schema.json\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#metadata_def_path","title":"metadata_def_path","text":"<p>Get the metadata definition file path.</p> <pre><code>@property\ndef metadata_def_path(self) -&gt; Path\n</code></pre> <p>Returns: - <code>Path</code>: Path to the metadata-def.json file in tasksupport directory</p> <p>Example: <pre><code>metadata_def = context.metadata_def_path\n# Returns: Path(\"data/tasksupport/metadata-def.json\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#metadata_path","title":"metadata_path","text":"<p>Get the metadata.json file path.</p> <pre><code>@property\ndef metadata_path(self) -&gt; Path\n</code></pre> <p>Returns: - <code>Path</code>: Path to the output metadata.json file</p> <p>Example: <pre><code>metadata_path = context.metadata_path\n# Returns: Path(\"data/divided/0001/meta/metadata.json\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#is_excel_mode","title":"is_excel_mode","text":"<p>Check if this is Excel invoice processing mode.</p> <pre><code>@property\ndef is_excel_mode(self) -&gt; bool\n</code></pre> <p>Returns: - <code>bool</code>: True if both excel_file and excel_index are set</p> <p>Example: <pre><code>if context.is_excel_mode:\n    print(\"Processing in Excel invoice mode\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#excel_invoice_file","title":"excel_invoice_file","text":"<p>Get the Excel invoice file path (for Excel mode only).</p> <pre><code>@property\ndef excel_invoice_file(self) -&gt; Path\n</code></pre> <p>Returns: - <code>Path</code>: Path to the Excel invoice file</p> <p>Raises: - <code>ValueError</code>: If excel_file is not set for this context</p> <p>Example: <pre><code>if context.is_excel_mode:\n    excel_file = context.excel_invoice_file\n    print(f\"Excel file: {excel_file}\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#is_smarttable_mode","title":"is_smarttable_mode","text":"<p>Check if this is SmartTable processing mode.</p> <pre><code>@property\ndef is_smarttable_mode(self) -&gt; bool\n</code></pre> <p>Returns: - <code>bool</code>: True if smarttable_file is set</p> <p>Example: <pre><code>if context.is_smarttable_mode:\n    print(\"Processing in SmartTable mode\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#smarttable_invoice_file","title":"smarttable_invoice_file","text":"<p>Get the SmartTable file path (for SmartTable mode only).</p> <pre><code>@property\ndef smarttable_invoice_file(self) -&gt; Path\n</code></pre> <p>Returns: - <code>Path</code>: Path to the SmartTable file</p> <p>Raises: - <code>ValueError</code>: If smarttable_file is not set for this context</p> <p>Example: <pre><code>if context.is_smarttable_mode:\n    smarttable_file = context.smarttable_invoice_file\n    print(f\"SmartTable file: {smarttable_file}\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/context/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/context/#basic-context-creation","title":"Basic Context Creation","text":"<pre><code>from rdetoolkit.processing.context import ProcessingContext\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\nfrom pathlib import Path\n\n# Create input and output paths\nsrcpaths = RdeInputDirPaths(\n    inputdata=Path(\"data/inputdata\"),\n    invoice=Path(\"data/invoice\"),\n    tasksupport=Path(\"data/tasksupport\")\n)\n\nresource_paths = RdeOutputResourcePath(\n    raw=Path(\"data/divided/0001/raw\"),\n    rawfiles=(Path(\"data/temp/sample.txt\"),),\n    struct=Path(\"data/divided/0001/structured\"),\n    main_image=Path(\"data/divided/0001/main_image\"),\n    # ... other paths\n)\n\n# Create processing context\ncontext = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=None,\n    mode_name=\"Invoice\"\n)\n</code></pre>"},{"location":"rdetoolkit/processing/context/#excel-mode-context","title":"Excel Mode Context","text":"<pre><code># Create context for Excel invoice processing\nexcel_context = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=None,\n    mode_name=\"ExcelInvoice\",\n    excel_file=Path(\"data/inputdata/dataset_excel_invoice.xlsx\"),\n    excel_index=1\n)\n\n# Check mode and access Excel-specific properties\nif excel_context.is_excel_mode:\n    excel_file = excel_context.excel_invoice_file\n    print(f\"Processing Excel file: {excel_file}\")\n</code></pre>"},{"location":"rdetoolkit/processing/context/#smarttable-mode-context","title":"SmartTable Mode Context","text":"<pre><code># Create context for SmartTable processing\nsmarttable_context = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=None,\n    mode_name=\"SmartTableInvoice\",\n    smarttable_file=Path(\"data/inputdata/smarttable_data.csv\")\n)\n\n# Check mode and access SmartTable-specific properties\nif smarttable_context.is_smarttable_mode:\n    smarttable_file = smarttable_context.smarttable_invoice_file\n    print(f\"Processing SmartTable file: {smarttable_file}\")\n</code></pre>"},{"location":"rdetoolkit/processing/context/#using-context-in-processors","title":"Using Context in Processors","text":"<pre><code>from rdetoolkit.processing.pipeline import Processor\n\nclass CustomProcessor(Processor):\n    def process(self, context: ProcessingContext) -&gt; None:\n        # Access input configuration\n        config = context.srcpaths.config\n\n        # Get output paths\n        raw_dir = context.resource_paths.raw\n        structured_dir = context.resource_paths.struct\n\n        # Check processing mode\n        if context.is_excel_mode:\n            excel_file = context.excel_invoice_file\n            # Process Excel-specific logic\n        elif context.is_smarttable_mode:\n            smarttable_file = context.smarttable_invoice_file\n            # Process SmartTable-specific logic\n        else:\n            # Standard processing logic\n            pass\n\n        # Access common paths\n        invoice_dst = context.invoice_dst_filepath\n        metadata_path = context.metadata_path\n</code></pre>"},{"location":"rdetoolkit/processing/context/#context-with-custom-dataset-function","title":"Context with Custom Dataset Function","text":"<pre><code>def custom_dataset_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Custom processing function.\"\"\"\n    print(f\"Processing {len(resource_paths.rawfiles)} files\")\n    # Custom processing logic here\n\n# Create context with custom function\ncontext = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=custom_dataset_function,\n    mode_name=\"MultiDataTile\"\n)\n</code></pre>"},{"location":"rdetoolkit/processing/context/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/context/#safe-property-access","title":"Safe Property Access","text":"<pre><code># Safe access to mode-specific properties\ntry:\n    if context.is_excel_mode:\n        excel_file = context.excel_invoice_file\n        print(f\"Excel file: {excel_file}\")\nexcept ValueError as e:\n    print(f\"Excel file not available: {e}\")\n\ntry:\n    if context.is_smarttable_mode:\n        smarttable_file = context.smarttable_invoice_file\n        print(f\"SmartTable file: {smarttable_file}\")\nexcept ValueError as e:\n    print(f\"SmartTable file not available: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/context/#path-validation","title":"Path Validation","text":"<pre><code># Validate paths before processing\nif not context.schema_path.exists():\n    raise FileNotFoundError(f\"Schema file not found: {context.schema_path}\")\n\nif not context.metadata_def_path.exists():\n    print(f\"Warning: Metadata definition not found: {context.metadata_def_path}\")\n</code></pre>"},{"location":"rdetoolkit/processing/context/#best-practices","title":"Best Practices","text":"<ol> <li>Immutable Context: Treat the context as immutable during processing</li> <li>Path Validation: Always validate paths before using them</li> <li>Mode Checking: Use mode properties to conditionally access mode-specific data</li> <li>Error Handling: Handle ValueError exceptions when accessing mode-specific properties</li> <li>Resource Management: Use context paths consistently across processors</li> </ol>"},{"location":"rdetoolkit/processing/context/#see-also","title":"See Also","text":"<ul> <li>Pipeline Architecture - Core pipeline and processor classes</li> <li>Processors - Individual processor implementations</li> <li>Models - Data type definitions</li> </ul>"},{"location":"rdetoolkit/processing/factories/","title":"Pipeline Factory","text":"<p>The <code>rdetoolkit.processing.factories</code> module provides factory classes and builders for creating predefined processing pipelines. It implements the Factory and Builder design patterns to automate pipeline construction for different processing modes.</p>"},{"location":"rdetoolkit/processing/factories/#overview","title":"Overview","text":"<p>The factory module provides:</p> <ul> <li>Automated Pipeline Creation: Pre-configured pipelines for different modes</li> <li>Builder Pattern: Flexible pipeline construction</li> <li>Mode Enumeration: Standardized processing mode definitions</li> <li>Backward Compatibility: Support for legacy method names</li> <li>Extensibility: Easy addition of new modes and pipelines</li> </ul>"},{"location":"rdetoolkit/processing/factories/#classes-and-enums","title":"Classes and Enums","text":""},{"location":"rdetoolkit/processing/factories/#processingmode","title":"ProcessingMode","text":"<p>Enumeration of supported processing modes.</p> <pre><code>class ProcessingMode(Enum):\n    \"\"\"Enumeration of supported processing modes.\"\"\"\n    RDEFORMAT = \"rdeformat\"\n    MULTIDATATILE = \"multidatatile\"\n    EXCELINVOICE = \"excelinvoice\"\n    INVOICE = \"invoice\"\n    SMARTTABLEINVOICE = \"smarttableinvoice\"\n</code></pre> <p>Supported Modes: - RDEFORMAT: Processes RDE format files with structured data - MULTIDATATILE: Handles multiple data files in flat structure - EXCELINVOICE: Processes Excel-based invoice data - INVOICE: Standard invoice processing - SMARTTABLEINVOICE: Processes SmartTable-generated data</p>"},{"location":"rdetoolkit/processing/factories/#pipelinebuilder","title":"PipelineBuilder","text":"<p>Abstract base class for pipeline builders.</p> <pre><code>class PipelineBuilder(ABC):\n    \"\"\"Abstract base class for pipeline builders.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/factories/#build","title":"build","text":"<p>Build and return a configured pipeline.</p> <pre><code>@abstractmethod\ndef build(self) -&gt; Pipeline\n</code></pre> <p>Returns: - <code>Pipeline</code>: Configured pipeline for the specific mode</p>"},{"location":"rdetoolkit/processing/factories/#concrete-builder-classes","title":"Concrete Builder Classes","text":""},{"location":"rdetoolkit/processing/factories/#rdeformatpipelinebuilder","title":"RDEFormatPipelineBuilder","text":"<p>Builder for RDEFormat mode pipelines.</p> <pre><code>def build(self) -&gt; Pipeline\n</code></pre> <p>Pipeline Configuration: 1. <code>StandardInvoiceInitializer</code> - Initialize invoice from original 2. <code>RDEFormatFileCopier</code> - Copy files by directory structure 3. <code>DatasetRunner</code> - Execute custom dataset processing 4. <code>ThumbnailGenerator</code> - Generate thumbnail images 5. <code>DescriptionUpdater</code> - Update descriptions with features 6. <code>MetadataValidator</code> - Validate metadata files 7. <code>InvoiceValidator</code> - Validate invoice files</p>"},{"location":"rdetoolkit/processing/factories/#multifilepipelinebuilder","title":"MultiFilePipelineBuilder","text":"<p>Builder for MultiFile mode pipelines.</p> <pre><code>def build(self) -&gt; Pipeline\n</code></pre> <p>Pipeline Configuration: 1. <code>StandardInvoiceInitializer</code> - Initialize invoice from original 2. <code>FileCopier</code> - Copy raw files to output directories 3. <code>DatasetRunner</code> - Execute custom dataset processing 4. <code>VariableApplier</code> - Apply magic variables to invoice 5. <code>ThumbnailGenerator</code> - Generate thumbnail images 6. <code>DescriptionUpdater</code> - Update descriptions with features 7. <code>MetadataValidator</code> - Validate metadata files 8. <code>InvoiceValidator</code> - Validate invoice files</p>"},{"location":"rdetoolkit/processing/factories/#excelinvoicepipelinebuilder","title":"ExcelInvoicePipelineBuilder","text":"<p>Builder for ExcelInvoice mode pipelines.</p> <pre><code>def build(self) -&gt; Pipeline\n</code></pre> <p>Pipeline Configuration: 1. <code>ExcelInvoiceInitializer</code> - Initialize invoice from Excel data 2. <code>FileCopier</code> - Copy raw files to output directories 3. <code>DatasetRunner</code> - Execute custom dataset processing 4. <code>VariableApplier</code> - Apply magic variables to invoice 5. <code>ThumbnailGenerator</code> - Generate thumbnail images 6. <code>DescriptionUpdater</code> - Update descriptions with features 7. <code>MetadataValidator</code> - Validate metadata files 8. <code>InvoiceValidator</code> - Validate invoice files</p>"},{"location":"rdetoolkit/processing/factories/#invoicepipelinebuilder","title":"InvoicePipelineBuilder","text":"<p>Builder for Invoice mode pipelines.</p> <pre><code>def build(self) -&gt; Pipeline\n</code></pre> <p>Pipeline Configuration: 1. <code>FileCopier</code> - Copy raw files to output directories 2. <code>DatasetRunner</code> - Execute custom dataset processing 3. <code>ThumbnailGenerator</code> - Generate thumbnail images 4. <code>VariableApplier</code> - Apply magic variables to invoice 5. <code>DescriptionUpdater</code> - Update descriptions with features 6. <code>MetadataValidator</code> - Validate metadata files 7. <code>InvoiceValidator</code> - Validate invoice files</p>"},{"location":"rdetoolkit/processing/factories/#smarttableinvoicepipelinebuilder","title":"SmartTableInvoicePipelineBuilder","text":"<p>Builder for SmartTableInvoice mode pipelines.</p> <pre><code>def build(self) -&gt; Pipeline\n</code></pre> <p>Pipeline Configuration: 1. <code>SmartTableInvoiceInitializer</code> - Initialize invoice from SmartTable data 2. <code>SmartTableFileCopier</code> - Copy files excluding SmartTable CSVs 3. <code>DatasetRunner</code> - Execute custom dataset processing 4. <code>ThumbnailGenerator</code> - Generate thumbnail images 5. <code>VariableApplier</code> - Apply magic variables to invoice 6. <code>DescriptionUpdater</code> - Update descriptions with features 7. <code>MetadataValidator</code> - Validate metadata files 8. <code>InvoiceValidator</code> - Validate invoice files</p>"},{"location":"rdetoolkit/processing/factories/#pipelinefactory","title":"PipelineFactory","text":"<p>Factory for creating predefined processing pipelines with Pythonic design.</p> <pre><code>class PipelineFactory:\n    \"\"\"Factory for creating predefined processing pipelines.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#class-methods","title":"Class Methods","text":""},{"location":"rdetoolkit/processing/factories/#create_pipeline","title":"create_pipeline","text":"<p>Create a pipeline for the specified mode.</p> <pre><code>@classmethod\ndef create_pipeline(cls, mode: str | ProcessingMode) -&gt; Pipeline\n</code></pre> <p>Parameters: - <code>mode</code> (str | ProcessingMode): Processing mode (string or ProcessingMode enum)</p> <p>Returns: - <code>Pipeline</code>: Pipeline configured for the specified mode</p> <p>Raises: - <code>ValueError</code>: If mode is not supported</p> <p>Example: <pre><code>from rdetoolkit.processing.factories import PipelineFactory\n\n# Using string mode\npipeline = PipelineFactory.create_pipeline(\"invoice\")\n\n# Using enum mode\nfrom rdetoolkit.processing.factories import ProcessingMode\npipeline = PipelineFactory.create_pipeline(ProcessingMode.INVOICE)\n</code></pre></p>"},{"location":"rdetoolkit/processing/factories/#get_supported_modes","title":"get_supported_modes","text":"<p>Get list of supported mode names.</p> <pre><code>@classmethod\ndef get_supported_modes(cls) -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of supported mode strings</p> <p>Example: <pre><code>modes = PipelineFactory.get_supported_modes()\nprint(f\"Supported modes: {', '.join(modes)}\")\n# Output: Supported modes: rdeformat, multidatatile, excelinvoice, invoice, smarttableinvoice\n</code></pre></p>"},{"location":"rdetoolkit/processing/factories/#static-methods-backward-compatibility","title":"Static Methods (Backward Compatibility)","text":""},{"location":"rdetoolkit/processing/factories/#create_rdeformat_pipeline","title":"create_rdeformat_pipeline","text":"<p>Create a pipeline for RDEFormat mode processing.</p> <pre><code>@staticmethod\ndef create_rdeformat_pipeline() -&gt; Pipeline\n</code></pre> <p>Returns: - <code>Pipeline</code>: Pipeline configured for RDEFormat mode</p>"},{"location":"rdetoolkit/processing/factories/#create_multifile_pipeline","title":"create_multifile_pipeline","text":"<p>Create a pipeline for MultiFile mode processing.</p> <pre><code>@staticmethod\ndef create_multifile_pipeline() -&gt; Pipeline\n</code></pre> <p>Returns: - <code>Pipeline</code>: Pipeline configured for MultiFile mode</p>"},{"location":"rdetoolkit/processing/factories/#create_excel_pipeline","title":"create_excel_pipeline","text":"<p>Create a pipeline for ExcelInvoice mode processing.</p> <pre><code>@staticmethod\ndef create_excel_pipeline() -&gt; Pipeline\n</code></pre> <p>Returns: - <code>Pipeline</code>: Pipeline configured for ExcelInvoice mode</p>"},{"location":"rdetoolkit/processing/factories/#create_invoice_pipeline","title":"create_invoice_pipeline","text":"<p>Create a pipeline for Invoice mode processing.</p> <pre><code>@staticmethod\ndef create_invoice_pipeline() -&gt; Pipeline\n</code></pre> <p>Returns: - <code>Pipeline</code>: Pipeline configured for Invoice mode</p>"},{"location":"rdetoolkit/processing/factories/#create_smarttable_invoice_pipeline","title":"create_smarttable_invoice_pipeline","text":"<p>Create a pipeline for SmartTableInvoice mode processing.</p> <pre><code>@staticmethod\ndef create_smarttable_invoice_pipeline() -&gt; Pipeline\n</code></pre> <p>Returns: - <code>Pipeline</code>: Pipeline configured for SmartTableInvoice mode</p>"},{"location":"rdetoolkit/processing/factories/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/factories/#basic-pipeline-creation","title":"Basic Pipeline Creation","text":"<pre><code>from rdetoolkit.processing.factories import PipelineFactory\n\n# Create pipelines for different modes\ninvoice_pipeline = PipelineFactory.create_pipeline(\"invoice\")\nexcel_pipeline = PipelineFactory.create_pipeline(\"excelinvoice\")\nsmarttable_pipeline = PipelineFactory.create_pipeline(\"smarttableinvoice\")\n\n# Execute pipeline\nresult = invoice_pipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#dynamic-mode-selection","title":"Dynamic Mode Selection","text":"<pre><code>from rdetoolkit.processing.factories import PipelineFactory\n\ndef create_pipeline_for_mode(mode_name: str):\n    \"\"\"Create pipeline based on mode name with validation.\"\"\"\n\n    supported_modes = PipelineFactory.get_supported_modes()\n\n    if mode_name.lower() not in supported_modes:\n        raise ValueError(f\"Unsupported mode: {mode_name}. Supported: {supported_modes}\")\n\n    return PipelineFactory.create_pipeline(mode_name.lower())\n\n# Usage\ntry:\n    pipeline = create_pipeline_for_mode(\"Invoice\")\n    print(f\"Created pipeline with {pipeline.get_processor_count()} processors\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#mode-based-processing","title":"Mode-Based Processing","text":"<pre><code>from rdetoolkit.processing.factories import PipelineFactory, ProcessingMode\n\ndef process_data_by_mode(context, mode_name: str):\n    \"\"\"Process data using the appropriate pipeline for the mode.\"\"\"\n\n    # Convert string to enum for validation\n    try:\n        mode = ProcessingMode(mode_name.lower())\n    except ValueError:\n        supported = [m.value for m in ProcessingMode]\n        raise ValueError(f\"Unsupported mode: {mode_name}. Supported: {supported}\")\n\n    # Create and execute pipeline\n    pipeline = PipelineFactory.create_pipeline(mode)\n\n    print(f\"Processing in {mode.value} mode\")\n    print(f\"Pipeline processors: {pipeline.get_processor_names()}\")\n\n    result = pipeline.execute(context)\n    return result\n\n# Usage\nresult = process_data_by_mode(context, \"invoice\")\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#custom-builder-implementation","title":"Custom Builder Implementation","text":"<pre><code>from rdetoolkit.processing.factories import PipelineBuilder\nfrom rdetoolkit.processing import Pipeline\nfrom rdetoolkit.processing.processors import *\n\nclass CustomPipelineBuilder(PipelineBuilder):\n    \"\"\"Custom builder for specialized processing.\"\"\"\n\n    def build(self) -&gt; Pipeline:\n        \"\"\"Build custom pipeline with specific processors.\"\"\"\n        return (self._create_base_pipeline()\n                .add(FileCopier())\n                .add(CustomDataProcessor())  # Custom processor\n                .add(DatasetRunner())\n                .add(ThumbnailGenerator())\n                .add(InvoiceValidator()))\n\n# Use custom builder\nbuilder = CustomPipelineBuilder()\ncustom_pipeline = builder.build()\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#pipeline-inspection","title":"Pipeline Inspection","text":"<pre><code>from rdetoolkit.processing.factories import PipelineFactory\n\ndef inspect_pipeline(mode: str):\n    \"\"\"Inspect pipeline configuration for a given mode.\"\"\"\n\n    pipeline = PipelineFactory.create_pipeline(mode)\n\n    print(f\"Pipeline for {mode} mode:\")\n    print(f\"  Processor count: {pipeline.get_processor_count()}\")\n    print(f\"  Processors:\")\n\n    for i, name in enumerate(pipeline.get_processor_names(), 1):\n        print(f\"    {i}. {name}\")\n\n# Inspect all supported modes\nfor mode in PipelineFactory.get_supported_modes():\n    inspect_pipeline(mode)\n    print()\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#backward-compatibility-usage","title":"Backward Compatibility Usage","text":"<pre><code>from rdetoolkit.processing.factories import PipelineFactory\n\n# Legacy method calls (backward compatibility)\nrde_pipeline = PipelineFactory.create_rdeformat_pipeline()\nmulti_pipeline = PipelineFactory.create_multifile_pipeline()\nexcel_pipeline = PipelineFactory.create_excel_pipeline()\ninvoice_pipeline = PipelineFactory.create_invoice_pipeline()\nsmarttable_pipeline = PipelineFactory.create_smarttable_invoice_pipeline()\n\n# Modern method calls (recommended)\nrde_pipeline = PipelineFactory.create_pipeline(\"rdeformat\")\nmulti_pipeline = PipelineFactory.create_pipeline(\"multidatatile\")\nexcel_pipeline = PipelineFactory.create_pipeline(\"excelinvoice\")\ninvoice_pipeline = PipelineFactory.create_pipeline(\"invoice\")\nsmarttable_pipeline = PipelineFactory.create_pipeline(\"smarttableinvoice\")\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#error-handling","title":"Error Handling","text":"<pre><code>from rdetoolkit.processing.factories import PipelineFactory\n\ndef safe_pipeline_creation(mode: str):\n    \"\"\"Safely create pipeline with comprehensive error handling.\"\"\"\n\n    try:\n        # Validate mode\n        supported_modes = PipelineFactory.get_supported_modes()\n        if mode.lower() not in supported_modes:\n            print(f\"Error: '{mode}' is not supported\")\n            print(f\"Supported modes: {', '.join(supported_modes)}\")\n            return None\n\n        # Create pipeline\n        pipeline = PipelineFactory.create_pipeline(mode)\n\n        print(f\"\u2713 Successfully created {mode} pipeline\")\n        print(f\"  Processors: {', '.join(pipeline.get_processor_names())}\")\n\n        return pipeline\n\n    except ValueError as e:\n        print(f\"\u2717 Value error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"\u2717 Unexpected error: {e}\")\n        return None\n\n# Usage\npipeline = safe_pipeline_creation(\"invoice\")\nif pipeline:\n    result = pipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#extending-the-factory","title":"Extending the Factory","text":""},{"location":"rdetoolkit/processing/factories/#adding-new-modes","title":"Adding New Modes","text":"<pre><code>from rdetoolkit.processing.factories import PipelineBuilder, PipelineFactory\nfrom rdetoolkit.processing import Pipeline\nfrom enum import Enum\n\n# 1. Extend ProcessingMode enum\nclass ExtendedProcessingMode(Enum):\n    RDEFORMAT = \"rdeformat\"\n    MULTIDATATILE = \"multidatatile\"\n    EXCELINVOICE = \"excelinvoice\"\n    INVOICE = \"invoice\"\n    SMARTTABLEINVOICE = \"smarttableinvoice\"\n    CUSTOM_MODE = \"custommode\"  # New mode\n\n# 2. Create custom builder\nclass CustomModePipelineBuilder(PipelineBuilder):\n    def build(self) -&gt; Pipeline:\n        return (self._create_base_pipeline()\n                .add(CustomProcessor())\n                .add(DatasetRunner())\n                .add(InvoiceValidator()))\n\n# 3. Extend factory (would require modifying the original class)\n# This is an example of how you might extend the factory\nclass ExtendedPipelineFactory(PipelineFactory):\n    _builders = {\n        **PipelineFactory._builders,\n        ExtendedProcessingMode.CUSTOM_MODE: CustomModePipelineBuilder,\n    }\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#custom-factory-pattern","title":"Custom Factory Pattern","text":"<pre><code>from typing import Dict, Type\nfrom rdetoolkit.processing.factories import PipelineBuilder\nfrom rdetoolkit.processing import Pipeline\n\nclass CustomPipelineFactory:\n    \"\"\"Custom factory for application-specific pipelines.\"\"\"\n\n    def __init__(self):\n        self._builders: Dict[str, Type[PipelineBuilder]] = {}\n\n    def register_builder(self, mode: str, builder_class: Type[PipelineBuilder]):\n        \"\"\"Register a new builder for a mode.\"\"\"\n        self._builders[mode.lower()] = builder_class\n\n    def create_pipeline(self, mode: str) -&gt; Pipeline:\n        \"\"\"Create pipeline for the specified mode.\"\"\"\n        builder_class = self._builders.get(mode.lower())\n        if not builder_class:\n            raise ValueError(f\"No builder registered for mode: {mode}\")\n\n        builder = builder_class()\n        return builder.build()\n\n    def get_supported_modes(self) -&gt; list[str]:\n        \"\"\"Get list of supported modes.\"\"\"\n        return list(self._builders.keys())\n\n# Usage\nfactory = CustomPipelineFactory()\nfactory.register_builder(\"custom\", CustomModePipelineBuilder)\npipeline = factory.create_pipeline(\"custom\")\n</code></pre>"},{"location":"rdetoolkit/processing/factories/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Lazy Instantiation: Builders create processors only when needed</li> <li>Memory Efficiency: Pipelines are lightweight objects</li> <li>Caching: Consider caching pipelines for repeated use</li> <li>Thread Safety: Factory methods are thread-safe</li> </ul>"},{"location":"rdetoolkit/processing/factories/#see-also","title":"See Also","text":"<ul> <li>Pipeline Architecture - Core pipeline and processor classes</li> <li>Processing Context - Context management and state</li> <li>Processors - Individual processor implementations</li> <li>Processing Module - Main processing module overview</li> </ul>"},{"location":"rdetoolkit/processing/pipeline/","title":"Pipeline Architecture","text":"<p>The <code>rdetoolkit.processing.pipeline</code> module provides the core pipeline architecture for sequential processing operations. It defines the base classes and interfaces for building modular, extensible processing workflows.</p>"},{"location":"rdetoolkit/processing/pipeline/#overview","title":"Overview","text":"<p>The pipeline architecture enables:</p> <ul> <li>Sequential Execution: Processors execute in defined order</li> <li>Shared Context: All processors share the same processing context</li> <li>Error Handling: Comprehensive error handling and logging</li> <li>Extensibility: Easy addition of new processing steps</li> <li>Status Reporting: Detailed execution results and error reporting</li> </ul>"},{"location":"rdetoolkit/processing/pipeline/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/pipeline/#processor","title":"Processor","text":"<p>Abstract base class for all processing operations. Each processor represents a single step in the processing pipeline.</p> <pre><code>class Processor(ABC):\n    \"\"\"Abstract base class for processing operations.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/pipeline/#process","title":"process","text":"<p>Execute the processing operation.</p> <pre><code>@abstractmethod\ndef process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): The processing context containing all necessary information</p> <p>Raises: - Any exceptions that occur during processing should be allowed to propagate unless the processor is designed to handle specific error conditions</p> <p>Example: <pre><code>class CustomProcessor(Processor):\n    def process(self, context: ProcessingContext) -&gt; None:\n        # Access resources from context\n        raw_files = context.resource_paths.rawfiles\n        output_dir = context.resource_paths.struct\n\n        # Perform processing operations\n        for file in raw_files:\n            process_file(file, output_dir)\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#get_name","title":"get_name","text":"<p>Get the name of this processor for logging purposes.</p> <pre><code>def get_name(self) -&gt; str\n</code></pre> <p>Returns: - <code>str</code>: The class name of the processor</p> <p>Example: <pre><code>processor = CustomProcessor()\nprint(processor.get_name())  # \"CustomProcessor\"\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#pipeline","title":"Pipeline","text":"<p>Pipeline for executing a sequence of processing operations. The pipeline executes processors in the order they were added, passing the same context to each processor.</p> <pre><code>class Pipeline:\n    \"\"\"Pipeline for executing a sequence of processing operations.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None\n</code></pre> <p>Initialize an empty pipeline.</p> <p>Example: <pre><code>pipeline = Pipeline()\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/processing/pipeline/#add","title":"add","text":"<p>Add a processor to the pipeline.</p> <pre><code>def add(self, processor: Processor) -&gt; Pipeline\n</code></pre> <p>Parameters: - <code>processor</code> (Processor): The processor to add</p> <p>Returns: - <code>Pipeline</code>: Self for method chaining</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import FileCopier, DatasetRunner\n\npipeline = (Pipeline()\n    .add(FileCopier())\n    .add(DatasetRunner()))\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#execute","title":"execute","text":"<p>Execute all processors in the pipeline.</p> <pre><code>def execute(self, context: ProcessingContext) -&gt; WorkflowExecutionStatus\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): The processing context</p> <p>Returns: - <code>WorkflowExecutionStatus</code>: Status indicating success or failure</p> <p>Raises: - Any exceptions from processors will propagate unless handled</p> <p>Example: <pre><code>try:\n    result = pipeline.execute(context)\n    if result.status == \"success\":\n        print(f\"Pipeline completed: {result.title}\")\n    else:\n        print(f\"Pipeline failed: {result.error_message}\")\nexcept Exception as e:\n    print(f\"Pipeline execution error: {e}\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#get_processor_count","title":"get_processor_count","text":"<p>Get the number of processors in this pipeline.</p> <pre><code>def get_processor_count(self) -&gt; int\n</code></pre> <p>Returns: - <code>int</code>: Number of processors in the pipeline</p> <p>Example: <pre><code>count = pipeline.get_processor_count()\nprint(f\"Pipeline has {count} processors\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#get_processor_names","title":"get_processor_names","text":"<p>Get the names of all processors in this pipeline.</p> <pre><code>def get_processor_names(self) -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of processor names</p> <p>Example: <pre><code>names = pipeline.get_processor_names()\nprint(f\"Processors: {', '.join(names)}\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/pipeline/#creating-a-custom-processor","title":"Creating a Custom Processor","text":"<pre><code>from rdetoolkit.processing.pipeline import Processor\nfrom rdetoolkit.processing.context import ProcessingContext\nimport shutil\n\nclass FileBackupProcessor(Processor):\n    \"\"\"Processor that creates backups of raw files.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        backup_dir = context.resource_paths.raw.parent / \"backup\"\n        backup_dir.mkdir(exist_ok=True)\n\n        for raw_file in context.resource_paths.rawfiles:\n            backup_path = backup_dir / raw_file.name\n            shutil.copy2(raw_file, backup_path)\n\n        print(f\"Backed up {len(context.resource_paths.rawfiles)} files\")\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#building-a-custom-pipeline","title":"Building a Custom Pipeline","text":"<pre><code>from rdetoolkit.processing import Pipeline\nfrom rdetoolkit.processing.processors import (\n    FileCopier, DatasetRunner, ThumbnailGenerator, InvoiceValidator\n)\n\n# Create custom pipeline\npipeline = (Pipeline()\n    .add(FileBackupProcessor())  # Custom processor\n    .add(FileCopier())           # Standard processor\n    .add(DatasetRunner())        # Standard processor\n    .add(ThumbnailGenerator())   # Standard processor\n    .add(InvoiceValidator()))    # Standard processor\n\nprint(f\"Pipeline has {pipeline.get_processor_count()} processors\")\nprint(f\"Processors: {pipeline.get_processor_names()}\")\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#conditional-processing","title":"Conditional Processing","text":"<pre><code>class ConditionalProcessor(Processor):\n    \"\"\"Processor that performs different operations based on context.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        config = context.srcpaths.config\n\n        if context.is_excel_mode:\n            self._process_excel_mode(context)\n        elif context.is_smarttable_mode:\n            self._process_smarttable_mode(context)\n        else:\n            self._process_standard_mode(context)\n\n    def _process_excel_mode(self, context: ProcessingContext) -&gt; None:\n        excel_file = context.excel_invoice_file\n        print(f\"Processing Excel file: {excel_file}\")\n        # Excel-specific processing\n\n    def _process_smarttable_mode(self, context: ProcessingContext) -&gt; None:\n        smarttable_file = context.smarttable_invoice_file\n        print(f\"Processing SmartTable file: {smarttable_file}\")\n        # SmartTable-specific processing\n\n    def _process_standard_mode(self, context: ProcessingContext) -&gt; None:\n        print(\"Processing in standard mode\")\n        # Standard processing\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#error-handling-in-processors","title":"Error Handling in Processors","text":"<pre><code>class RobustProcessor(Processor):\n    \"\"\"Processor with comprehensive error handling.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        try:\n            self._do_processing(context)\n        except FileNotFoundError as e:\n            # Handle specific errors gracefully\n            print(f\"Warning: File not found, skipping: {e}\")\n        except Exception as e:\n            # Log error and re-raise for pipeline to handle\n            print(f\"Error in {self.get_name()}: {e}\")\n            raise\n\n    def _do_processing(self, context: ProcessingContext) -&gt; None:\n        # Actual processing logic\n        pass\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#pipeline-execution-with-status-handling","title":"Pipeline Execution with Status Handling","text":"<pre><code>def execute_pipeline_with_monitoring(pipeline: Pipeline, context: ProcessingContext):\n    \"\"\"Execute pipeline with detailed status monitoring.\"\"\"\n\n    print(f\"Starting pipeline with {pipeline.get_processor_count()} processors\")\n    print(f\"Processors: {', '.join(pipeline.get_processor_names())}\")\n\n    try:\n        result = pipeline.execute(context)\n\n        if result.status == \"success\":\n            print(f\"\u2713 Pipeline completed successfully\")\n            print(f\"  Title: {result.title}\")\n            print(f\"  Mode: {result.mode}\")\n            print(f\"  Target: {result.target}\")\n        else:\n            print(f\"\u2717 Pipeline failed\")\n            print(f\"  Error Code: {result.error_code}\")\n            print(f\"  Error Message: {result.error_message}\")\n            if result.stacktrace:\n                print(f\"  Stack Trace: {result.stacktrace}\")\n\n        return result\n\n    except Exception as e:\n        print(f\"\u2717 Pipeline execution failed with exception: {e}\")\n        raise\n\n# Usage\npipeline = Pipeline().add(FileCopier()).add(DatasetRunner())\nresult = execute_pipeline_with_monitoring(pipeline, context)\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#parallel-processing-design","title":"Parallel Processing Design","text":"<pre><code>class ParallelFileProcessor(Processor):\n    \"\"\"Processor designed for parallel execution.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        import concurrent.futures\n\n        raw_files = context.resource_paths.rawfiles\n        output_dir = context.resource_paths.struct\n\n        # Process files in parallel\n        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n            futures = [\n                executor.submit(self._process_single_file, file, output_dir)\n                for file in raw_files\n            ]\n\n            # Wait for all tasks to complete\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    future.result()\n                except Exception as e:\n                    print(f\"Error processing file: {e}\")\n                    raise\n\n    def _process_single_file(self, file_path, output_dir):\n        \"\"\"Process a single file (thread-safe).\"\"\"\n        # File processing logic here\n        pass\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#design-patterns","title":"Design Patterns","text":""},{"location":"rdetoolkit/processing/pipeline/#template-method-pattern","title":"Template Method Pattern","text":"<pre><code>class TemplateProcessor(Processor):\n    \"\"\"Template processor with customizable steps.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        self._pre_process(context)\n        self._main_process(context)\n        self._post_process(context)\n\n    def _pre_process(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Override in subclasses for pre-processing.\"\"\"\n        pass\n\n    def _main_process(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Override in subclasses for main processing.\"\"\"\n        raise NotImplementedError\n\n    def _post_process(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Override in subclasses for post-processing.\"\"\"\n        pass\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#strategy-pattern","title":"Strategy Pattern","text":"<pre><code>class StrategyProcessor(Processor):\n    \"\"\"Processor using strategy pattern for different operations.\"\"\"\n\n    def __init__(self, strategy_name: str):\n        self._strategy_name = strategy_name\n        self._strategies = {\n            \"compress\": self._compress_strategy,\n            \"encrypt\": self._encrypt_strategy,\n            \"archive\": self._archive_strategy,\n        }\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        strategy = self._strategies.get(self._strategy_name)\n        if not strategy:\n            raise ValueError(f\"Unknown strategy: {self._strategy_name}\")\n\n        strategy(context)\n\n    def _compress_strategy(self, context: ProcessingContext) -&gt; None:\n        # Compression logic\n        pass\n\n    def _encrypt_strategy(self, context: ProcessingContext) -&gt; None:\n        # Encryption logic\n        pass\n\n    def _archive_strategy(self, context: ProcessingContext) -&gt; None:\n        # Archiving logic\n        pass\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/pipeline/#pipeline-error-handling","title":"Pipeline Error Handling","text":"<p>The pipeline provides comprehensive error handling:</p> <ol> <li>Processor Errors: Caught and converted to StructuredError</li> <li>Status Creation: Automatic creation of success/failure status</li> <li>Logging: Detailed logging of all operations</li> <li>Exception Propagation: Controlled exception handling</li> </ol>"},{"location":"rdetoolkit/processing/pipeline/#custom-error-handling","title":"Custom Error Handling","text":"<pre><code>class ErrorHandlingProcessor(Processor):\n    \"\"\"Processor with custom error handling strategies.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        try:\n            self._risky_operation(context)\n        except FileNotFoundError:\n            # Graceful degradation\n            self._fallback_operation(context)\n        except PermissionError as e:\n            # Critical error - re-raise\n            raise StructuredError(\n                emsg=f\"Permission denied: {e}\",\n                ecode=403,\n                eobj=e\n            )\n        except Exception as e:\n            # Log and re-raise\n            logger.error(f\"Unexpected error in {self.get_name()}: {e}\")\n            raise\n</code></pre>"},{"location":"rdetoolkit/processing/pipeline/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Stateless Design: Processors should be stateless for thread safety</li> <li>Resource Management: Proper cleanup of resources in processors</li> <li>Memory Usage: Avoid loading large files entirely into memory</li> <li>I/O Operations: Use async operations for I/O-heavy processors</li> <li>Caching: Cache expensive operations where appropriate</li> </ul>"},{"location":"rdetoolkit/processing/pipeline/#see-also","title":"See Also","text":"<ul> <li>Processing Context - Context management and state</li> <li>Pipeline Factory - Automated pipeline creation</li> <li>Processors - Individual processor implementations</li> <li>Models - Status and result data structures</li> </ul>"},{"location":"rdetoolkit/processing/processors/","title":"Processors","text":"<p>The <code>rdetoolkit.processing.processors</code> module provides individual processor implementations for the processing pipeline. Each processor handles a specific aspect of the data processing workflow.</p>"},{"location":"rdetoolkit/processing/processors/#overview","title":"Overview","text":"<p>The processors module contains specialized processing components:</p> <ul> <li>File Operations: File copying and management</li> <li>Invoice Processing: Invoice initialization and validation</li> <li>Data Processing: Custom dataset execution and variable application</li> <li>Image Processing: Thumbnail generation</li> <li>Validation: Metadata and invoice validation</li> <li>Content Updates: Description and feature updates</li> </ul>"},{"location":"rdetoolkit/processing/processors/#processor-classes","title":"Processor Classes","text":""},{"location":"rdetoolkit/processing/processors/#file-processing","title":"File Processing","text":"<ul> <li>FileCopier: Standard file copying for raw files</li> <li>RDEFormatFileCopier: Specialized copying for RDEFormat mode</li> <li>SmartTableFileCopier: Specialized copying for SmartTable mode</li> </ul>"},{"location":"rdetoolkit/processing/processors/#invoice-processing","title":"Invoice Processing","text":"<ul> <li>StandardInvoiceInitializer: Standard invoice initialization</li> <li>ExcelInvoiceInitializer: Excel-based invoice initialization</li> <li>SmartTableInvoiceInitializer: SmartTable-based invoice initialization</li> <li>InvoiceInitializerFactory: Factory for creating appropriate initializers</li> </ul>"},{"location":"rdetoolkit/processing/processors/#data-processing","title":"Data Processing","text":"<ul> <li>DatasetRunner: Executes custom dataset processing functions</li> <li>VariableApplier: Applies magic variable replacement</li> </ul>"},{"location":"rdetoolkit/processing/processors/#content-processing","title":"Content Processing","text":"<ul> <li>DescriptionUpdater: Updates descriptions with feature information</li> <li>ThumbnailGenerator: Generates thumbnail images</li> </ul>"},{"location":"rdetoolkit/processing/processors/#validation","title":"Validation","text":"<ul> <li>InvoiceValidator: Validates invoice files against schema</li> <li>MetadataValidator: Validates metadata files against schema</li> </ul>"},{"location":"rdetoolkit/processing/processors/#common-patterns","title":"Common Patterns","text":"<p>All processors follow these design patterns:</p>"},{"location":"rdetoolkit/processing/processors/#base-processor-interface","title":"Base Processor Interface","text":"<pre><code>from rdetoolkit.processing.pipeline import Processor\nfrom rdetoolkit.processing.context import ProcessingContext\n\nclass ExampleProcessor(Processor):\n    def process(self, context: ProcessingContext) -&gt; None:\n        # Processing logic here\n        pass\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#configuration-based-processing","title":"Configuration-Based Processing","text":"<pre><code>def process(self, context: ProcessingContext) -&gt; None:\n    config = context.srcpaths.config\n\n    if not config.some_feature_enabled:\n        return  # Skip processing if disabled\n\n    # Perform processing\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#error-handling","title":"Error Handling","text":"<pre><code>def process(self, context: ProcessingContext) -&gt; None:\n    try:\n        # Main processing logic\n        self._do_processing(context)\n    except SpecificError:\n        # Handle specific errors gracefully\n        logger.warning(\"Specific error occurred, continuing...\")\n    except Exception as e:\n        # Critical errors should propagate\n        logger.error(f\"Critical error in {self.get_name()}: {e}\")\n        raise\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#resource-management","title":"Resource Management","text":"<pre><code>def process(self, context: ProcessingContext) -&gt; None:\n    # Access resources from context\n    input_files = context.resource_paths.rawfiles\n    output_dir = context.resource_paths.struct\n\n    # Create output directory if needed\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Process files\n    for file in input_files:\n        self._process_file(file, output_dir)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/#basic-processor-usage","title":"Basic Processor Usage","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier, DatasetRunner\n\n# Create processors\nfile_copier = FileCopier()\ndataset_runner = DatasetRunner()\n\n# Execute processors with context\nfile_copier.process(context)\ndataset_runner.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>from rdetoolkit.processing import Pipeline\nfrom rdetoolkit.processing.processors import (\n    FileCopier, DatasetRunner, ThumbnailGenerator, InvoiceValidator\n)\n\n# Build pipeline with processors\npipeline = (Pipeline()\n    .add(FileCopier())\n    .add(DatasetRunner())\n    .add(ThumbnailGenerator())\n    .add(InvoiceValidator()))\n\n# Execute pipeline\nresult = pipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#custom-processor-implementation","title":"Custom Processor Implementation","text":"<pre><code>from rdetoolkit.processing.pipeline import Processor\nfrom rdetoolkit.processing.context import ProcessingContext\nimport json\n\nclass MetadataProcessor(Processor):\n    \"\"\"Custom processor that creates metadata files.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        metadata = {\n            \"processing_mode\": context.mode_name,\n            \"file_count\": len(context.resource_paths.rawfiles),\n            \"timestamp\": self._get_timestamp(),\n        }\n\n        metadata_file = context.metadata_path\n        metadata_file.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(metadata_file, 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n    def _get_timestamp(self) -&gt; str:\n        from datetime import datetime\n        return datetime.now().isoformat()\n\n# Use custom processor\ncustom_processor = MetadataProcessor()\ncustom_processor.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#conditional-processing","title":"Conditional Processing","text":"<pre><code>from rdetoolkit.processing.processors import ThumbnailGenerator\n\nclass ConditionalThumbnailGenerator(ThumbnailGenerator):\n    \"\"\"Thumbnail generator with additional conditions.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        config = context.srcpaths.config\n\n        # Check if thumbnails are enabled\n        if not config.save_thumbnail_image:\n            return\n\n        # Check if we have image files\n        image_files = [f for f in context.resource_paths.rawfiles\n                      if f.suffix.lower() in ['.jpg', '.png', '.tiff']]\n\n        if not image_files:\n            return  # No images to process\n\n        # Call parent implementation\n        super().process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#processor-factory-pattern","title":"Processor Factory Pattern","text":"<pre><code>from typing import Dict, Type\nfrom rdetoolkit.processing.pipeline import Processor\n\nclass ProcessorFactory:\n    \"\"\"Factory for creating processors based on configuration.\"\"\"\n\n    _processors: Dict[str, Type[Processor]] = {\n        'files': FileCopier,\n        'datasets': DatasetRunner,\n        'thumbnails': ThumbnailGenerator,\n        'validation': InvoiceValidator,\n    }\n\n    @classmethod\n    def create_processor(cls, processor_type: str) -&gt; Processor:\n        processor_class = cls._processors.get(processor_type)\n        if not processor_class:\n            raise ValueError(f\"Unknown processor type: {processor_type}\")\n        return processor_class()\n\n    @classmethod\n    def create_processors(cls, processor_types: list[str]) -&gt; list[Processor]:\n        return [cls.create_processor(ptype) for ptype in processor_types]\n\n# Usage\nprocessors = ProcessorFactory.create_processors(['files', 'datasets', 'validation'])\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#performance-considerations","title":"Performance Considerations","text":""},{"location":"rdetoolkit/processing/processors/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code>def process(self, context: ProcessingContext) -&gt; None:\n    # Process files one at a time instead of loading all into memory\n    for raw_file in context.resource_paths.rawfiles:\n        self._process_single_file(raw_file, context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#io-optimization","title":"I/O Optimization","text":"<pre><code>def process(self, context: ProcessingContext) -&gt; None:\n    # Batch I/O operations\n    operations = self._prepare_operations(context)\n    self._execute_batch_operations(operations)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#error-recovery","title":"Error Recovery","text":"<pre><code>def process(self, context: ProcessingContext) -&gt; None:\n    failed_files = []\n\n    for raw_file in context.resource_paths.rawfiles:\n        try:\n            self._process_file(raw_file, context)\n        except Exception as e:\n            failed_files.append((raw_file, str(e)))\n            logger.warning(f\"Failed to process {raw_file}: {e}\")\n\n    if failed_files:\n        # Report failed files but don't stop processing\n        logger.info(f\"Completed with {len(failed_files)} failures\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#testing-processors","title":"Testing Processors","text":""},{"location":"rdetoolkit/processing/processors/#unit-testing","title":"Unit Testing","text":"<pre><code>import unittest\nfrom unittest.mock import Mock, patch\nfrom rdetoolkit.processing.processors import FileCopier\n\nclass TestFileCopier(unittest.TestCase):\n    def setUp(self):\n        self.processor = FileCopier()\n        self.context = Mock()\n        # Setup mock context attributes\n\n    def test_process_success(self):\n        # Test successful processing\n        self.processor.process(self.context)\n        # Assert expected behavior\n\n    def test_process_error_handling(self):\n        # Test error handling\n        with self.assertRaises(SpecificError):\n            self.processor.process(self.context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#integration-testing","title":"Integration Testing","text":"<pre><code>from rdetoolkit.processing import Pipeline\nfrom rdetoolkit.processing.processors import FileCopier, DatasetRunner\n\ndef test_processor_integration():\n    # Create test context\n    context = create_test_context()\n\n    # Create pipeline with processors\n    pipeline = Pipeline().add(FileCopier()).add(DatasetRunner())\n\n    # Execute and verify results\n    result = pipeline.execute(context)\n    assert result.status == \"success\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/#see-also","title":"See Also","text":"<ul> <li>File Processors - File copying and management</li> <li>Invoice Processors - Invoice initialization and handling</li> <li>Dataset Processors - Custom dataset processing</li> <li>Validation Processors - Data validation</li> <li>Other Processors - Variable application and content updates</li> </ul>"},{"location":"rdetoolkit/processing/processors/datasets/","title":"Dataset Processor","text":"<p>The dataset processor executes custom dataset processing functions as part of the processing pipeline.</p>"},{"location":"rdetoolkit/processing/processors/datasets/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/datasets/#datasetrunner","title":"DatasetRunner","text":"<p>Executes user-provided custom dataset processing functions with comprehensive error handling and logging.</p> <pre><code>class DatasetRunner(Processor):\n    \"\"\"Executes custom dataset processing functions.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/datasets/#process","title":"process","text":"<p>Execute custom dataset processing function if provided.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing dataset function and resources</p> <p>Behavior: - Checks if a custom dataset function is provided in the context - Executes the function with input and output path parameters - Handles function execution errors gracefully with logging - Skips processing if no custom function is provided</p> <p>Custom Function Signature: The custom dataset function should accept two parameters: - <code>srcpaths</code> (RdeInputDirPaths): Input directory paths and configuration - <code>resource_paths</code> (RdeOutputResourcePath): Output resource paths</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import DatasetRunner\n\n# Create and execute dataset runner\ndataset_runner = DatasetRunner()\ndataset_runner.process(context)\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/datasets/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/datasets/#basic-dataset-function","title":"Basic Dataset Function","text":"<pre><code>from rdetoolkit.processing.processors import DatasetRunner\nfrom rdetoolkit.processing.context import ProcessingContext\nfrom rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\n\ndef simple_dataset_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Simple dataset processing function.\"\"\"\n    print(f\"Processing {len(resource_paths.rawfiles)} files\")\n\n    # Process each raw file\n    for raw_file in resource_paths.rawfiles:\n        print(f\"Processing file: {raw_file.name}\")\n\n        # Copy file to structured directory (example processing)\n        structured_file = resource_paths.struct / raw_file.name\n        resource_paths.struct.mkdir(parents=True, exist_ok=True)\n\n        import shutil\n        shutil.copy2(raw_file, structured_file)\n        print(f\"Copied to structured: {structured_file}\")\n\n# Create context with custom function\ncontext = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=simple_dataset_function,\n    mode_name=\"Invoice\"\n)\n\n# Execute dataset processing\ndataset_runner = DatasetRunner()\ndataset_runner.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#advanced-dataset-processing","title":"Advanced Dataset Processing","text":"<pre><code>from rdetoolkit.models.rde2types import RdeInputDirPaths, RdeOutputResourcePath\nimport json\nimport pandas as pd\nfrom pathlib import Path\n\ndef advanced_dataset_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Advanced dataset processing with multiple file types.\"\"\"\n\n    config = srcpaths.config\n    print(f\"Processing in {config.system.extended_mode or 'standard'} mode\")\n\n    # Process different file types\n    csv_files = [f for f in resource_paths.rawfiles if f.suffix == '.csv']\n    image_files = [f for f in resource_paths.rawfiles if f.suffix.lower() in ['.jpg', '.png', '.tiff']]\n    other_files = [f for f in resource_paths.rawfiles if f not in csv_files + image_files]\n\n    # Process CSV files\n    if csv_files:\n        process_csv_files(csv_files, resource_paths)\n\n    # Process image files\n    if image_files:\n        process_image_files(image_files, resource_paths, config)\n\n    # Process other files\n    if other_files:\n        process_other_files(other_files, resource_paths)\n\n    # Generate processing summary\n    generate_processing_summary(resource_paths, len(csv_files), len(image_files), len(other_files))\n\ndef process_csv_files(csv_files: list[Path], resource_paths: RdeOutputResourcePath):\n    \"\"\"Process CSV files and create combined dataset.\"\"\"\n    combined_data = []\n\n    for csv_file in csv_files:\n        try:\n            df = pd.read_csv(csv_file)\n            df['source_file'] = csv_file.name\n            combined_data.append(df)\n            print(f\"Processed CSV: {csv_file.name} ({len(df)} rows)\")\n        except Exception as e:\n            print(f\"Error processing CSV {csv_file.name}: {e}\")\n\n    if combined_data:\n        # Combine all CSV data\n        combined_df = pd.concat(combined_data, ignore_index=True)\n\n        # Save combined dataset\n        output_file = resource_paths.struct / \"combined_dataset.csv\"\n        resource_paths.struct.mkdir(parents=True, exist_ok=True)\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Created combined dataset: {output_file} ({len(combined_df)} rows)\")\n\ndef process_image_files(image_files: list[Path], resource_paths: RdeOutputResourcePath, config):\n    \"\"\"Process image files with optional resizing.\"\"\"\n    from PIL import Image\n\n    # Create image processing directories\n    resource_paths.main_image.mkdir(parents=True, exist_ok=True)\n    if config.save_thumbnail_image:\n        resource_paths.thumbnail.mkdir(parents=True, exist_ok=True)\n\n    for image_file in image_files:\n        try:\n            with Image.open(image_file) as img:\n                # Save main image (potentially resized)\n                main_image_path = resource_paths.main_image / image_file.name\n                if img.size[0] &gt; 1920 or img.size[1] &gt; 1080:\n                    img.thumbnail((1920, 1080), Image.Resampling.LANCZOS)\n                img.save(main_image_path)\n\n                # Create thumbnail if enabled\n                if config.save_thumbnail_image:\n                    thumbnail_path = resource_paths.thumbnail / f\"thumb_{image_file.name}\"\n                    img.thumbnail((200, 200), Image.Resampling.LANCZOS)\n                    img.save(thumbnail_path)\n\n                print(f\"Processed image: {image_file.name}\")\n\n        except Exception as e:\n            print(f\"Error processing image {image_file.name}: {e}\")\n\ndef process_other_files(other_files: list[Path], resource_paths: RdeOutputResourcePath):\n    \"\"\"Process other file types.\"\"\"\n    import shutil\n\n    resource_paths.struct.mkdir(parents=True, exist_ok=True)\n\n    for file in other_files:\n        try:\n            output_file = resource_paths.struct / file.name\n            shutil.copy2(file, output_file)\n            print(f\"Copied file: {file.name}\")\n        except Exception as e:\n            print(f\"Error copying file {file.name}: {e}\")\n\ndef generate_processing_summary(resource_paths: RdeOutputResourcePath, csv_count: int, image_count: int, other_count: int):\n    \"\"\"Generate processing summary.\"\"\"\n    summary = {\n        \"processing_summary\": {\n            \"total_files\": csv_count + image_count + other_count,\n            \"csv_files\": csv_count,\n            \"image_files\": image_count,\n            \"other_files\": other_count,\n            \"output_directories\": {\n                \"structured\": str(resource_paths.struct),\n                \"main_image\": str(resource_paths.main_image),\n                \"thumbnail\": str(resource_paths.thumbnail),\n                \"meta\": str(resource_paths.meta)\n            }\n        }\n    }\n\n    # Save summary\n    summary_file = resource_paths.meta / \"processing_summary.json\"\n    resource_paths.meta.mkdir(parents=True, exist_ok=True)\n\n    with open(summary_file, 'w') as f:\n        json.dump(summary, f, indent=2)\n\n    print(f\"Generated processing summary: {summary_file}\")\n\n# Usage\ncontext_with_advanced_function = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=advanced_dataset_function,\n    mode_name=\"MultiDataTile\"\n)\n\ndataset_runner = DatasetRunner()\ndataset_runner.process(context_with_advanced_function)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#conditional-dataset-processing","title":"Conditional Dataset Processing","text":"<pre><code>def conditional_dataset_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Dataset function with conditional processing based on configuration.\"\"\"\n\n    config = srcpaths.config\n\n    # Check processing mode\n    if hasattr(config.system, 'extended_mode') and config.system.extended_mode:\n        mode = config.system.extended_mode.lower()\n        print(f\"Processing in extended mode: {mode}\")\n\n        if mode == \"rdeformat\":\n            process_rde_format_data(resource_paths)\n        elif mode == \"multidatatile\":\n            process_multi_data_tile(resource_paths, config)\n        else:\n            process_standard_data(resource_paths)\n    else:\n        process_standard_data(resource_paths)\n\n    # Always generate metadata\n    generate_metadata(resource_paths, config)\n\ndef process_rde_format_data(resource_paths: RdeOutputResourcePath):\n    \"\"\"Process data in RDE format mode.\"\"\"\n    print(\"Processing RDE format data...\")\n\n    # RDE format specific processing\n    for raw_file in resource_paths.rawfiles:\n        if raw_file.suffix == '.zip':\n            extract_and_process_zip(raw_file, resource_paths)\n        else:\n            copy_file_to_structured(raw_file, resource_paths)\n\ndef process_multi_data_tile(resource_paths: RdeOutputResourcePath, config):\n    \"\"\"Process data in multi-data tile mode.\"\"\"\n    print(\"Processing multi-data tile...\")\n\n    # Group files by type for tile processing\n    file_groups = group_files_by_type(resource_paths.rawfiles)\n\n    for file_type, files in file_groups.items():\n        process_file_group(file_type, files, resource_paths, config)\n\ndef process_standard_data(resource_paths: RdeOutputResourcePath):\n    \"\"\"Standard data processing.\"\"\"\n    print(\"Processing standard data...\")\n\n    import shutil\n    resource_paths.struct.mkdir(parents=True, exist_ok=True)\n\n    for raw_file in resource_paths.rawfiles:\n        output_file = resource_paths.struct / raw_file.name\n        shutil.copy2(raw_file, output_file)\n\ndef generate_metadata(resource_paths: RdeOutputResourcePath, config):\n    \"\"\"Generate metadata for processed data.\"\"\"\n    metadata = {\n        \"files_processed\": len(resource_paths.rawfiles),\n        \"configuration\": {\n            \"save_raw\": config.save_raw,\n            \"save_thumbnail_image\": config.save_thumbnail_image,\n            \"magic_variable\": config.magic_variable\n        },\n        \"processing_timestamp\": pd.Timestamp.now().isoformat()\n    }\n\n    # Save metadata\n    metadata_file = resource_paths.meta / \"dataset_metadata.json\"\n    resource_paths.meta.mkdir(parents=True, exist_ok=True)\n\n    with open(metadata_file, 'w') as f:\n        json.dump(metadata, f, indent=2)\n\n# Additional helper functions\ndef extract_and_process_zip(zip_file: Path, resource_paths: RdeOutputResourcePath):\n    \"\"\"Extract and process ZIP files.\"\"\"\n    import zipfile\n\n    extract_dir = resource_paths.temp / f\"extracted_{zip_file.stem}\"\n    extract_dir.mkdir(parents=True, exist_ok=True)\n\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        zip_ref.extractall(extract_dir)\n\n    # Process extracted files\n    for extracted_file in extract_dir.rglob('*'):\n        if extracted_file.is_file():\n            copy_file_to_structured(extracted_file, resource_paths)\n\ndef copy_file_to_structured(file: Path, resource_paths: RdeOutputResourcePath):\n    \"\"\"Copy file to structured directory.\"\"\"\n    import shutil\n\n    resource_paths.struct.mkdir(parents=True, exist_ok=True)\n    output_file = resource_paths.struct / file.name\n    shutil.copy2(file, output_file)\n\ndef group_files_by_type(files: tuple[Path, ...]) -&gt; dict[str, list[Path]]:\n    \"\"\"Group files by their extension.\"\"\"\n    groups = {}\n\n    for file in files:\n        ext = file.suffix.lower()\n        if ext not in groups:\n            groups[ext] = []\n        groups[ext].append(file)\n\n    return groups\n\ndef process_file_group(file_type: str, files: list[Path], resource_paths: RdeOutputResourcePath, config):\n    \"\"\"Process a group of files of the same type.\"\"\"\n    print(f\"Processing {len(files)} {file_type} files\")\n\n    if file_type in ['.jpg', '.png', '.tiff'] and config.save_thumbnail_image:\n        # Special handling for images\n        process_image_files(files, resource_paths, config)\n    else:\n        # Standard file copying\n        for file in files:\n            copy_file_to_structured(file, resource_paths)\n\n# Usage with conditional processing\ncontext_conditional = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=conditional_dataset_function,\n    mode_name=\"MultiDataTile\"\n)\n\ndataset_runner = DatasetRunner()\ndataset_runner.process(context_conditional)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#error-resilient-dataset-processing","title":"Error-Resilient Dataset Processing","text":"<pre><code>def resilient_dataset_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Dataset function with comprehensive error handling.\"\"\"\n\n    processing_log = []\n    successful_files = 0\n    failed_files = 0\n\n    try:\n        print(f\"Starting processing of {len(resource_paths.rawfiles)} files\")\n\n        for i, raw_file in enumerate(resource_paths.rawfiles):\n            file_log = {\n                \"file\": str(raw_file),\n                \"index\": i,\n                \"status\": \"pending\"\n            }\n\n            try:\n                # Process individual file\n                process_single_file(raw_file, resource_paths, srcpaths.config)\n\n                file_log[\"status\"] = \"success\"\n                successful_files += 1\n                print(f\"\u2713 Processed {raw_file.name}\")\n\n            except Exception as e:\n                file_log[\"status\"] = \"failed\"\n                file_log[\"error\"] = str(e)\n                failed_files += 1\n                print(f\"\u2717 Failed to process {raw_file.name}: {e}\")\n\n            processing_log.append(file_log)\n\n    except Exception as e:\n        print(f\"Critical error in dataset processing: {e}\")\n        raise\n\n    finally:\n        # Always save processing log\n        save_processing_log(processing_log, resource_paths, successful_files, failed_files)\n\ndef process_single_file(raw_file: Path, resource_paths: RdeOutputResourcePath, config) -&gt; None:\n    \"\"\"Process a single file with error handling.\"\"\"\n\n    # Validate file exists and is readable\n    if not raw_file.exists():\n        raise FileNotFoundError(f\"File not found: {raw_file}\")\n\n    if not raw_file.is_file():\n        raise ValueError(f\"Path is not a file: {raw_file}\")\n\n    # Process based on file type\n    file_ext = raw_file.suffix.lower()\n\n    if file_ext == '.csv':\n        process_csv_file(raw_file, resource_paths)\n    elif file_ext in ['.jpg', '.png', '.tiff']:\n        process_image_file(raw_file, resource_paths, config)\n    elif file_ext == '.json':\n        process_json_file(raw_file, resource_paths)\n    else:\n        process_generic_file(raw_file, resource_paths)\n\ndef process_csv_file(csv_file: Path, resource_paths: RdeOutputResourcePath):\n    \"\"\"Process CSV file with validation.\"\"\"\n    try:\n        df = pd.read_csv(csv_file)\n\n        if len(df) == 0:\n            raise ValueError(\"CSV file is empty\")\n\n        # Save processed CSV\n        output_file = resource_paths.struct / csv_file.name\n        resource_paths.struct.mkdir(parents=True, exist_ok=True)\n        df.to_csv(output_file, index=False)\n\n    except pd.errors.EmptyDataError:\n        raise ValueError(\"CSV file contains no data\")\n    except pd.errors.ParserError as e:\n        raise ValueError(f\"CSV parsing error: {e}\")\n\ndef process_image_file(image_file: Path, resource_paths: RdeOutputResourcePath, config):\n    \"\"\"Process image file with validation.\"\"\"\n    from PIL import Image, UnidentifiedImageError\n\n    try:\n        with Image.open(image_file) as img:\n            # Validate image\n            img.verify()\n\n        # Process image (reopen after verify)\n        with Image.open(image_file) as img:\n            output_file = resource_paths.main_image / image_file.name\n            resource_paths.main_image.mkdir(parents=True, exist_ok=True)\n\n            # Resize if too large\n            if img.size[0] &gt; 2000 or img.size[1] &gt; 2000:\n                img.thumbnail((2000, 2000), Image.Resampling.LANCZOS)\n\n            img.save(output_file)\n\n    except UnidentifiedImageError:\n        raise ValueError(\"File is not a valid image\")\n    except Exception as e:\n        raise ValueError(f\"Image processing error: {e}\")\n\ndef process_json_file(json_file: Path, resource_paths: RdeOutputResourcePath):\n    \"\"\"Process JSON file with validation.\"\"\"\n    try:\n        with open(json_file, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        # Validate JSON structure (example validation)\n        if not isinstance(data, dict):\n            raise ValueError(\"JSON must be an object\")\n\n        # Save processed JSON\n        output_file = resource_paths.struct / json_file.name\n        resource_paths.struct.mkdir(parents=True, exist_ok=True)\n\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON format: {e}\")\n\ndef process_generic_file(file: Path, resource_paths: RdeOutputResourcePath):\n    \"\"\"Process generic file by copying.\"\"\"\n    import shutil\n\n    output_file = resource_paths.struct / file.name\n    resource_paths.struct.mkdir(parents=True, exist_ok=True)\n    shutil.copy2(file, output_file)\n\ndef save_processing_log(processing_log: list, resource_paths: RdeOutputResourcePath, successful: int, failed: int):\n    \"\"\"Save processing log with summary.\"\"\"\n\n    log_data = {\n        \"summary\": {\n            \"total_files\": len(processing_log),\n            \"successful\": successful,\n            \"failed\": failed,\n            \"success_rate\": successful / len(processing_log) if processing_log else 0\n        },\n        \"files\": processing_log,\n        \"timestamp\": pd.Timestamp.now().isoformat()\n    }\n\n    # Save log\n    log_file = resource_paths.logs / \"dataset_processing.log\"\n    resource_paths.logs.mkdir(parents=True, exist_ok=True)\n\n    with open(log_file, 'w', encoding='utf-8') as f:\n        json.dump(log_data, f, indent=2, ensure_ascii=False)\n\n    print(f\"Processing complete: {successful}/{len(processing_log)} files successful\")\n    print(f\"Processing log saved: {log_file}\")\n\n# Usage with error-resilient processing\ncontext_resilient = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=resilient_dataset_function,\n    mode_name=\"Invoice\"\n)\n\ndataset_runner = DatasetRunner()\ndataset_runner.process(context_resilient)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#no-custom-function-handling","title":"No Custom Function Handling","text":"<pre><code>def test_without_custom_function():\n    \"\"\"Test DatasetRunner when no custom function is provided.\"\"\"\n\n    # Context without custom function\n    context_no_function = ProcessingContext(\n        index=\"1\",\n        srcpaths=srcpaths,\n        resource_paths=resource_paths,\n        datasets_function=None,  # No custom function\n        mode_name=\"Invoice\"\n    )\n\n    dataset_runner = DatasetRunner()\n    dataset_runner.process(context_no_function)  # Should skip processing gracefully\n\n    print(\"DatasetRunner completed (no custom function provided)\")\n\n# Test\ntest_without_custom_function()\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/processors/datasets/#function-execution-errors","title":"Function Execution Errors","text":"<p>The DatasetRunner handles errors in custom dataset functions gracefully:</p> <pre><code>def error_prone_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Function that may raise errors.\"\"\"\n\n    # This will raise an error\n    raise ValueError(\"Something went wrong in custom processing\")\n\n# Create context with error-prone function\ncontext_with_error = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=error_prone_function,\n    mode_name=\"Invoice\"\n)\n\n# DatasetRunner will catch and log the error\ndataset_runner = DatasetRunner()\ntry:\n    dataset_runner.process(context_with_error)\nexcept Exception as e:\n    print(f\"Custom function error was handled: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#validation-and-safety","title":"Validation and Safety","text":"<pre><code>def safe_dataset_function(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath) -&gt; None:\n    \"\"\"Dataset function with safety checks.\"\"\"\n\n    # Validate input parameters\n    if not resource_paths.rawfiles:\n        print(\"Warning: No raw files to process\")\n        return\n\n    # Validate output directories can be created\n    required_dirs = [resource_paths.struct, resource_paths.meta]\n    for dir_path in required_dirs:\n        try:\n            dir_path.mkdir(parents=True, exist_ok=True)\n        except Exception as e:\n            raise PermissionError(f\"Cannot create directory {dir_path}: {e}\")\n\n    # Safe file processing\n    for raw_file in resource_paths.rawfiles:\n        if raw_file.exists() and raw_file.is_file():\n            try:\n                # Process file safely\n                process_file_safely(raw_file, resource_paths)\n            except Exception as e:\n                print(f\"Warning: Failed to process {raw_file.name}: {e}\")\n                # Continue with other files\n        else:\n            print(f\"Warning: Skipping invalid file: {raw_file}\")\n\ndef process_file_safely(file: Path, resource_paths: RdeOutputResourcePath):\n    \"\"\"Process file with safety checks.\"\"\"\n    import shutil\n\n    # Check file size (avoid processing extremely large files)\n    file_size = file.stat().st_size\n    max_size = 100 * 1024 * 1024  # 100MB limit\n\n    if file_size &gt; max_size:\n        raise ValueError(f\"File too large: {file_size} bytes\")\n\n    # Copy file safely\n    output_file = resource_paths.struct / file.name\n    shutil.copy2(file, output_file)\n\n# Usage with safe function\ncontext_safe = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=safe_dataset_function,\n    mode_name=\"Invoice\"\n)\n\ndataset_runner = DatasetRunner()\ndataset_runner.process(context_safe)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/datasets/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory Usage: Avoid loading large files entirely into memory</li> <li>I/O Operations: Minimize disk I/O through efficient file handling</li> <li>Error Recovery: Implement graceful error handling for partial failures</li> <li>Progress Tracking: Provide feedback for long-running operations</li> <li>Resource Cleanup: Ensure proper cleanup of temporary resources</li> </ul>"},{"location":"rdetoolkit/processing/processors/datasets/#see-also","title":"See Also","text":"<ul> <li>Processing Context - Context management and configuration</li> <li>Pipeline Architecture - Core pipeline classes</li> <li>File Processors - File copying and management</li> <li>Validation Processors - Data validation</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/","title":"Description Processor","text":"<p>The <code>rdetoolkit.processing.processors.descriptions</code> module provides processing functionality for updating descriptions with feature information. This module contains processors specifically designed to enhance invoice descriptions with extracted features while maintaining robust error handling.</p>"},{"location":"rdetoolkit/processing/processors/descriptions/#overview","title":"Overview","text":"<p>The descriptions processor module focuses on:</p> <ul> <li>Description Enhancement: Updates invoice descriptions with feature information</li> <li>Robust Error Handling: Suppresses errors to ensure processing pipeline continuity</li> <li>Feature Integration: Integrates extracted features into invoice metadata</li> <li>Pipeline Compatibility: Implements the standard processor interface for pipeline integration</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/descriptions/#descriptionupdater","title":"DescriptionUpdater","text":"<p>A processor that updates descriptions with feature information, providing robust error handling to prevent pipeline failures.</p>"},{"location":"rdetoolkit/processing/processors/descriptions/#constructor","title":"Constructor","text":"<pre><code>DescriptionUpdater()\n</code></pre> <p>The <code>DescriptionUpdater</code> class inherits from the base <code>Processor</code> class and requires no additional parameters for initialization.</p>"},{"location":"rdetoolkit/processing/processors/descriptions/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/descriptions/#processcontext","title":"process(context)","text":"<p>Update descriptions with features, ignoring any errors to ensure pipeline continuity.</p> <pre><code>def process(context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>context</code> (ProcessingContext): Processing context containing resource paths and configuration</li> </ul> <p>Returns:</p> <ul> <li><code>None</code>: This method does not return a value</li> </ul> <p>Error Handling:</p> <ul> <li>Uses <code>contextlib.suppress(Exception)</code> to ignore any exceptions during processing</li> <li>Logs warnings for any unexpected errors</li> <li>Ensures pipeline continuation even if description updates fail</li> </ul> <p>Processing Flow:</p> <ol> <li>Logs the start of description update process</li> <li>Calls <code>update_description_with_features()</code> with context information</li> <li>Suppresses any exceptions that occur during processing</li> <li>Logs completion status</li> </ol> <p>Example:</p> <pre><code>from rdetoolkit.processing.processors.descriptions import DescriptionUpdater\nfrom rdetoolkit.processing.context import ProcessingContext\n\n# Initialize the processor\nprocessor = DescriptionUpdater()\n\n# Process within a pipeline context\n# (context is typically provided by the pipeline framework)\nprocessor.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#base-class-integration","title":"Base Class Integration","text":"<p>The <code>DescriptionUpdater</code> class inherits from <code>Processor</code>, providing:</p> <ul> <li>Pipeline Compatibility: Standard interface for use in processing pipelines</li> <li>Context Management: Access to processing context and resource paths</li> <li>Error Handling: Built-in error handling patterns</li> <li>Logging Integration: Consistent logging behavior across processors</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#processing-context-requirements","title":"Processing Context Requirements","text":"<p>The <code>DescriptionUpdater</code> requires the following context attributes:</p> <ul> <li><code>resource_paths</code>: Output resource paths for the current processing tile</li> <li><code>invoice_dst_filepath</code>: Destination path for the invoice file</li> <li><code>metadata_def_path</code>: Path to metadata definition file</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#integration-with-invoice-system","title":"Integration with Invoice System","text":"<p>The description processor integrates with the invoice system through:</p>"},{"location":"rdetoolkit/processing/processors/descriptions/#update_description_with_features","title":"update_description_with_features()","text":"<p>The processor delegates to <code>rdetoolkit.invoicefile.update_description_with_features()</code>:</p> <pre><code>update_description_with_features(\n    resource_paths,      # RdeOutputResourcePath\n    invoice_dst_filepath, # Path to invoice destination\n    metadata_def_path    # Path to metadata definitions\n)\n</code></pre> <p>This function:</p> <ul> <li>Reads existing invoice data</li> <li>Extracts features from processed data</li> <li>Updates description fields with feature information</li> <li>Saves the enhanced invoice data</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#usage-in-processing-pipelines","title":"Usage in Processing Pipelines","text":""},{"location":"rdetoolkit/processing/processors/descriptions/#basic-pipeline-integration","title":"Basic Pipeline Integration","text":"<pre><code>from rdetoolkit.processing.pipeline import Pipeline\nfrom rdetoolkit.processing.processors.descriptions import DescriptionUpdater\n\n# Create pipeline with description processor\npipeline = Pipeline([\n    # ... other processors ...\n    DescriptionUpdater(),\n    # ... additional processors ...\n])\n\n# Execute pipeline\npipeline.execute(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#custom-pipeline-configuration","title":"Custom Pipeline Configuration","text":"<pre><code>from rdetoolkit.processing.factories import ProcessorFactory\nfrom rdetoolkit.processing.processors.descriptions import DescriptionUpdater\n\n# Register description processor in factory\nfactory = ProcessorFactory()\nfactory.register(\"description_updater\", DescriptionUpdater)\n\n# Create processor through factory\nprocessor = factory.create(\"description_updater\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#conditional-processing","title":"Conditional Processing","text":"<pre><code>from rdetoolkit.processing.processors.descriptions import DescriptionUpdater\nfrom rdetoolkit.processing.context import ProcessingContext\n\nclass ConditionalDescriptionProcessor(DescriptionUpdater):\n    \"\"\"Description processor with conditional execution.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Process only if features are available.\"\"\"\n\n        # Check if feature extraction was successful\n        if self._has_features(context):\n            super().process(context)\n        else:\n            logger.info(\"Skipping description update - no features available\")\n\n    def _has_features(self, context: ProcessingContext) -&gt; bool:\n        \"\"\"Check if features are available for processing.\"\"\"\n        # Implementation to check feature availability\n        feature_file = context.resource_paths.meta / \"features.json\"\n        return feature_file.exists()\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"rdetoolkit/processing/processors/descriptions/#exception-suppression","title":"Exception Suppression","text":"<p>The processor uses <code>contextlib.suppress(Exception)</code> to handle errors gracefully:</p> <pre><code>with contextlib.suppress(Exception):\n    update_description_with_features(\n        context.resource_paths,\n        context.invoice_dst_filepath,\n        context.metadata_def_path,\n    )\n</code></pre> <p>Benefits:</p> <ul> <li>Prevents pipeline failures due to description update issues</li> <li>Maintains processing continuity for other pipeline stages</li> <li>Allows partial success in multi-stage processing</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#logging-strategy","title":"Logging Strategy","text":"<p>The processor implements comprehensive logging:</p> <pre><code>logger.debug(\"Updating descriptions with features\")\n# ... processing ...\nlogger.debug(\"Description update completed (errors suppressed)\")\n</code></pre> <p>Log Levels:</p> <ul> <li><code>DEBUG</code>: Normal processing flow information</li> <li><code>WARNING</code>: Unexpected errors that are handled gracefully</li> <li><code>INFO</code>: Significant processing milestones</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#error-recovery-patterns","title":"Error Recovery Patterns","text":"<pre><code>class RobustDescriptionUpdater(DescriptionUpdater):\n    \"\"\"Enhanced description processor with error recovery.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Process with multiple error recovery strategies.\"\"\"\n\n        try:\n            # Attempt primary processing\n            super().process(context)\n\n        except MemoryError:\n            # Handle memory-specific issues\n            logger.warning(\"Memory error during description update\")\n            self._cleanup_memory(context)\n\n        except FileNotFoundError:\n            # Handle missing file issues\n            logger.warning(\"Required files missing for description update\")\n            self._create_default_description(context)\n\n    def _cleanup_memory(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Clean up memory and retry with reduced processing.\"\"\"\n        # Implementation for memory cleanup\n        pass\n\n    def _create_default_description(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Create default description when features are unavailable.\"\"\"\n        # Implementation for default description creation\n        pass\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/descriptions/#basic-description-processing","title":"Basic Description Processing","text":"<pre><code>from rdetoolkit.processing.processors.descriptions import DescriptionUpdater\nfrom rdetoolkit.processing.context import ProcessingContext\nfrom rdetoolkit.models.rde2types import RdeOutputResourcePath\nfrom pathlib import Path\n\n# Setup processing context\nresource_paths = RdeOutputResourcePath(\n    raw=Path(\"data/divided/0001/raw\"),\n    struct=Path(\"data/divided/0001/structured\"),\n    meta=Path(\"data/divided/0001/meta\"),\n    invoice=Path(\"data/divided/0001/invoice\"),\n    # ... other paths ...\n)\n\ncontext = ProcessingContext(\n    resource_paths=resource_paths,\n    invoice_dst_filepath=Path(\"data/divided/0001/invoice/invoice.json\"),\n    metadata_def_path=Path(\"data/tasksupport/metadata_def.json\")\n)\n\n# Process descriptions\nprocessor = DescriptionUpdater()\nprocessor.process(context)\nprint(\"Description processing completed\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#pipeline-integration-with-multiple-processors","title":"Pipeline Integration with Multiple Processors","text":"<pre><code>from rdetoolkit.processing.pipeline import Pipeline\nfrom rdetoolkit.processing.processors.descriptions import DescriptionUpdater\nfrom rdetoolkit.processing.processors.files import FileProcessor\nfrom rdetoolkit.processing.processors.validation import ValidationProcessor\n\n# Create comprehensive processing pipeline\npipeline = Pipeline([\n    FileProcessor(),           # Process raw files\n    DescriptionUpdater(),      # Update descriptions\n    ValidationProcessor(),     # Validate results\n])\n\n# Execute complete pipeline\ntry:\n    pipeline.execute(context)\n    print(\"Pipeline execution completed successfully\")\nexcept Exception as e:\n    print(f\"Pipeline execution failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#custom-description-enhancement","title":"Custom Description Enhancement","text":"<pre><code>from rdetoolkit.processing.processors.descriptions import DescriptionUpdater\nfrom rdetoolkit.processing.context import ProcessingContext\nimport json\n\nclass EnhancedDescriptionUpdater(DescriptionUpdater):\n    \"\"\"Enhanced description processor with additional features.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Enhanced processing with custom feature extraction.\"\"\"\n\n        # Pre-processing: extract custom features\n        self._extract_custom_features(context)\n\n        # Standard description processing\n        super().process(context)\n\n        # Post-processing: add metadata\n        self._add_processing_metadata(context)\n\n    def _extract_custom_features(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Extract custom features before description update.\"\"\"\n\n        custom_features = {\n            \"processing_timestamp\": \"2024-01-01T12:00:00Z\",\n            \"feature_version\": \"1.0.0\",\n            \"custom_tags\": [\"enhanced\", \"processed\"]\n        }\n\n        # Save custom features\n        feature_file = context.resource_paths.meta / \"custom_features.json\"\n        with feature_file.open(\"w\") as f:\n            json.dump(custom_features, f, indent=2)\n\n    def _add_processing_metadata(self, context: ProcessingContext) -&gt; None:\n        \"\"\"Add processing metadata after description update.\"\"\"\n\n        metadata = {\n            \"description_updated\": True,\n            \"processor_version\": \"2.0.0\",\n            \"processing_notes\": \"Enhanced description processing completed\"\n        }\n\n        # Save processing metadata\n        metadata_file = context.resource_paths.meta / \"processing_metadata.json\"\n        with metadata_file.open(\"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n# Usage\nenhanced_processor = EnhancedDescriptionUpdater()\nenhanced_processor.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/descriptions/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Error Suppression: Minimal performance impact from exception handling</li> <li>Memory Usage: Low memory footprint for description processing</li> <li>I/O Operations: Optimized file operations for invoice updates</li> <li>Pipeline Integration: Efficient integration with other processors</li> </ul>"},{"location":"rdetoolkit/processing/processors/descriptions/#best-practices","title":"Best Practices","text":"<ol> <li>Always Use in Pipelines: Integrate with other processors for complete workflows</li> <li>Monitor Logs: Check debug logs for processing status and any suppressed errors</li> <li>Validate Context: Ensure required context attributes are available</li> <li>Handle Dependencies: Ensure prerequisite processors have executed successfully</li> </ol>"},{"location":"rdetoolkit/processing/processors/descriptions/#see-also","title":"See Also","text":"<ul> <li>Processing Pipeline - For pipeline framework documentation</li> <li>Processing Context - For context management details</li> <li>Invoice Module - For invoice processing functions</li> <li>Processor Base Class - For base processor interface</li> </ul>"},{"location":"rdetoolkit/processing/processors/files/","title":"File Processors","text":"<p>The file processors handle file operations including copying raw files to designated directories with support for different processing modes.</p>"},{"location":"rdetoolkit/processing/processors/files/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/files/#filecopier","title":"FileCopier","text":"<p>Standard file copier for raw files. Handles copying files based on configuration settings to appropriate output directories.</p> <pre><code>class FileCopier(Processor):\n    \"\"\"Standard file copier for raw files.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/files/#process","title":"process","text":"<p>Execute file copying operations based on configuration.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing configuration and file paths</p> <p>Configuration Dependencies: - <code>config.save_raw</code>: If True, copies files to raw directory - <code>config.save_nonshared_raw</code>: If True, copies files to nonshared_raw directory</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import FileCopier\n\nfile_copier = FileCopier()\nfile_copier.process(context)\n</code></pre></p> <p>Behavior: - Copies files to <code>raw</code> directory if <code>save_raw</code> is enabled - Copies files to <code>nonshared_raw</code> directory if <code>save_nonshared_raw</code> is enabled - Creates target directories if they don't exist - Handles file copying errors gracefully with logging</p>"},{"location":"rdetoolkit/processing/processors/files/#rdeformatfilecopier","title":"RDEFormatFileCopier","text":"<p>Specialized file copier for RDEFormat mode. Copies files by matching directory structure patterns.</p> <pre><code>class RDEFormatFileCopier(Processor):\n    \"\"\"Specialized copier for RDEFormat mode.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/files/#process_1","title":"process","text":"<p>Execute RDEFormat-specific file copying operations.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing configuration and file paths</p> <p>Configuration Dependencies: - <code>config.save_raw</code>: Controls whether files are copied to raw directory</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import RDEFormatFileCopier\n\nrde_copier = RDEFormatFileCopier()\nrde_copier.process(context)\n</code></pre></p> <p>Behavior: - Copies files by directory structure matching - Preserves original directory organization - Only operates when <code>save_raw</code> is enabled - Designed for RDEFormat mode processing</p>"},{"location":"rdetoolkit/processing/processors/files/#smarttablefilecopier","title":"SmartTableFileCopier","text":"<p>Specialized file copier for SmartTable mode. Copies files while excluding SmartTable-generated CSV files.</p> <pre><code>class SmartTableFileCopier(Processor):\n    \"\"\"Specialized copier for SmartTable mode.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/files/#process_2","title":"process","text":"<p>Execute SmartTable-specific file copying operations.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing configuration and file paths</p> <p>Configuration Dependencies: - <code>config.save_raw</code>: If True, copies filtered files to raw directory - <code>config.save_nonshared_raw</code>: If True, copies filtered files to nonshared_raw directory</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import SmartTableFileCopier\n\nsmarttable_copier = SmartTableFileCopier()\nsmarttable_copier.process(context)\n</code></pre></p> <p>Behavior: - Filters out SmartTable-generated CSV files before copying - Identifies SmartTable CSVs by naming pattern (contains <code>_smarttable_</code>) - Copies remaining files to appropriate directories based on configuration - Maintains file organization while excluding auto-generated content</p>"},{"location":"rdetoolkit/processing/processors/files/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/files/#basic-file-copying","title":"Basic File Copying","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier\nfrom rdetoolkit.processing.context import ProcessingContext\n\n# Standard file copying\nfile_copier = FileCopier()\nfile_copier.process(context)\n\n# Check what was copied\nif context.srcpaths.config.save_raw:\n    print(f\"Files copied to: {context.resource_paths.raw}\")\nif context.srcpaths.config.save_nonshared_raw:\n    print(f\"Files copied to: {context.resource_paths.nonshared_raw}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#mode-specific-file-copying","title":"Mode-Specific File Copying","text":"<pre><code>from rdetoolkit.processing.processors import (\n    FileCopier, RDEFormatFileCopier, SmartTableFileCopier\n)\n\ndef copy_files_by_mode(context: ProcessingContext):\n    \"\"\"Copy files using appropriate copier for the mode.\"\"\"\n\n    if context.mode_name.lower() == \"rdeformat\":\n        copier = RDEFormatFileCopier()\n    elif context.mode_name.lower() == \"smarttableinvoice\":\n        copier = SmartTableFileCopier()\n    else:\n        copier = FileCopier()\n\n    print(f\"Using {copier.get_name()} for {context.mode_name} mode\")\n    copier.process(context)\n\n# Usage\ncopy_files_by_mode(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#custom-file-filtering","title":"Custom File Filtering","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier\nfrom pathlib import Path\n\nclass FilteredFileCopier(FileCopier):\n    \"\"\"File copier with custom filtering logic.\"\"\"\n\n    def __init__(self, allowed_extensions=None):\n        self.allowed_extensions = allowed_extensions or ['.txt', '.pdf', '.jpg', '.png']\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        # Filter files before copying\n        original_files = context.resource_paths.rawfiles\n        filtered_files = [\n            f for f in original_files\n            if f.suffix.lower() in self.allowed_extensions\n        ]\n\n        # Temporarily replace rawfiles for copying\n        context.resource_paths = context.resource_paths._replace(\n            rawfiles=tuple(filtered_files)\n        )\n\n        # Perform copying\n        super().process(context)\n\n        # Restore original file list\n        context.resource_paths = context.resource_paths._replace(\n            rawfiles=original_files\n        )\n\n        print(f\"Copied {len(filtered_files)} of {len(original_files)} files\")\n\n# Usage\nfiltered_copier = FilteredFileCopier(['.pdf', '.txt'])\nfiltered_copier.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#parallel-file-copying","title":"Parallel File Copying","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier\nimport concurrent.futures\nimport shutil\n\nclass ParallelFileCopier(FileCopier):\n    \"\"\"File copier with parallel processing support.\"\"\"\n\n    def __init__(self, max_workers=4):\n        self.max_workers = max_workers\n\n    def _copy_files(self, source_files, target_dir):\n        \"\"\"Copy files in parallel.\"\"\"\n        target_dir.mkdir(parents=True, exist_ok=True)\n\n        def copy_single_file(source_file):\n            target_file = target_dir / source_file.name\n            shutil.copy2(source_file, target_file)\n            return source_file, target_file\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            futures = [executor.submit(copy_single_file, f) for f in source_files]\n\n            results = []\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    source, target = future.result()\n                    results.append((source, target))\n                except Exception as e:\n                    print(f\"Error copying file: {e}\")\n\n            return results\n\n# Usage\nparallel_copier = ParallelFileCopier(max_workers=8)\nparallel_copier.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#file-copy-verification","title":"File Copy Verification","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier\nimport hashlib\n\nclass VerifiedFileCopier(FileCopier):\n    \"\"\"File copier with integrity verification.\"\"\"\n\n    def _copy_files(self, source_files, target_dir):\n        \"\"\"Copy files with verification.\"\"\"\n        target_dir.mkdir(parents=True, exist_ok=True)\n\n        verified_copies = []\n        failed_copies = []\n\n        for source_file in source_files:\n            target_file = target_dir / source_file.name\n\n            try:\n                # Copy file\n                shutil.copy2(source_file, target_file)\n\n                # Verify integrity\n                if self._verify_file_integrity(source_file, target_file):\n                    verified_copies.append((source_file, target_file))\n                else:\n                    failed_copies.append(source_file)\n                    target_file.unlink()  # Remove corrupted copy\n\n            except Exception as e:\n                print(f\"Error copying {source_file}: {e}\")\n                failed_copies.append(source_file)\n\n        if failed_copies:\n            print(f\"Warning: {len(failed_copies)} files failed verification\")\n\n        return verified_copies\n\n    def _verify_file_integrity(self, source_file, target_file):\n        \"\"\"Verify file integrity using SHA-256.\"\"\"\n        def get_file_hash(file_path):\n            hash_sha256 = hashlib.sha256()\n            with open(file_path, \"rb\") as f:\n                for chunk in iter(lambda: f.read(4096), b\"\"):\n                    hash_sha256.update(chunk)\n            return hash_sha256.hexdigest()\n\n        return get_file_hash(source_file) == get_file_hash(target_file)\n\n# Usage\nverified_copier = VerifiedFileCopier()\nverified_copier.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/processors/files/#file-operation-errors","title":"File Operation Errors","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier\nimport shutil\n\nclass RobustFileCopier(FileCopier):\n    \"\"\"File copier with comprehensive error handling.\"\"\"\n\n    def _copy_files(self, source_files, target_dir):\n        \"\"\"Copy files with detailed error handling.\"\"\"\n        target_dir.mkdir(parents=True, exist_ok=True)\n\n        successful_copies = []\n        failed_copies = []\n\n        for source_file in source_files:\n            target_file = target_dir / source_file.name\n\n            try:\n                # Check source file exists and is readable\n                if not source_file.exists():\n                    raise FileNotFoundError(f\"Source file not found: {source_file}\")\n\n                if not source_file.is_file():\n                    raise ValueError(f\"Source is not a file: {source_file}\")\n\n                # Check target directory is writable\n                if not target_dir.exists():\n                    target_dir.mkdir(parents=True, exist_ok=True)\n\n                # Perform copy with error handling\n                shutil.copy2(source_file, target_file)\n                successful_copies.append((source_file, target_file))\n\n            except PermissionError as e:\n                print(f\"Permission denied copying {source_file}: {e}\")\n                failed_copies.append(source_file)\n            except FileNotFoundError as e:\n                print(f\"File not found: {e}\")\n                failed_copies.append(source_file)\n            except OSError as e:\n                print(f\"OS error copying {source_file}: {e}\")\n                failed_copies.append(source_file)\n            except Exception as e:\n                print(f\"Unexpected error copying {source_file}: {e}\")\n                failed_copies.append(source_file)\n\n        print(f\"File copy results: {len(successful_copies)} successful, {len(failed_copies)} failed\")\n        return successful_copies\n\n# Usage\nrobust_copier = RobustFileCopier()\nrobust_copier.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#configuration-validation","title":"Configuration Validation","text":"<pre><code>from rdetoolkit.processing.processors import FileCopier\n\nclass ValidatedFileCopier(FileCopier):\n    \"\"\"File copier with configuration validation.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        # Validate configuration\n        config = context.srcpaths.config\n\n        if not config.save_raw and not config.save_nonshared_raw:\n            print(\"Warning: No file copying enabled (save_raw and save_nonshared_raw both False)\")\n            return\n\n        # Validate source files exist\n        missing_files = [f for f in context.resource_paths.rawfiles if not f.exists()]\n        if missing_files:\n            print(f\"Warning: {len(missing_files)} source files not found\")\n            for f in missing_files:\n                print(f\"  Missing: {f}\")\n\n        # Validate output directories are accessible\n        if config.save_raw:\n            self._validate_output_directory(context.resource_paths.raw)\n        if config.save_nonshared_raw:\n            self._validate_output_directory(context.resource_paths.nonshared_raw)\n\n        # Perform copying\n        super().process(context)\n\n    def _validate_output_directory(self, directory: Path):\n        \"\"\"Validate output directory is accessible.\"\"\"\n        try:\n            directory.mkdir(parents=True, exist_ok=True)\n            # Test write access\n            test_file = directory / \".write_test\"\n            test_file.touch()\n            test_file.unlink()\n        except Exception as e:\n            raise PermissionError(f\"Cannot write to directory {directory}: {e}\")\n\n# Usage\nvalidated_copier = ValidatedFileCopier()\nvalidated_copier.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/files/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Large Files: Use streaming operations for large file copying</li> <li>Many Files: Consider parallel processing for multiple files</li> <li>Network Storage: Handle network timeout and retry logic</li> <li>Disk Space: Check available space before copying</li> <li>Memory Usage: Avoid loading entire files into memory</li> </ul>"},{"location":"rdetoolkit/processing/processors/files/#see-also","title":"See Also","text":"<ul> <li>Processing Context - Context management and configuration</li> <li>Pipeline Architecture - Core pipeline classes</li> <li>Invoice Processors - Invoice initialization processors</li> <li>Validation Processors - File validation processors</li> </ul>"},{"location":"rdetoolkit/processing/processors/invoice/","title":"Invoice Processors","text":"<p>The invoice processors handle invoice initialization for different processing modes including standard, Excel-based, and SmartTable-based invoice creation.</p>"},{"location":"rdetoolkit/processing/processors/invoice/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/invoice/#standardinvoiceinitializer","title":"StandardInvoiceInitializer","text":"<p>Handles invoice initialization for RDEFormat, MultiFile, and Invoice modes by copying the original invoice to the destination.</p> <pre><code>class StandardInvoiceInitializer(Processor):\n    \"\"\"Standard invoice initializer for RDEFormat, MultiFile, and Invoice modes.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/invoice/#process","title":"process","text":"<p>Execute standard invoice initialization by copying the original invoice.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing invoice paths</p> <p>Behavior: - Copies invoice from <code>invoice_org</code> to destination <code>invoice.json</code> - Creates destination directory if it doesn't exist - Handles file copying errors gracefully</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import StandardInvoiceInitializer\n\ninitializer = StandardInvoiceInitializer()\ninitializer.process(context)\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/invoice/#excelinvoiceinitializer","title":"ExcelInvoiceInitializer","text":"<p>Handles invoice initialization for ExcelInvoice mode by creating invoice from Excel file data.</p> <pre><code>class ExcelInvoiceInitializer(Processor):\n    \"\"\"Invoice initializer for ExcelInvoice mode.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/invoice/#process_1","title":"process","text":"<p>Execute Excel invoice initialization by parsing Excel data and creating invoice.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context with Excel file information</p> <p>Raises: - <code>ValueError</code>: If context is not configured for Excel mode or if Excel index is invalid</p> <p>Behavior: - Validates context is in Excel mode - Parses Excel index from context - Creates invoice using Excel invoice processing functions - Handles Excel processing errors</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import ExcelInvoiceInitializer\n\n# Context must be configured for Excel mode\nexcel_initializer = ExcelInvoiceInitializer()\nexcel_initializer.process(context)  # context.is_excel_mode must be True\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/invoice/#smarttableinvoiceinitializer","title":"SmartTableInvoiceInitializer","text":"<p>Handles invoice initialization for SmartTable mode by generating invoice from CSV data with complex field mapping.</p> <pre><code>class SmartTableInvoiceInitializer(Processor):\n    \"\"\"Invoice initializer for SmartTable mode.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#methods_2","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/invoice/#process_2","title":"process","text":"<p>Execute SmartTable invoice initialization by parsing CSV data and creating invoice.</p> <pre><code>def process(self, context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context with SmartTable file information</p> <p>Raises: - <code>ValueError</code>: If context is not configured for SmartTable mode - <code>FileNotFoundError</code>: If SmartTable CSV file is not found - <code>Exception</code>: If CSV parsing or invoice creation fails</p> <p>Behavior: - Validates context is in SmartTable mode - Reads and parses SmartTable CSV file - Maps CSV columns to invoice structure using complex field mapping - Processes general and specific attributes - Ensures required invoice fields are present - Creates final invoice JSON file</p> <p>Field Mapping: - Supports nested field mapping (e.g., <code>\"basic.dataName\"</code>) - Handles array fields and complex data structures - Provides fallback values for missing fields</p> <p>Example: <pre><code>from rdetoolkit.processing.processors import SmartTableInvoiceInitializer\n\n# Context must be configured for SmartTable mode\nsmarttable_initializer = SmartTableInvoiceInitializer()\nsmarttable_initializer.process(context)  # context.is_smarttable_mode must be True\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/invoice/#invoiceinitializerfactory","title":"InvoiceInitializerFactory","text":"<p>Factory class for creating appropriate invoice initializers based on processing mode.</p> <pre><code>class InvoiceInitializerFactory:\n    \"\"\"Factory for creating invoice initializers based on mode.\"\"\"\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#class-methods","title":"Class Methods","text":""},{"location":"rdetoolkit/processing/processors/invoice/#create","title":"create","text":"<p>Create an appropriate invoice initializer for the specified mode.</p> <pre><code>@classmethod\ndef create(cls, mode: str) -&gt; Processor\n</code></pre> <p>Parameters: - <code>mode</code> (str): Processing mode name</p> <p>Returns: - <code>Processor</code>: Appropriate invoice initializer for the mode</p> <p>Raises: - <code>ValueError</code>: If mode is not supported</p> <p>Supported Modes: - <code>\"rdeformat\"</code>: Returns <code>StandardInvoiceInitializer</code> - <code>\"multidatatile\"</code>: Returns <code>StandardInvoiceInitializer</code> - <code>\"invoice\"</code>: Returns <code>StandardInvoiceInitializer</code> - <code>\"excelinvoice\"</code>: Returns <code>ExcelInvoiceInitializer</code> - <code>\"smarttableinvoice\"</code>: Returns <code>SmartTableInvoiceInitializer</code></p> <p>Example: <pre><code>from rdetoolkit.processing.processors import InvoiceInitializerFactory\n\n# Create initializer for specific mode\ninitializer = InvoiceInitializerFactory.create(\"excelinvoice\")\ninitializer.process(context)\n\n# Dynamic mode selection\nmode = context.mode_name.lower()\ninitializer = InvoiceInitializerFactory.create(mode)\ninitializer.process(context)\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/invoice/#get_supported_modes","title":"get_supported_modes","text":"<p>Get list of supported processing modes.</p> <pre><code>@classmethod\ndef get_supported_modes(cls) -&gt; list[str]\n</code></pre> <p>Returns: - <code>list[str]</code>: List of supported mode names</p> <p>Example: <pre><code>modes = InvoiceInitializerFactory.get_supported_modes()\nprint(f\"Supported modes: {', '.join(modes)}\")\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/invoice/#usage-examples","title":"Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/invoice/#basic-invoice-initialization","title":"Basic Invoice Initialization","text":"<pre><code>from rdetoolkit.processing.processors import StandardInvoiceInitializer\n\n# Standard invoice processing\nstandard_init = StandardInvoiceInitializer()\nstandard_init.process(context)\n\n# Check if invoice was created\nif context.invoice_dst_filepath.exists():\n    print(f\"Invoice created at: {context.invoice_dst_filepath}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#mode-based-invoice-initialization","title":"Mode-Based Invoice Initialization","text":"<pre><code>from rdetoolkit.processing.processors import InvoiceInitializerFactory\n\ndef initialize_invoice_by_mode(context: ProcessingContext):\n    \"\"\"Initialize invoice using appropriate initializer for the mode.\"\"\"\n\n    try:\n        # Create appropriate initializer\n        initializer = InvoiceInitializerFactory.create(context.mode_name.lower())\n\n        print(f\"Using {initializer.get_name()} for {context.mode_name} mode\")\n\n        # Execute initialization\n        initializer.process(context)\n\n        print(f\"Invoice initialized successfully\")\n\n    except ValueError as e:\n        print(f\"Unsupported mode: {e}\")\n    except Exception as e:\n        print(f\"Invoice initialization failed: {e}\")\n        raise\n\n# Usage\ninitialize_invoice_by_mode(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#excel-invoice-processing","title":"Excel Invoice Processing","text":"<pre><code>from rdetoolkit.processing.processors import ExcelInvoiceInitializer\nfrom rdetoolkit.processing.context import ProcessingContext\n\ndef process_excel_invoice(context: ProcessingContext):\n    \"\"\"Process Excel invoice with validation.\"\"\"\n\n    # Validate context for Excel processing\n    if not context.is_excel_mode:\n        raise ValueError(\"Context not configured for Excel mode\")\n\n    # Check Excel file exists\n    excel_file = context.excel_invoice_file\n    if not excel_file.exists():\n        raise FileNotFoundError(f\"Excel file not found: {excel_file}\")\n\n    # Initialize Excel invoice\n    excel_init = ExcelInvoiceInitializer()\n    excel_init.process(context)\n\n    print(f\"Excel invoice processed from: {excel_file}\")\n\n# Usage with Excel context\nexcel_context = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=None,\n    mode_name=\"ExcelInvoice\",\n    excel_file=Path(\"data/inputdata/dataset_excel_invoice.xlsx\"),\n    excel_index=1\n)\n\nprocess_excel_invoice(excel_context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#smarttable-invoice-processing","title":"SmartTable Invoice Processing","text":"<pre><code>from rdetoolkit.processing.processors import SmartTableInvoiceInitializer\nfrom rdetoolkit.processing.context import ProcessingContext\n\ndef process_smarttable_invoice(context: ProcessingContext):\n    \"\"\"Process SmartTable invoice with validation.\"\"\"\n\n    # Validate context for SmartTable processing\n    if not context.is_smarttable_mode:\n        raise ValueError(\"Context not configured for SmartTable mode\")\n\n    # Check SmartTable file exists\n    smarttable_file = context.smarttable_invoice_file\n    if not smarttable_file.exists():\n        raise FileNotFoundError(f\"SmartTable file not found: {smarttable_file}\")\n\n    # Initialize SmartTable invoice\n    smarttable_init = SmartTableInvoiceInitializer()\n    smarttable_init.process(context)\n\n    print(f\"SmartTable invoice processed from: {smarttable_file}\")\n\n# Usage with SmartTable context\nsmarttable_context = ProcessingContext(\n    index=\"1\",\n    srcpaths=srcpaths,\n    resource_paths=resource_paths,\n    datasets_function=None,\n    mode_name=\"SmartTableInvoice\",\n    smarttable_file=Path(\"data/inputdata/smarttable_data.csv\")\n)\n\nprocess_smarttable_invoice(smarttable_context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#custom-invoice-initializer","title":"Custom Invoice Initializer","text":"<pre><code>from rdetoolkit.processing.processors import StandardInvoiceInitializer\nimport json\nfrom datetime import datetime\n\nclass TimestampedInvoiceInitializer(StandardInvoiceInitializer):\n    \"\"\"Invoice initializer that adds timestamp to invoice data.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        # First, copy the original invoice\n        super().process(context)\n\n        # Then add timestamp\n        self._add_timestamp(context)\n\n    def _add_timestamp(self, context: ProcessingContext):\n        \"\"\"Add processing timestamp to invoice.\"\"\"\n        invoice_file = context.invoice_dst_filepath\n\n        if not invoice_file.exists():\n            return\n\n        try:\n            # Read existing invoice\n            with open(invoice_file, 'r', encoding='utf-8') as f:\n                invoice_data = json.load(f)\n\n            # Add timestamp\n            if 'processing' not in invoice_data:\n                invoice_data['processing'] = {}\n\n            invoice_data['processing']['timestamp'] = datetime.now().isoformat()\n            invoice_data['processing']['mode'] = context.mode_name\n\n            # Write back\n            with open(invoice_file, 'w', encoding='utf-8') as f:\n                json.dump(invoice_data, f, indent=2, ensure_ascii=False)\n\n            print(f\"Added timestamp to invoice: {invoice_file}\")\n\n        except Exception as e:\n            print(f\"Warning: Could not add timestamp to invoice: {e}\")\n\n# Usage\ntimestamped_init = TimestampedInvoiceInitializer()\ntimestamped_init.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#parallel-invoice-processing","title":"Parallel Invoice Processing","text":"<pre><code>from rdetoolkit.processing.processors import InvoiceInitializerFactory\nimport concurrent.futures\n\ndef process_multiple_invoices(contexts: list[ProcessingContext]):\n    \"\"\"Process multiple invoices in parallel.\"\"\"\n\n    def process_single_invoice(context):\n        try:\n            initializer = InvoiceInitializerFactory.create(context.mode_name.lower())\n            initializer.process(context)\n            return context.index, \"success\", None\n        except Exception as e:\n            return context.index, \"failed\", str(e)\n\n    # Process invoices in parallel\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [executor.submit(process_single_invoice, ctx) for ctx in contexts]\n\n        results = []\n        for future in concurrent.futures.as_completed(futures):\n            index, status, error = future.result()\n            results.append((index, status, error))\n\n            if status == \"success\":\n                print(f\"\u2713 Invoice {index} processed successfully\")\n            else:\n                print(f\"\u2717 Invoice {index} failed: {error}\")\n\n    # Summary\n    successful = sum(1 for _, status, _ in results if status == \"success\")\n    failed = len(results) - successful\n    print(f\"Processing complete: {successful} successful, {failed} failed\")\n\n    return results\n\n# Usage\ncontexts = [context1, context2, context3]  # List of processing contexts\nresults = process_multiple_invoices(contexts)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/processors/invoice/#mode-validation","title":"Mode Validation","text":"<pre><code>from rdetoolkit.processing.processors import InvoiceInitializerFactory\n\ndef safe_invoice_initialization(context: ProcessingContext):\n    \"\"\"Safely initialize invoice with comprehensive error handling.\"\"\"\n\n    try:\n        # Validate mode\n        supported_modes = InvoiceInitializerFactory.get_supported_modes()\n        mode = context.mode_name.lower()\n\n        if mode not in supported_modes:\n            raise ValueError(f\"Unsupported mode: {mode}. Supported: {supported_modes}\")\n\n        # Validate context configuration\n        if mode == \"excelinvoice\" and not context.is_excel_mode:\n            raise ValueError(\"Context not properly configured for Excel mode\")\n\n        if mode == \"smarttableinvoice\" and not context.is_smarttable_mode:\n            raise ValueError(\"Context not properly configured for SmartTable mode\")\n\n        # Create and execute initializer\n        initializer = InvoiceInitializerFactory.create(mode)\n        initializer.process(context)\n\n        # Validate result\n        if not context.invoice_dst_filepath.exists():\n            raise RuntimeError(\"Invoice file was not created\")\n\n        print(f\"\u2713 Invoice initialization successful for {mode} mode\")\n        return True\n\n    except ValueError as e:\n        print(f\"\u2717 Configuration error: {e}\")\n        return False\n    except FileNotFoundError as e:\n        print(f\"\u2717 File not found: {e}\")\n        return False\n    except RuntimeError as e:\n        print(f\"\u2717 Processing error: {e}\")\n        return False\n    except Exception as e:\n        print(f\"\u2717 Unexpected error: {e}\")\n        return False\n\n# Usage\nsuccess = safe_invoice_initialization(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#excel-processing-errors","title":"Excel Processing Errors","text":"<pre><code>from rdetoolkit.processing.processors import ExcelInvoiceInitializer\n\nclass RobustExcelInvoiceInitializer(ExcelInvoiceInitializer):\n    \"\"\"Excel invoice initializer with enhanced error handling.\"\"\"\n\n    def process(self, context: ProcessingContext) -&gt; None:\n        try:\n            # Validate Excel context\n            self._validate_excel_context(context)\n\n            # Validate Excel file\n            self._validate_excel_file(context)\n\n            # Process Excel invoice\n            super().process(context)\n\n        except ValueError as e:\n            print(f\"Excel validation error: {e}\")\n            raise\n        except FileNotFoundError as e:\n            print(f\"Excel file error: {e}\")\n            raise\n        except Exception as e:\n            print(f\"Excel processing error: {e}\")\n            # Try to create a minimal invoice as fallback\n            self._create_fallback_invoice(context)\n\n    def _validate_excel_context(self, context: ProcessingContext):\n        \"\"\"Validate context for Excel processing.\"\"\"\n        if not context.is_excel_mode:\n            raise ValueError(\"Context not configured for Excel mode\")\n\n        if context.excel_index is None:\n            raise ValueError(\"Excel index not set\")\n\n    def _validate_excel_file(self, context: ProcessingContext):\n        \"\"\"Validate Excel file exists and is readable.\"\"\"\n        excel_file = context.excel_invoice_file\n\n        if not excel_file.exists():\n            raise FileNotFoundError(f\"Excel file not found: {excel_file}\")\n\n        if not excel_file.is_file():\n            raise ValueError(f\"Excel path is not a file: {excel_file}\")\n\n        # Check file extension\n        if excel_file.suffix.lower() not in ['.xlsx', '.xls']:\n            raise ValueError(f\"Invalid Excel file extension: {excel_file.suffix}\")\n\n    def _create_fallback_invoice(self, context: ProcessingContext):\n        \"\"\"Create minimal invoice as fallback.\"\"\"\n        fallback_data = {\n            \"basic\": {\n                \"dataName\": f\"Fallback Invoice {context.index}\",\n                \"experimentDate\": datetime.now().strftime(\"%Y-%m-%d\"),\n            },\n            \"custom\": {},\n            \"processing\": {\n                \"mode\": \"excelinvoice_fallback\",\n                \"error\": \"Failed to process Excel file, using fallback\"\n            }\n        }\n\n        # Write fallback invoice\n        invoice_file = context.invoice_dst_filepath\n        invoice_file.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(invoice_file, 'w', encoding='utf-8') as f:\n            json.dump(fallback_data, f, indent=2, ensure_ascii=False)\n\n        print(f\"Created fallback invoice: {invoice_file}\")\n\n# Usage\nrobust_excel_init = RobustExcelInvoiceInitializer()\nrobust_excel_init.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#smarttable-field-mapping","title":"SmartTable Field Mapping","text":""},{"location":"rdetoolkit/processing/processors/invoice/#custom-field-mapping","title":"Custom Field Mapping","text":"<pre><code>from rdetoolkit.processing.processors import SmartTableInvoiceInitializer\nimport json\n\nclass CustomSmartTableInitializer(SmartTableInvoiceInitializer):\n    \"\"\"SmartTable initializer with custom field mapping.\"\"\"\n\n    def __init__(self, custom_mapping=None):\n        self.custom_mapping = custom_mapping or {}\n\n    def _process_mapping_key(self, row_data, mapping_key, invoice_data):\n        \"\"\"Process mapping with custom rules.\"\"\"\n\n        # Check for custom mapping first\n        if mapping_key in self.custom_mapping:\n            custom_processor = self.custom_mapping[mapping_key]\n            return custom_processor(row_data, invoice_data)\n\n        # Fall back to default processing\n        return super()._process_mapping_key(row_data, mapping_key, invoice_data)\n\n# Define custom field processors\ndef process_temperature_field(row_data, invoice_data):\n    \"\"\"Custom processor for temperature fields.\"\"\"\n    temp_celsius = row_data.get('temperature_c')\n    if temp_celsius:\n        # Convert to Kelvin and store both\n        temp_kelvin = float(temp_celsius) + 273.15\n        return {\n            'temperature': {\n                'celsius': float(temp_celsius),\n                'kelvin': temp_kelvin,\n                'unit': 'C'\n            }\n        }\n    return {}\n\ndef process_sample_info(row_data, invoice_data):\n    \"\"\"Custom processor for sample information.\"\"\"\n    return {\n        'sample': {\n            'id': row_data.get('sample_id', ''),\n            'name': row_data.get('sample_name', ''),\n            'batch': row_data.get('batch_number', ''),\n            'prepared_date': row_data.get('prep_date', '')\n        }\n    }\n\n# Usage with custom mapping\ncustom_mapping = {\n    'temperature': process_temperature_field,\n    'sample_info': process_sample_info,\n}\n\ncustom_init = CustomSmartTableInitializer(custom_mapping)\ncustom_init.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/invoice/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Large Excel Files: Consider memory usage when processing large Excel files</li> <li>CSV Parsing: Use efficient CSV parsing for large SmartTable files</li> <li>File I/O: Minimize file read/write operations</li> <li>Error Recovery: Implement fallback strategies for critical processing</li> <li>Validation: Validate data early to prevent downstream errors</li> </ul>"},{"location":"rdetoolkit/processing/processors/invoice/#see-also","title":"See Also","text":"<ul> <li>Processing Context - Context management and mode configuration</li> <li>Pipeline Architecture - Core pipeline classes</li> <li>File Processors - File copying and management</li> <li>Validation Processors - Invoice and metadata validation</li> </ul>"},{"location":"rdetoolkit/processing/processors/thumbnails/","title":"Thumbnail Processing Processor","text":"<p>The <code>rdetoolkit.processing.processors.thumbnails</code> module provides a processor for generating thumbnail images from source images. This processor creates optimized thumbnail versions of images for quick preview and display purposes.</p>"},{"location":"rdetoolkit/processing/processors/thumbnails/#overview","title":"Overview","text":"<p>The thumbnail processor provides:</p> <ul> <li>Automatic Thumbnail Generation: Create thumbnails from main image files</li> <li>Conditional Processing: Generate thumbnails only when enabled in configuration</li> <li>Error Tolerance: Non-critical processing that continues on errors</li> <li>Image Optimization: Efficient thumbnail creation with appropriate sizing</li> <li>Batch Processing: Handle multiple images in a single processing run</li> </ul>"},{"location":"rdetoolkit/processing/processors/thumbnails/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#thumbnailgenerator","title":"ThumbnailGenerator","text":"<p>Generates thumbnail images from source images in the main_image directory.</p>"},{"location":"rdetoolkit/processing/processors/thumbnails/#constructor","title":"Constructor","text":"<pre><code>ThumbnailGenerator()\n</code></pre> <p>No parameters required. Inherits from <code>Processor</code> base class.</p>"},{"location":"rdetoolkit/processing/processors/thumbnails/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#processcontext","title":"process(context)","text":"<p>Generate thumbnails if enabled in configuration.</p> <pre><code>def process(context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing configuration and resource paths</p> <p>Returns: - <code>None</code></p> <p>Raises: - Does not raise exceptions for thumbnail generation failures (logs warnings instead)</p> <p>Example: <pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\n\ngenerator = ThumbnailGenerator()\ngenerator.process(context)  # Generates thumbnails if enabled\n</code></pre></p> <p>Required Context Attributes: - <code>context.srcpaths.config.system.save_thumbnail_image</code>: Boolean flag to enable/disable thumbnail generation - <code>context.resource_paths.thumbnail</code>: Path to thumbnail output directory - <code>context.resource_paths.main_image</code>: Path to main image source directory</p> <p>Processing Logic: 1. Check if thumbnail generation is enabled in configuration 2. Verify main image directory contains image files 3. Generate thumbnails for all supported image formats 4. Save thumbnails to designated thumbnail directory 5. Handle errors gracefully without interrupting the processing pipeline</p>"},{"location":"rdetoolkit/processing/processors/thumbnails/#image-processing-details","title":"Image Processing Details","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#supported-image-formats","title":"Supported Image Formats","text":"<p>The thumbnail generator supports common image formats: - JPEG (.jpg, .jpeg) - PNG (.png) - TIFF (.tiff, .tif) - BMP (.bmp) - GIF (.gif)</p>"},{"location":"rdetoolkit/processing/processors/thumbnails/#thumbnail-specifications","title":"Thumbnail Specifications","text":"<ul> <li>Default Size: Optimized for quick loading and display</li> <li>Aspect Ratio: Preserved from original images</li> <li>Quality: Balanced between file size and visual quality</li> <li>Format: Output format matches input format where possible</li> </ul>"},{"location":"rdetoolkit/processing/processors/thumbnails/#generation-process","title":"Generation Process","text":"<ol> <li>Source Detection: Scan main_image directory for supported image files</li> <li>Size Calculation: Calculate optimal thumbnail dimensions</li> <li>Image Processing: Resize images while preserving aspect ratio</li> <li>Format Conversion: Convert to appropriate thumbnail format</li> <li>Output: Save thumbnails with consistent naming convention</li> </ol>"},{"location":"rdetoolkit/processing/processors/thumbnails/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#basic-thumbnail-generation","title":"Basic Thumbnail Generation","text":"<pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\nfrom rdetoolkit.processing.context import ProcessingContext\nfrom pathlib import Path\n\n# Create thumbnail generator\ngenerator = ThumbnailGenerator()\n\n# Create processing context with thumbnail generation enabled\ncontext = ProcessingContext(\n    srcpaths=srcpaths,  # srcpaths.config.system.save_thumbnail_image = True\n    resource_paths=resource_paths,  # Contains thumbnail and main_image paths\n    # ... other parameters\n)\n\n# Generate thumbnails\ngenerator.process(context)\nprint(\"Thumbnail generation completed\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#conditional-thumbnail-processing","title":"Conditional Thumbnail Processing","text":"<pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\n\ndef process_thumbnails_conditionally(context):\n    \"\"\"Generate thumbnails with configuration check.\"\"\"\n\n    # Check if thumbnail generation is enabled\n    if not context.srcpaths.config.system.save_thumbnail_image:\n        print(\"Thumbnail generation disabled, skipping\")\n        return\n\n    # Check if main images exist\n    main_image_path = context.resource_paths.main_image\n    if not main_image_path.exists() or not any(main_image_path.iterdir()):\n        print(\"No main images found for thumbnail generation\")\n        return\n\n    # Generate thumbnails\n    generator = ThumbnailGenerator()\n    generator.process(context)\n    print(\"Thumbnails generated successfully\")\n\n# Usage\nprocess_thumbnails_conditionally(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#thumbnail-generation-with-monitoring","title":"Thumbnail Generation with Monitoring","text":"<pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\nfrom pathlib import Path\nimport logging\n\ndef generate_thumbnails_with_monitoring(context):\n    \"\"\"Generate thumbnails with detailed monitoring.\"\"\"\n\n    logger = logging.getLogger(__name__)\n\n    # Pre-processing checks\n    main_image_dir = context.resource_paths.main_image\n    thumbnail_dir = context.resource_paths.thumbnail\n\n    if not main_image_dir.exists():\n        logger.warning(f\"Main image directory not found: {main_image_dir}\")\n        return\n\n    # Count source images\n    image_extensions = {'.jpg', '.jpeg', '.png', '.tiff', '.tif', '.bmp', '.gif'}\n    source_images = [\n        f for f in main_image_dir.iterdir()\n        if f.is_file() and f.suffix.lower() in image_extensions\n    ]\n\n    logger.info(f\"Found {len(source_images)} source images for thumbnail generation\")\n\n    # Generate thumbnails\n    generator = ThumbnailGenerator()\n    generator.process(context)\n\n    # Post-processing verification\n    if thumbnail_dir.exists():\n        thumbnail_files = list(thumbnail_dir.glob('*'))\n        logger.info(f\"Generated {len(thumbnail_files)} thumbnail files\")\n\n        # Log thumbnail details\n        for thumb_file in thumbnail_files:\n            size = thumb_file.stat().st_size\n            logger.debug(f\"Thumbnail: {thumb_file.name} ({size} bytes)\")\n    else:\n        logger.warning(\"Thumbnail directory not created\")\n\n# Usage\ngenerate_thumbnails_with_monitoring(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#batch-thumbnail-processing","title":"Batch Thumbnail Processing","text":"<pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\nfrom pathlib import Path\n\ndef batch_thumbnail_processing(contexts):\n    \"\"\"Process thumbnails for multiple datasets.\"\"\"\n\n    generator = ThumbnailGenerator()\n    results = []\n\n    for i, context in enumerate(contexts):\n        print(f\"Processing thumbnails for dataset {i+1}/{len(contexts)}\")\n\n        try:\n            # Check configuration\n            if context.srcpaths.config.system.save_thumbnail_image:\n                generator.process(context)\n                results.append({\n                    \"dataset\": i,\n                    \"status\": \"completed\",\n                    \"thumbnail_dir\": str(context.resource_paths.thumbnail)\n                })\n            else:\n                results.append({\n                    \"dataset\": i,\n                    \"status\": \"skipped\",\n                    \"reason\": \"thumbnail generation disabled\"\n                })\n\n        except Exception as e:\n            # This should rarely happen as ThumbnailGenerator handles errors internally\n            results.append({\n                \"dataset\": i,\n                \"status\": \"error\",\n                \"error\": str(e)\n            })\n\n    return results\n\n# Create multiple contexts for batch processing\ncontexts = [\n    ProcessingContext(\n        resource_paths=ResourcePaths(\n            main_image=Path(f\"dataset_{i}/main_image\"),\n            thumbnail=Path(f\"dataset_{i}/thumbnail\")\n        ),\n        srcpaths=srcpaths\n    )\n    for i in range(5)\n]\n\n# Process batch\nresults = batch_thumbnail_processing(contexts)\ncompleted = len([r for r in results if r['status'] == 'completed'])\nprint(f\"Processed thumbnails for {completed} datasets\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#custom-thumbnail-workflow","title":"Custom Thumbnail Workflow","text":"<pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\nfrom pathlib import Path\nimport shutil\n\nclass ThumbnailWorkflow:\n    \"\"\"Custom workflow for thumbnail processing with additional features.\"\"\"\n\n    def __init__(self, backup_enabled=False):\n        self.backup_enabled = backup_enabled\n        self.processing_stats = {\n            \"processed\": 0,\n            \"skipped\": 0,\n            \"errors\": 0\n        }\n\n    def process_with_backup(self, context):\n        \"\"\"Process thumbnails with optional backup.\"\"\"\n\n        thumbnail_dir = context.resource_paths.thumbnail\n\n        # Create backup if enabled\n        backup_dir = None\n        if self.backup_enabled and thumbnail_dir.exists():\n            backup_dir = self._create_backup(thumbnail_dir)\n\n        try:\n            # Generate thumbnails\n            generator = ThumbnailGenerator()\n            generator.process(context)\n\n            self.processing_stats[\"processed\"] += 1\n            return True\n\n        except Exception as e:\n            self.processing_stats[\"errors\"] += 1\n\n            # Restore backup if generation failed\n            if backup_dir:\n                self._restore_backup(backup_dir, thumbnail_dir)\n\n            print(f\"Thumbnail processing failed: {e}\")\n            return False\n\n    def _create_backup(self, thumbnail_dir: Path) -&gt; Path:\n        \"\"\"Create backup of existing thumbnails.\"\"\"\n        backup_dir = thumbnail_dir.parent / f\"{thumbnail_dir.name}_backup\"\n        if backup_dir.exists():\n            shutil.rmtree(backup_dir)\n        shutil.copytree(thumbnail_dir, backup_dir)\n        return backup_dir\n\n    def _restore_backup(self, backup_dir: Path, thumbnail_dir: Path):\n        \"\"\"Restore thumbnails from backup.\"\"\"\n        if thumbnail_dir.exists():\n            shutil.rmtree(thumbnail_dir)\n        shutil.copytree(backup_dir, thumbnail_dir)\n        shutil.rmtree(backup_dir)  # Clean up backup\n\n    def get_statistics(self):\n        \"\"\"Get processing statistics.\"\"\"\n        return self.processing_stats.copy()\n\n# Usage\nworkflow = ThumbnailWorkflow(backup_enabled=True)\nsuccess = workflow.process_with_backup(context)\nstats = workflow.get_statistics()\nprint(f\"Processing stats: {stats}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#thumbnail-quality-verification","title":"Thumbnail Quality Verification","text":"<pre><code>from rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\nfrom pathlib import Path\nfrom PIL import Image\nimport logging\n\ndef verify_thumbnail_quality(context):\n    \"\"\"Generate thumbnails and verify quality.\"\"\"\n\n    logger = logging.getLogger(__name__)\n\n    # Generate thumbnails\n    generator = ThumbnailGenerator()\n    generator.process(context)\n\n    # Verify generated thumbnails\n    thumbnail_dir = context.resource_paths.thumbnail\n    main_image_dir = context.resource_paths.main_image\n\n    if not thumbnail_dir.exists():\n        logger.warning(\"No thumbnail directory created\")\n        return\n\n    # Check each thumbnail\n    verification_results = []\n\n    for thumb_file in thumbnail_dir.iterdir():\n        if thumb_file.is_file():\n            try:\n                # Open and verify thumbnail\n                with Image.open(thumb_file) as img:\n                    width, height = img.size\n                    format_name = img.format\n\n                    # Find corresponding source image\n                    source_file = find_source_image(thumb_file.stem, main_image_dir)\n\n                    verification_results.append({\n                        \"thumbnail\": thumb_file.name,\n                        \"size\": f\"{width}x{height}\",\n                        \"format\": format_name,\n                        \"source_found\": source_file is not None,\n                        \"file_size\": thumb_file.stat().st_size\n                    })\n\n            except Exception as e:\n                logger.error(f\"Failed to verify thumbnail {thumb_file}: {e}\")\n                verification_results.append({\n                    \"thumbnail\": thumb_file.name,\n                    \"error\": str(e)\n                })\n\n    # Log verification results\n    valid_thumbnails = [r for r in verification_results if \"error\" not in r]\n    logger.info(f\"Verified {len(valid_thumbnails)} valid thumbnails\")\n\n    return verification_results\n\ndef find_source_image(thumb_stem: str, main_image_dir: Path) -&gt; Path:\n    \"\"\"Find source image for thumbnail.\"\"\"\n    extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif', '.bmp', '.gif']\n\n    for ext in extensions:\n        source_file = main_image_dir / f\"{thumb_stem}{ext}\"\n        if source_file.exists():\n            return source_file\n\n    return None\n\n# Usage\nresults = verify_thumbnail_quality(context)\nprint(f\"Verification completed for {len(results)} thumbnails\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#integration-with-processing-pipeline","title":"Integration with Processing Pipeline","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>from rdetoolkit.processing.pipeline import ProcessingPipeline\nfrom rdetoolkit.processing.processors.thumbnails import ThumbnailGenerator\n\n# Create processing pipeline\npipeline = ProcessingPipeline()\n\n# Add processors in logical order\n# pipeline.add_processor(FileProcessor())  # Copy images to main_image\n# pipeline.add_processor(ImageProcessor())  # Process main images\n\n# Add thumbnail generator after image processing\npipeline.add_processor(ThumbnailGenerator())\n\n# Add final processors\n# pipeline.add_processor(ValidationProcessor())\n\n# Execute pipeline\npipeline.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#conditional-pipeline-processing","title":"Conditional Pipeline Processing","text":"<pre><code>def create_image_processing_pipeline(config):\n    \"\"\"Create pipeline with conditional thumbnail generation.\"\"\"\n\n    pipeline = ProcessingPipeline()\n\n    # Add standard image processors\n    pipeline.add_processor(ImageFileProcessor())\n    pipeline.add_processor(ImageResizer())\n\n    # Add thumbnail generator only if enabled\n    if config.system.save_thumbnail_image:\n        pipeline.add_processor(ThumbnailGenerator())\n\n    return pipeline\n\n# Usage\npipeline = create_image_processing_pipeline(config)\npipeline.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#error-tolerance","title":"Error Tolerance","text":"<p>The ThumbnailGenerator is designed to be error-tolerant:</p> <pre><code># Internal error handling (from the processor implementation)\ntry:\n    img2thumb.copy_images_to_thumbnail(\n        context.resource_paths.thumbnail,\n        context.resource_paths.main_image,\n    )\n    logger.debug(\"Thumbnail generation completed successfully\")\nexcept Exception as e:\n    logger.warning(f\"Thumbnail generation failed: {str(e)}\")\n    # Don't raise the exception as thumbnail generation is not critical\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always check configuration before processing:    <pre><code>if context.srcpaths.config.system.save_thumbnail_image:\n    generator.process(context)\nelse:\n    logger.debug(\"Thumbnail generation disabled\")\n</code></pre></p> </li> <li> <p>Verify source images exist:    <pre><code>main_image_dir = context.resource_paths.main_image\nif main_image_dir.exists() and any(main_image_dir.iterdir()):\n    generator.process(context)\nelse:\n    logger.info(\"No source images found for thumbnail generation\")\n</code></pre></p> </li> <li> <p>Handle thumbnail generation as optional:    <pre><code>try:\n    generator.process(context)\nexcept Exception as e:\n    # Thumbnail generation failures should not stop processing\n    logger.warning(f\"Thumbnail generation failed, continuing: {e}\")\n</code></pre></p> </li> <li> <p>Monitor disk space for thumbnail storage:    <pre><code>import shutil\n\n# Check available space\ntotal, used, free = shutil.disk_usage(context.resource_paths.thumbnail.parent)\nif free &lt; estimated_thumbnail_size:\n    logger.warning(\"Low disk space, thumbnail generation may fail\")\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/processing/processors/thumbnails/#configuration-dependencies","title":"Configuration Dependencies","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#system-configuration","title":"System Configuration","text":"<p>Thumbnail processing depends on system configuration:</p> <pre><code>system:\n  save_thumbnail_image: true  # Enable thumbnail generation\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#directory-structure","title":"Directory Structure","text":"<p>Thumbnail processing requires: - Main Image Directory: <code>context.resource_paths.main_image</code> containing source images - Thumbnail Directory: <code>context.resource_paths.thumbnail</code> for output thumbnails</p>"},{"location":"rdetoolkit/processing/processors/thumbnails/#image-processing-dependencies","title":"Image Processing Dependencies","text":"<ul> <li>img2thumb module: Core image processing functionality</li> <li>PIL/Pillow: Image processing library (dependency of img2thumb)</li> <li>Supported formats: Depends on PIL/Pillow installation</li> </ul>"},{"location":"rdetoolkit/processing/processors/thumbnails/#performance-notes","title":"Performance Notes","text":"<ul> <li>Thumbnail generation is CPU-intensive for large images</li> <li>Processing is optimized for batch operations on multiple images</li> <li>Memory usage is managed efficiently during image processing</li> <li>Error handling is designed to not interrupt the processing pipeline</li> <li>Thumbnail generation is performed in parallel where possible</li> </ul>"},{"location":"rdetoolkit/processing/processors/thumbnails/#use-cases","title":"Use Cases","text":""},{"location":"rdetoolkit/processing/processors/thumbnails/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Web Display: Generate thumbnails for web-based image galleries</li> <li>Quick Preview: Create small images for rapid preview in applications</li> <li>Index Generation: Generate image indexes with thumbnail previews</li> <li>Bandwidth Optimization: Reduce bandwidth usage for image previews</li> <li>Mobile Applications: Provide optimized images for mobile displays</li> </ol>"},{"location":"rdetoolkit/processing/processors/thumbnails/#example-directory-structure","title":"Example Directory Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 main_image/           # Source images\n\u2502   \u251c\u2500\u2500 image001.jpg\n\u2502   \u251c\u2500\u2500 image002.png\n\u2502   \u2514\u2500\u2500 image003.tiff\n\u2514\u2500\u2500 thumbnail/            # Generated thumbnails\n    \u251c\u2500\u2500 image001.jpg      # Thumbnail version\n    \u251c\u2500\u2500 image002.png      # Thumbnail version\n    \u2514\u2500\u2500 image003.jpg      # Converted thumbnail\n</code></pre>"},{"location":"rdetoolkit/processing/processors/thumbnails/#see-also","title":"See Also","text":"<ul> <li>Processing Context - For understanding processing context structure</li> <li>Pipeline Documentation - For processor pipeline integration</li> <li>Image Processing - For core image processing utilities</li> <li>Configuration Guide - For system configuration options</li> </ul>"},{"location":"rdetoolkit/processing/processors/validation/","title":"Validation Processors","text":"<p>The <code>rdetoolkit.processing.processors.validation</code> module provides validation processors that ensure data integrity and compliance with schemas. These processors validate invoice files and metadata against predefined schemas and validation rules.</p>"},{"location":"rdetoolkit/processing/processors/validation/#overview","title":"Overview","text":"<p>The validation processors provide:</p> <ul> <li>Invoice Validation: Validate invoice.json files against JSON schemas</li> <li>Metadata Validation: Validate metadata.json files for completeness and format</li> <li>Schema Compliance: Ensure data structure compliance with RDE standards</li> <li>Error Reporting: Comprehensive error reporting for validation failures</li> <li>Optional Validation: Graceful handling when files don't exist</li> </ul>"},{"location":"rdetoolkit/processing/processors/validation/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/validation/#invoicevalidator","title":"InvoiceValidator","text":"<p>Validates invoice files against JSON schema specifications.</p>"},{"location":"rdetoolkit/processing/processors/validation/#constructor","title":"Constructor","text":"<pre><code>InvoiceValidator()\n</code></pre> <p>No parameters required. Inherits from <code>Processor</code> base class.</p>"},{"location":"rdetoolkit/processing/processors/validation/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/validation/#processcontext","title":"process(context)","text":"<p>Validate invoice.json against schema.</p> <pre><code>def process(context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing invoice and schema paths</p> <p>Returns: - <code>None</code></p> <p>Raises: - <code>Exception</code>: If invoice validation fails</p> <p>Example: <pre><code>from rdetoolkit.processing.processors.validation import InvoiceValidator\n\nvalidator = InvoiceValidator()\nvalidator.process(context)  # Validates invoice against schema\n</code></pre></p> <p>Required Context Attributes: - <code>context.invoice_dst_filepath</code>: Path to invoice.json file to validate - <code>context.schema_path</code>: Path to JSON schema file for validation</p> <p>Validation Process: 1. Loads invoice.json file 2. Loads JSON schema file 3. Performs schema validation using jsonschema library 4. Reports any validation errors with detailed messages</p>"},{"location":"rdetoolkit/processing/processors/validation/#metadatavalidator","title":"MetadataValidator","text":"<p>Validates metadata.json files for completeness and format compliance.</p>"},{"location":"rdetoolkit/processing/processors/validation/#constructor_1","title":"Constructor","text":"<pre><code>MetadataValidator()\n</code></pre> <p>No parameters required. Inherits from <code>Processor</code> base class.</p>"},{"location":"rdetoolkit/processing/processors/validation/#methods_1","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/validation/#processcontext_1","title":"process(context)","text":"<p>Validate metadata.json if it exists.</p> <pre><code>def process(context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing metadata path</p> <p>Returns: - <code>None</code></p> <p>Raises: - <code>Exception</code>: If metadata validation fails</p> <p>Example: <pre><code>from rdetoolkit.processing.processors.validation import MetadataValidator\n\nvalidator = MetadataValidator()\nvalidator.process(context)  # Validates metadata if present\n</code></pre></p> <p>Required Context Attributes: - <code>context.metadata_path</code>: Path to metadata.json file</p> <p>Validation Behavior: - If metadata.json doesn't exist, validation is skipped gracefully - If metadata.json exists, it's validated for format and completeness - Validation errors are logged and raised for pipeline handling</p>"},{"location":"rdetoolkit/processing/processors/validation/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/validation/#basic-invoice-validation","title":"Basic Invoice Validation","text":"<pre><code>from rdetoolkit.processing.processors.validation import InvoiceValidator\nfrom rdetoolkit.processing.context import ProcessingContext\nfrom pathlib import Path\n\n# Create invoice validator\nvalidator = InvoiceValidator()\n\n# Create processing context with paths\ncontext = ProcessingContext(\n    invoice_dst_filepath=Path(\"output/invoice.json\"),\n    schema_path=Path(\"schemas/invoice_schema.json\"),\n    # ... other parameters\n)\n\n# Validate invoice\ntry:\n    validator.process(context)\n    print(\"Invoice validation passed\")\nexcept Exception as e:\n    print(f\"Invoice validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#basic-metadata-validation","title":"Basic Metadata Validation","text":"<pre><code>from rdetoolkit.processing.processors.validation import MetadataValidator\nfrom pathlib import Path\n\n# Create metadata validator\nvalidator = MetadataValidator()\n\n# Create processing context\ncontext = ProcessingContext(\n    metadata_path=Path(\"output/metadata.json\"),\n    # ... other parameters\n)\n\n# Validate metadata\ntry:\n    validator.process(context)\n    print(\"Metadata validation passed\")\nexcept Exception as e:\n    print(f\"Metadata validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#combined-validation-pipeline","title":"Combined Validation Pipeline","text":"<pre><code>from rdetoolkit.processing.processors.validation import InvoiceValidator, MetadataValidator\nfrom rdetoolkit.processing.pipeline import ProcessingPipeline\n\n# Create validation pipeline\npipeline = ProcessingPipeline()\n\n# Add validation processors\npipeline.add_processor(InvoiceValidator())\npipeline.add_processor(MetadataValidator())\n\n# Execute validation pipeline\ntry:\n    pipeline.process(context)\n    print(\"All validation checks passed\")\nexcept Exception as e:\n    print(f\"Validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#validation-with-error-handling","title":"Validation with Error Handling","text":"<pre><code>from rdetoolkit.processing.processors.validation import InvoiceValidator, MetadataValidator\nimport logging\n\n# Setup logging\nlogger = logging.getLogger(__name__)\n\ndef validate_with_detailed_logging(context):\n    \"\"\"Validate with detailed error logging.\"\"\"\n\n    # Invoice validation\n    invoice_validator = InvoiceValidator()\n    try:\n        invoice_validator.process(context)\n        logger.info(\"Invoice validation: PASSED\")\n    except Exception as e:\n        logger.error(f\"Invoice validation: FAILED - {e}\")\n        raise\n\n    # Metadata validation\n    metadata_validator = MetadataValidator()\n    try:\n        metadata_validator.process(context)\n        logger.info(\"Metadata validation: PASSED\")\n    except Exception as e:\n        logger.error(f\"Metadata validation: FAILED - {e}\")\n        raise\n\n    logger.info(\"All validation checks completed successfully\")\n\n# Execute validation\ntry:\n    validate_with_detailed_logging(context)\nexcept Exception as e:\n    logger.error(f\"Validation pipeline failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#conditional-validation","title":"Conditional Validation","text":"<pre><code>from rdetoolkit.processing.processors.validation import InvoiceValidator, MetadataValidator\nfrom pathlib import Path\n\ndef conditional_validation(context):\n    \"\"\"Perform validation based on file existence.\"\"\"\n\n    validation_results = {\n        \"invoice_validated\": False,\n        \"metadata_validated\": False,\n        \"errors\": []\n    }\n\n    # Check if invoice exists before validation\n    if context.invoice_dst_filepath.exists():\n        try:\n            invoice_validator = InvoiceValidator()\n            invoice_validator.process(context)\n            validation_results[\"invoice_validated\"] = True\n        except Exception as e:\n            validation_results[\"errors\"].append(f\"Invoice validation: {e}\")\n    else:\n        validation_results[\"errors\"].append(\"Invoice file not found\")\n\n    # Check if metadata exists before validation\n    if context.metadata_path.exists():\n        try:\n            metadata_validator = MetadataValidator()\n            metadata_validator.process(context)\n            validation_results[\"metadata_validated\"] = True\n        except Exception as e:\n            validation_results[\"errors\"].append(f\"Metadata validation: {e}\")\n    else:\n        print(\"Metadata file not found - skipping validation\")\n\n    return validation_results\n\n# Execute conditional validation\nresults = conditional_validation(context)\nprint(f\"Validation results: {results}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#custom-validation-workflow","title":"Custom Validation Workflow","text":"<pre><code>from rdetoolkit.processing.processors.validation import InvoiceValidator, MetadataValidator\nimport json\nfrom pathlib import Path\n\nclass ValidationWorkflow:\n    \"\"\"Custom validation workflow with reporting.\"\"\"\n\n    def __init__(self, output_dir: Path):\n        self.output_dir = output_dir\n        self.validation_report = {\n            \"timestamp\": None,\n            \"invoice_validation\": {\"status\": \"pending\", \"errors\": []},\n            \"metadata_validation\": {\"status\": \"pending\", \"errors\": []},\n            \"overall_status\": \"pending\"\n        }\n\n    def run_validation(self, context):\n        \"\"\"Run complete validation workflow.\"\"\"\n        import datetime\n\n        self.validation_report[\"timestamp\"] = datetime.datetime.now().isoformat()\n\n        # Invoice validation\n        self._validate_invoice(context)\n\n        # Metadata validation\n        self._validate_metadata(context)\n\n        # Determine overall status\n        self._determine_overall_status()\n\n        # Save validation report\n        self._save_report()\n\n        return self.validation_report\n\n    def _validate_invoice(self, context):\n        \"\"\"Validate invoice with error capture.\"\"\"\n        try:\n            validator = InvoiceValidator()\n            validator.process(context)\n            self.validation_report[\"invoice_validation\"][\"status\"] = \"passed\"\n        except Exception as e:\n            self.validation_report[\"invoice_validation\"][\"status\"] = \"failed\"\n            self.validation_report[\"invoice_validation\"][\"errors\"].append(str(e))\n\n    def _validate_metadata(self, context):\n        \"\"\"Validate metadata with error capture.\"\"\"\n        try:\n            validator = MetadataValidator()\n            validator.process(context)\n            self.validation_report[\"metadata_validation\"][\"status\"] = \"passed\"\n        except Exception as e:\n            self.validation_report[\"metadata_validation\"][\"status\"] = \"failed\"\n            self.validation_report[\"metadata_validation\"][\"errors\"].append(str(e))\n\n    def _determine_overall_status(self):\n        \"\"\"Determine overall validation status.\"\"\"\n        invoice_passed = self.validation_report[\"invoice_validation\"][\"status\"] == \"passed\"\n        metadata_passed = self.validation_report[\"metadata_validation\"][\"status\"] == \"passed\"\n\n        if invoice_passed and metadata_passed:\n            self.validation_report[\"overall_status\"] = \"passed\"\n        else:\n            self.validation_report[\"overall_status\"] = \"failed\"\n\n    def _save_report(self):\n        \"\"\"Save validation report to file.\"\"\"\n        report_path = self.output_dir / \"validation_report.json\"\n        with open(report_path, 'w') as f:\n            json.dump(self.validation_report, f, indent=2)\n\n# Usage\nworkflow = ValidationWorkflow(Path(\"output\"))\nreport = workflow.run_validation(context)\nprint(f\"Validation completed. Overall status: {report['overall_status']}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#schema-validation-details","title":"Schema Validation Details","text":""},{"location":"rdetoolkit/processing/processors/validation/#invoice-schema-validation","title":"Invoice Schema Validation","text":"<p>The invoice validation process follows these steps:</p> <ol> <li>Schema Loading: Load JSON schema from <code>context.schema_path</code></li> <li>Invoice Loading: Load invoice data from <code>context.invoice_dst_filepath</code></li> <li>Validation: Use jsonschema library to validate structure</li> <li>Error Reporting: Provide detailed error messages for failures</li> </ol> <p>Common Invoice Validation Errors: <pre><code># Missing required fields\n{\n    \"error\": \"ValidationError\",\n    \"message\": \"'dataName' is a required property\",\n    \"path\": \"$.basic\"\n}\n\n# Invalid data types\n{\n    \"error\": \"ValidationError\",\n    \"message\": \"'25.5' is not of type 'number'\",\n    \"path\": \"$.sample.generalAttributes[0].value\"\n}\n\n# Invalid enum values\n{\n    \"error\": \"ValidationError\",\n    \"message\": \"'invalid_status' is not one of ['active', 'inactive', 'pending']\",\n    \"path\": \"$.basic.status\"\n}\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/validation/#metadata-validation","title":"Metadata Validation","text":"<p>Metadata validation checks for:</p> <ul> <li>File Format: Valid JSON structure</li> <li>Required Fields: Presence of mandatory metadata fields</li> <li>Data Types: Correct data types for all fields</li> <li>Value Constraints: Valid values within acceptable ranges</li> </ul> <p>Example Metadata Structure: <pre><code>{\n    \"version\": \"1.0\",\n    \"created\": \"2024-01-01T00:00:00Z\",\n    \"modified\": \"2024-01-01T00:00:00Z\",\n    \"description\": \"Sample metadata\",\n    \"keywords\": [\"research\", \"data\"],\n    \"contributors\": [\n        {\n            \"name\": \"John Doe\",\n            \"role\": \"researcher\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/validation/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/processors/validation/#validation-error-types","title":"Validation Error Types","text":""},{"location":"rdetoolkit/processing/processors/validation/#schema-validation-errors","title":"Schema Validation Errors","text":"<pre><code>try:\n    validator.process(context)\nexcept jsonschema.ValidationError as e:\n    print(f\"Schema validation failed: {e.message}\")\n    print(f\"Failed at path: {e.absolute_path}\")\n    print(f\"Invalid value: {e.instance}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#file-not-found-errors","title":"File Not Found Errors","text":"<pre><code>try:\n    validator.process(context)\nexcept FileNotFoundError as e:\n    print(f\"Validation file not found: {e}\")\n    # Handle missing schema or data files\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#json-parse-errors","title":"JSON Parse Errors","text":"<pre><code>try:\n    validator.process(context)\nexcept json.JSONDecodeError as e:\n    print(f\"Invalid JSON format: {e}\")\n    print(f\"Error at line {e.lineno}, column {e.colno}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always handle validation errors gracefully:    <pre><code>try:\n    validator.process(context)\nexcept Exception as e:\n    logger.error(f\"Validation failed: {e}\")\n    # Decide whether to continue or abort processing\n</code></pre></p> </li> <li> <p>Verify file existence before validation:    <pre><code>if context.invoice_dst_filepath.exists():\n    validator.process(context)\nelse:\n    logger.warning(\"Invoice file not found for validation\")\n</code></pre></p> </li> <li> <p>Use appropriate logging levels:    <pre><code>logger.debug(\"Starting validation process\")\ntry:\n    validator.process(context)\n    logger.info(\"Validation completed successfully\")\nexcept Exception as e:\n    logger.error(f\"Validation failed: {e}\")\n</code></pre></p> </li> <li> <p>Validate schema files themselves:    <pre><code>if not context.schema_path.exists():\n    raise FileNotFoundError(f\"Schema file not found: {context.schema_path}\")\n\n# Optionally validate schema format\ntry:\n    with open(context.schema_path) as f:\n        json.load(f)\nexcept json.JSONDecodeError as e:\n    raise ValueError(f\"Invalid schema file format: {e}\")\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/processing/processors/validation/#integration-with-processing-pipeline","title":"Integration with Processing Pipeline","text":""},{"location":"rdetoolkit/processing/processors/validation/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>from rdetoolkit.processing.pipeline import ProcessingPipeline\nfrom rdetoolkit.processing.processors.validation import InvoiceValidator, MetadataValidator\n\n# Create processing pipeline\npipeline = ProcessingPipeline()\n\n# Add other processors first\n# pipeline.add_processor(InvoiceInitializer())\n# pipeline.add_processor(FileProcessor())\n\n# Add validation processors at the end\npipeline.add_processor(InvoiceValidator())\npipeline.add_processor(MetadataValidator())\n\n# Execute pipeline with validation\npipeline.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#validation-first-approach","title":"Validation-First Approach","text":"<pre><code># Validate before processing\ndef validate_inputs(context):\n    \"\"\"Validate inputs before main processing.\"\"\"\n    if context.metadata_path.exists():\n        metadata_validator = MetadataValidator()\n        metadata_validator.process(context)\n\n# Validate after processing\ndef validate_outputs(context):\n    \"\"\"Validate outputs after main processing.\"\"\"\n    invoice_validator = InvoiceValidator()\n    invoice_validator.process(context)\n\n# Main processing workflow\nvalidate_inputs(context)\n# ... main processing ...\nvalidate_outputs(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/validation/#performance-notes","title":"Performance Notes","text":"<ul> <li>Validation processors are lightweight with minimal overhead</li> <li>JSON schema validation performance depends on schema complexity</li> <li>File I/O operations are optimized for typical file sizes</li> <li>Validation errors include detailed path information for debugging</li> <li>Logging is optimized to minimize performance impact</li> </ul>"},{"location":"rdetoolkit/processing/processors/validation/#see-also","title":"See Also","text":"<ul> <li>Processing Context - For understanding processing context structure</li> <li>Pipeline Documentation - For processor pipeline integration</li> <li>Invoice Processors - For invoice creation and initialization</li> <li>Validation Module - For core validation functions</li> </ul>"},{"location":"rdetoolkit/processing/processors/variables/","title":"Variable Processing Processor","text":"<p>The <code>rdetoolkit.processing.processors.variables</code> module provides a processor for applying magic variables to invoice files. This processor enables dynamic variable substitution in invoice files using data from raw files.</p>"},{"location":"rdetoolkit/processing/processors/variables/#overview","title":"Overview","text":"<p>The variable processor provides:</p> <ul> <li>Magic Variable Substitution: Replace placeholder variables in invoice files with actual values</li> <li>Dynamic Content: Generate dynamic content based on raw file data</li> <li>Conditional Processing: Apply variables only when enabled in configuration</li> <li>File-Based Variables: Extract variable values from raw files</li> <li>Error Handling: Comprehensive error handling for variable processing</li> </ul>"},{"location":"rdetoolkit/processing/processors/variables/#classes","title":"Classes","text":""},{"location":"rdetoolkit/processing/processors/variables/#variableapplier","title":"VariableApplier","text":"<p>Applies magic variables to invoice files using data from raw files.</p>"},{"location":"rdetoolkit/processing/processors/variables/#constructor","title":"Constructor","text":"<pre><code>VariableApplier()\n</code></pre> <p>No parameters required. Inherits from <code>Processor</code> base class.</p>"},{"location":"rdetoolkit/processing/processors/variables/#methods","title":"Methods","text":""},{"location":"rdetoolkit/processing/processors/variables/#processcontext","title":"process(context)","text":"<p>Apply magic variables if enabled in configuration.</p> <pre><code>def process(context: ProcessingContext) -&gt; None\n</code></pre> <p>Parameters: - <code>context</code> (ProcessingContext): Processing context containing configuration and file paths</p> <p>Returns: - <code>None</code></p> <p>Raises: - <code>Exception</code>: If magic variable processing fails</p> <p>Example: <pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\n\napplier = VariableApplier()\napplier.process(context)  # Applies magic variables if enabled\n</code></pre></p> <p>Required Context Attributes: - <code>context.srcpaths.config.system.magic_variable</code>: Boolean flag to enable/disable magic variables - <code>context.invoice_dst_filepath</code>: Path to invoice file to process - <code>context.resource_paths.rawfiles</code>: Tuple of raw file paths for variable extraction</p> <p>Processing Logic: 1. Check if magic variables are enabled in configuration 2. Verify raw files are available for variable extraction 3. Apply magic variable replacement using the first raw file 4. Save updated invoice file with substituted variables</p>"},{"location":"rdetoolkit/processing/processors/variables/#magic-variable-system","title":"Magic Variable System","text":""},{"location":"rdetoolkit/processing/processors/variables/#variable-format","title":"Variable Format","text":"<p>Magic variables in invoice files use the format: <pre><code>${variable_name}\n</code></pre></p>"},{"location":"rdetoolkit/processing/processors/variables/#common-magic-variables","title":"Common Magic Variables","text":"<ul> <li><code>${filename}</code>: Original filename without extension</li> <li><code>${filepath}</code>: Full path to the file</li> <li><code>${filesize}</code>: File size in bytes</li> <li><code>${timestamp}</code>: File modification timestamp</li> <li><code>${index}</code>: Processing index</li> <li><code>${date}</code>: Current date</li> <li><code>${time}</code>: Current time</li> </ul>"},{"location":"rdetoolkit/processing/processors/variables/#variable-substitution-process","title":"Variable Substitution Process","text":"<ol> <li>Invoice Loading: Load invoice.json file</li> <li>Variable Detection: Scan for magic variable patterns</li> <li>Value Extraction: Extract values from raw files</li> <li>Substitution: Replace variables with actual values</li> <li>Save: Write updated invoice back to file</li> </ol>"},{"location":"rdetoolkit/processing/processors/variables/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"rdetoolkit/processing/processors/variables/#basic-magic-variable-processing","title":"Basic Magic Variable Processing","text":"<pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\nfrom rdetoolkit.processing.context import ProcessingContext\nfrom pathlib import Path\n\n# Create variable applier\napplier = VariableApplier()\n\n# Create processing context with magic variables enabled\ncontext = ProcessingContext(\n    srcpaths=srcpaths,  # srcpaths.config.system.magic_variable = True\n    invoice_dst_filepath=Path(\"output/invoice.json\"),\n    resource_paths=resource_paths,  # Contains rawfiles\n    # ... other parameters\n)\n\n# Apply magic variables\ntry:\n    applier.process(context)\n    print(\"Magic variables applied successfully\")\nexcept Exception as e:\n    print(f\"Magic variable processing failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#invoice-template-with-magic-variables","title":"Invoice Template with Magic Variables","text":"<pre><code># Example invoice.json template with magic variables\ninvoice_template = {\n    \"basic\": {\n        \"dataName\": \"${filename}\",\n        \"description\": \"Data file: ${filepath}\",\n        \"fileSize\": \"${filesize}\",\n        \"createdDate\": \"${date}\",\n        \"processedTime\": \"${time}\"\n    },\n    \"custom\": {\n        \"originalFilename\": \"${filename}\",\n        \"processingIndex\": \"${index}\",\n        \"lastModified\": \"${timestamp}\"\n    },\n    \"sample\": {\n        \"names\": [\"Sample from ${filename}\"],\n        \"generalAttributes\": [\n            {\n                \"termId\": \"sourceFile\",\n                \"value\": \"${filepath}\"\n            },\n            {\n                \"termId\": \"fileSize\",\n                \"value\": \"${filesize}\"\n            }\n        ]\n    }\n}\n\n# After processing, variables would be replaced with actual values:\nprocessed_invoice = {\n    \"basic\": {\n        \"dataName\": \"experiment_data\",\n        \"description\": \"Data file: /data/raw/experiment_data.csv\",\n        \"fileSize\": \"1024\",\n        \"createdDate\": \"2024-01-01\",\n        \"processedTime\": \"12:00:00\"\n    },\n    \"custom\": {\n        \"originalFilename\": \"experiment_data\",\n        \"processingIndex\": \"001\",\n        \"lastModified\": \"2024-01-01T10:30:00\"\n    },\n    \"sample\": {\n        \"names\": [\"Sample from experiment_data\"],\n        \"generalAttributes\": [\n            {\n                \"termId\": \"sourceFile\",\n                \"value\": \"/data/raw/experiment_data.csv\"\n            },\n            {\n                \"termId\": \"fileSize\",\n                \"value\": \"1024\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#conditional-magic-variable-processing","title":"Conditional Magic Variable Processing","text":"<pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\n\ndef process_with_variable_check(context):\n    \"\"\"Process with magic variable configuration check.\"\"\"\n\n    # Check configuration\n    if not context.srcpaths.config.system.magic_variable:\n        print(\"Magic variables disabled, skipping processing\")\n        return\n\n    # Check for raw files\n    if not context.resource_paths.rawfiles:\n        print(\"No raw files available for variable extraction\")\n        return\n\n    # Apply magic variables\n    applier = VariableApplier()\n    try:\n        applier.process(context)\n        print(\"Magic variables processed successfully\")\n    except Exception as e:\n        print(f\"Magic variable processing failed: {e}\")\n\n# Usage\nprocess_with_variable_check(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#magic-variable-processing-with-multiple-files","title":"Magic Variable Processing with Multiple Files","text":"<pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\nfrom pathlib import Path\n\ndef process_multiple_files(contexts):\n    \"\"\"Process magic variables for multiple files.\"\"\"\n\n    applier = VariableApplier()\n    results = []\n\n    for i, context in enumerate(contexts):\n        try:\n            print(f\"Processing file {i+1}/{len(contexts)}\")\n            applier.process(context)\n            results.append({\n                \"index\": i,\n                \"status\": \"success\",\n                \"file\": str(context.invoice_dst_filepath)\n            })\n        except Exception as e:\n            results.append({\n                \"index\": i,\n                \"status\": \"failed\",\n                \"file\": str(context.invoice_dst_filepath),\n                \"error\": str(e)\n            })\n\n    return results\n\n# Create multiple contexts\ncontexts = [\n    ProcessingContext(\n        invoice_dst_filepath=Path(f\"output/invoice_{i:03d}.json\"),\n        resource_paths=resource_paths,\n        srcpaths=srcpaths\n    )\n    for i in range(10)\n]\n\n# Process all files\nresults = process_multiple_files(contexts)\nprint(f\"Processed {len([r for r in results if r['status'] == 'success'])} files successfully\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#custom-magic-variable-workflow","title":"Custom Magic Variable Workflow","text":"<pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\nimport json\nfrom pathlib import Path\n\nclass MagicVariableWorkflow:\n    \"\"\"Custom workflow for magic variable processing.\"\"\"\n\n    def __init__(self, backup_dir: Path):\n        self.backup_dir = backup_dir\n        self.backup_dir.mkdir(parents=True, exist_ok=True)\n\n    def process_with_backup(self, context):\n        \"\"\"Process magic variables with backup creation.\"\"\"\n\n        # Create backup of original invoice\n        backup_path = self._create_backup(context.invoice_dst_filepath)\n\n        try:\n            # Apply magic variables\n            applier = VariableApplier()\n            applier.process(context)\n\n            print(f\"Magic variables applied. Backup saved to: {backup_path}\")\n            return True\n\n        except Exception as e:\n            # Restore from backup on failure\n            self._restore_backup(backup_path, context.invoice_dst_filepath)\n            print(f\"Magic variable processing failed, restored from backup: {e}\")\n            return False\n\n    def _create_backup(self, invoice_path: Path) -&gt; Path:\n        \"\"\"Create backup of invoice file.\"\"\"\n        if invoice_path.exists():\n            backup_path = self.backup_dir / f\"{invoice_path.stem}_backup.json\"\n            shutil.copy2(invoice_path, backup_path)\n            return backup_path\n        return None\n\n    def _restore_backup(self, backup_path: Path, invoice_path: Path):\n        \"\"\"Restore invoice from backup.\"\"\"\n        if backup_path and backup_path.exists():\n            shutil.copy2(backup_path, invoice_path)\n\n# Usage\nworkflow = MagicVariableWorkflow(Path(\"backups\"))\nsuccess = workflow.process_with_backup(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#pre-processing-variable-validation","title":"Pre-Processing Variable Validation","text":"<pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\nimport json\nimport re\n\ndef validate_magic_variables(invoice_path: Path, available_variables: set):\n    \"\"\"Validate magic variables in invoice before processing.\"\"\"\n\n    # Load invoice\n    with open(invoice_path) as f:\n        invoice_data = json.load(f)\n\n    # Convert to string for pattern matching\n    invoice_str = json.dumps(invoice_data)\n\n    # Find all magic variables\n    pattern = r'\\$\\{([^}]+)\\}'\n    found_variables = set(re.findall(pattern, invoice_str))\n\n    # Check for undefined variables\n    undefined_variables = found_variables - available_variables\n\n    if undefined_variables:\n        raise ValueError(f\"Undefined magic variables: {undefined_variables}\")\n\n    return found_variables\n\n# Define available variables\navailable_vars = {\n    'filename', 'filepath', 'filesize', 'timestamp',\n    'index', 'date', 'time'\n}\n\n# Validate before processing\ntry:\n    used_vars = validate_magic_variables(context.invoice_dst_filepath, available_vars)\n    print(f\"Found valid magic variables: {used_vars}\")\n\n    # Process magic variables\n    applier = VariableApplier()\n    applier.process(context)\n\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#integration-with-processing-pipeline","title":"Integration with Processing Pipeline","text":""},{"location":"rdetoolkit/processing/processors/variables/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code>from rdetoolkit.processing.pipeline import ProcessingPipeline\nfrom rdetoolkit.processing.processors.variables import VariableApplier\n\n# Create processing pipeline\npipeline = ProcessingPipeline()\n\n# Add processors in order\n# pipeline.add_processor(InvoiceInitializer())\n# pipeline.add_processor(FileProcessor())\n\n# Add variable applier after invoice creation\npipeline.add_processor(VariableApplier())\n\n# Add validation processors after variable processing\n# pipeline.add_processor(InvoiceValidator())\n\n# Execute pipeline\npipeline.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#conditional-pipeline-processing","title":"Conditional Pipeline Processing","text":"<pre><code>from rdetoolkit.processing.processors.variables import VariableApplier\n\ndef create_conditional_pipeline(config):\n    \"\"\"Create pipeline with conditional magic variable processing.\"\"\"\n\n    pipeline = ProcessingPipeline()\n\n    # Add standard processors\n    pipeline.add_processor(StandardProcessor())\n\n    # Add variable applier only if enabled\n    if config.system.magic_variable:\n        pipeline.add_processor(VariableApplier())\n\n    # Add validation\n    pipeline.add_processor(ValidationProcessor())\n\n    return pipeline\n\n# Usage\npipeline = create_conditional_pipeline(config)\npipeline.process(context)\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#error-handling","title":"Error Handling","text":""},{"location":"rdetoolkit/processing/processors/variables/#common-exceptions","title":"Common Exceptions","text":""},{"location":"rdetoolkit/processing/processors/variables/#configuration-errors","title":"Configuration Errors","text":"<pre><code>try:\n    applier.process(context)\nexcept AttributeError as e:\n    print(f\"Configuration error: {e}\")\n    # Handle missing configuration attributes\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#file-processing-errors","title":"File Processing Errors","text":"<pre><code>try:\n    applier.process(context)\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\n    # Handle missing invoice or raw files\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#variable-processing-errors","title":"Variable Processing Errors","text":"<pre><code>try:\n    applier.process(context)\nexcept Exception as e:\n    print(f\"Variable processing error: {e}\")\n    # Handle variable substitution failures\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always check configuration before processing:    <pre><code>if context.srcpaths.config.system.magic_variable:\n    applier.process(context)\nelse:\n    logger.debug(\"Magic variables disabled\")\n</code></pre></p> </li> <li> <p>Verify raw files exist:    <pre><code>if context.resource_paths.rawfiles:\n    applier.process(context)\nelse:\n    logger.warning(\"No raw files available for variable extraction\")\n</code></pre></p> </li> <li> <p>Handle processing failures gracefully:    <pre><code>try:\n    applier.process(context)\nexcept Exception as e:\n    logger.error(f\"Magic variable processing failed: {e}\")\n    # Decide whether to continue or abort processing\n</code></pre></p> </li> <li> <p>Log variable processing results:    <pre><code>logger.debug(\"Starting magic variable processing\")\ntry:\n    result = apply_magic_variable(\n        context.invoice_dst_filepath,\n        context.resource_paths.rawfiles[0],\n        save_filepath=context.invoice_dst_filepath\n    )\n    if result:\n        logger.info(\"Magic variables applied successfully\")\n    else:\n        logger.info(\"No magic variables found for replacement\")\nexcept Exception as e:\n    logger.error(f\"Magic variable processing failed: {e}\")\n</code></pre></p> </li> </ol>"},{"location":"rdetoolkit/processing/processors/variables/#configuration-dependencies","title":"Configuration Dependencies","text":""},{"location":"rdetoolkit/processing/processors/variables/#system-configuration","title":"System Configuration","text":"<p>Magic variable processing depends on system configuration:</p> <pre><code>system:\n  magic_variable: true  # Enable magic variable processing\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#required-files","title":"Required Files","text":"<ul> <li>Invoice File: Must exist at <code>context.invoice_dst_filepath</code></li> <li>Raw Files: At least one file in <code>context.resource_paths.rawfiles</code></li> <li>Configuration: Valid system configuration with magic_variable setting</li> </ul>"},{"location":"rdetoolkit/processing/processors/variables/#performance-notes","title":"Performance Notes","text":"<ul> <li>Magic variable processing is performed in-memory for optimal performance</li> <li>File I/O operations are minimized by processing all variables in one pass</li> <li>Variable substitution uses efficient string replacement algorithms</li> <li>Logging is optimized to minimize performance impact on processing</li> <li>The processor gracefully skips processing when disabled, adding minimal overhead</li> </ul>"},{"location":"rdetoolkit/processing/processors/variables/#use-cases","title":"Use Cases","text":""},{"location":"rdetoolkit/processing/processors/variables/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Dynamic File Naming: Include original filenames in processed data</li> <li>Metadata Enrichment: Add file metadata to invoice data</li> <li>Audit Trails: Include processing timestamps and file information</li> <li>Data Lineage: Track source files in processed data</li> <li>Custom Identifiers: Generate unique identifiers based on file properties</li> </ol>"},{"location":"rdetoolkit/processing/processors/variables/#example-use-cases","title":"Example Use Cases","text":"<pre><code># Dynamic naming example\ninvoice_template = {\n    \"basic\": {\n        \"dataName\": \"processed_${filename}\",\n        \"description\": \"Processed data from ${filename} on ${date}\"\n    }\n}\n\n# Metadata enrichment example\ninvoice_template = {\n    \"custom\": {\n        \"sourceFile\": \"${filepath}\",\n        \"originalSize\": \"${filesize}\",\n        \"processedTimestamp\": \"${timestamp}\"\n    }\n}\n</code></pre>"},{"location":"rdetoolkit/processing/processors/variables/#see-also","title":"See Also","text":"<ul> <li>Processing Context - For understanding processing context structure</li> <li>Pipeline Documentation - For processor pipeline integration</li> <li>Invoice File Operations - For invoice file utilities</li> <li>Configuration Guide - For system configuration options</li> </ul>"},{"location":"rdetoolkit/storage/minio/","title":"MinIO Storage","text":"<p>MinIO is a high-performance, S3-compatible object storage service. It is designed for large-scale data storage and retrieval, making it an ideal choice for applications that require fast access to large datasets.</p>"},{"location":"rdetoolkit/storage/minio/#miniostorage","title":"MinIOStorage","text":""},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage","title":"<code>src.rdetoolkit.storage.minio.MinIOStorage(endpoint, access_key=None, secret_key=None, secure=True, region=None, http_client=None)</code>","text":"<p>Handles file operations on MinIO.</p> <p>Attributes:</p> Name Type Description <code>access_key</code> <code>str</code> <p>MinIO access key.</p> <code>secret_key</code> <code>str</code> <p>MinIO secret key.</p> <code>client</code> <code>Minio</code> <p>MinIO client instance.</p> <p>Initializes the MinIOStorage.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>MinIO endpoint.</p> required <code>access_key</code> <code>str | None</code> <p>Access key value. Defaults to environment variable.</p> <code>None</code> <code>secret_key</code> <code>str | None</code> <p>Secret key value. Defaults to environment variable.</p> <code>None</code> <code>secure</code> <code>bool</code> <p>Whether SSL is required.</p> <code>True</code> <code>region</code> <code>str | None</code> <p>Region of the bucket.</p> <code>None</code> <code>http_client</code> <code>PoolManager | None</code> <p>HTTP client for the Minio instance.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If access_key or secret_key is not provided.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.create_default_http_client","title":"<code>create_default_http_client()</code>  <code>staticmethod</code>","text":"<p>Creates a default HTTP client with optional proxy.</p> <p>Returns:</p> Type Description <code>ProxyManager | PoolManager</code> <p>ProxyManager | PoolManager: Configured proxy or pool manager.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.create_proxy_client","title":"<code>create_proxy_client(proxy_url, timeout=urllib3.Timeout.DEFAULT_TIMEOUT, cert_reqs='CERT_REQUIRED', ca_certs=None, retries=None)</code>  <code>staticmethod</code>","text":"<p>Creates a proxy client with specified settings.</p> <p>Parameters:</p> Name Type Description Default <code>proxy_url</code> <code>str</code> <p>The proxy URL.</p> required <code>timeout</code> <code>Any</code> <p>Timeout object or setting.</p> <code>DEFAULT_TIMEOUT</code> <code>cert_reqs</code> <code>str</code> <p>Certificate requirement level.</p> <code>'CERT_REQUIRED'</code> <code>ca_certs</code> <code>str | None</code> <p>Path to CA bundle file.</p> <code>None</code> <code>retries</code> <code>Any</code> <p>Retry settings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ProxyManager</code> <p>A configured proxy manager instance.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.make_bucket","title":"<code>make_bucket(backet_name, location='us-east-1', object_lock=False)</code>","text":"<p>Creates a new bucket in MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>backet_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>location</code> <code>str</code> <p>Region for the bucket.</p> <code>'us-east-1'</code> <code>object_lock</code> <code>bool</code> <p>Whether to enable object lock.</p> <code>False</code>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.list_buckets","title":"<code>list_buckets()</code>","text":"<p>Lists all existing buckets.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of bucket information.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.bucket_exists","title":"<code>bucket_exists(bucket_name)</code>","text":"<p>Checks if a bucket exists.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if bucket exists, else False.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.remove_bucket","title":"<code>remove_bucket(bucket_name)</code>","text":"<p>Removes an existing bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If bucket does not exist.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.put_object","title":"<code>put_object(bucket_name, object_name, data, length, *, content_type='application/octet-stream', metadata=None)</code>","text":"<p>Uploads data as an object to a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>data</code> <code>bytes | str</code> <p>Data to upload.</p> required <code>length</code> <code>int</code> <p>Size of the data.</p> required <code>content_type</code> <code>str</code> <p>MIME type of the object.</p> <code>'application/octet-stream'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Additional metadata.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Information about the upload result.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.fput_object","title":"<code>fput_object(bucket_name, object_name, file_path, content_type='application/octet-stream', metadata=None, sse=None, part_size=0, num_parallel_uploads=3, tags=None, retention=None, legal_hold=False)</code>","text":"<p>Uploads a file from local storage to a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>file_path</code> <code>str | Path</code> <p>Path to source file.</p> required <code>content_type</code> <code>str</code> <p>MIME type of the file.</p> <code>'application/octet-stream'</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Additional metadata.</p> <code>None</code> <code>sse</code> <code>SseCustomerKey | None</code> <p>Server-side encryption.</p> <code>None</code> <code>part_size</code> <code>int</code> <p>Part size for multipart upload.</p> <code>0</code> <code>num_parallel_uploads</code> <code>int</code> <p>Number of parallel uploads.</p> <code>3</code> <code>tags</code> <code>Tags | None</code> <p>Key-value tags for the object.</p> <code>None</code> <code>retention</code> <code>Retention | None</code> <p>Retention configuration.</p> <code>None</code> <code>legal_hold</code> <code>bool</code> <p>Whether to enable legal hold.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Information about the upload result.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.get_object","title":"<code>get_object(bucket_name, object_name, offset=0, length=0, ssec=None, version_id=None, extra_query_params=None)</code>","text":"<p>Retrieves an object from a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>offset</code> <code>int</code> <p>Start byte of the requested range.</p> <code>0</code> <code>length</code> <code>int</code> <p>Number of bytes to read.</p> <code>0</code> <code>ssec</code> <code>SseCustomerKey | None</code> <p>Server-side encryption key.</p> <code>None</code> <code>version_id</code> <code>str | None</code> <p>Specific version of the object.</p> <code>None</code> <code>extra_query_params</code> <code>Any</code> <p>Extra query parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BaseHTTPResponse</code> <code>HTTPResponse</code> <p>The retrieved object data response.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.fget_object","title":"<code>fget_object(bucket_name, object_name, file_path, request_headers=None, ssec=None, version_id=None, extra_query_params=None, tmp_file_path=None)</code>","text":"<p>Downloads an object to a file.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>file_path</code> <code>str</code> <p>Destination path for the downloaded file.</p> required <code>request_headers</code> <code>dict[str, Any] | None</code> <p>Extra request headers.</p> <code>None</code> <code>ssec</code> <code>SseCustomerKey | None</code> <p>Encryption key.</p> <code>None</code> <code>version_id</code> <code>str | None</code> <p>Specific version of the object.</p> <code>None</code> <code>extra_query_params</code> <code>dict[str, Any] | None</code> <p>Extra query parameters.</p> <code>None</code> <code>tmp_file_path</code> <code>str | None</code> <p>Temporary file path.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BaseHTTPResponse</code> <code>HTTPResponse</code> <p>The downloaded file response.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.stat_object","title":"<code>stat_object(bucket_name, object_name, ssec=None, version_id=None, extra_headers=None)</code>","text":"<p>Fetches metadata of an object in a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>ssec</code> <code>SseCustomerKey | None</code> <p>Encryption key.</p> <code>None</code> <code>version_id</code> <code>str | None</code> <p>Specific version of the object.</p> <code>None</code> <code>extra_headers</code> <code>dict[str, Any] | None</code> <p>Additional headers.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Metadata of the requested object.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.remove_object","title":"<code>remove_object(bucket_name, object_name, version_id=None)</code>","text":"<p>Removes an object from a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>version_id</code> <code>str | None</code> <p>Specific version of the object.</p> <code>None</code>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.presigned_get_object","title":"<code>presigned_get_object(bucket_name, object_name, expires=timedelta(days=7), response_headers=None, request_date=None, version_id=None, extra_query_params=None)</code>","text":"<p>Generates a presigned GET URL.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>expires</code> <code>timedelta</code> <p>URL expiration time.</p> <code>timedelta(days=7)</code> <code>response_headers</code> <code>dict[str, Any] | None</code> <p>Custom response headers.</p> <code>None</code> <code>request_date</code> <code>datetime | None</code> <p>A specific request date.</p> <code>None</code> <code>version_id</code> <code>str | None</code> <p>Specific version of the object.</p> <code>None</code> <code>extra_query_params</code> <code>dict[str, Any] | None</code> <p>Extra parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The presigned URL.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.presigned_put_object","title":"<code>presigned_put_object(bucket_name, object_name, expires=timedelta(days=7))</code>","text":"<p>Generates a presigned PUT URL.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Object name in the bucket.</p> required <code>expires</code> <code>timedelta</code> <p>URL expiration time.</p> <code>timedelta(days=7)</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The presigned URL.</p>"},{"location":"rdetoolkit/storage/minio/#src.rdetoolkit.storage.minio.MinIOStorage.secure_get_object","title":"<code>secure_get_object(bucket_name, object_name, *, expires=timedelta(minutes=15), ssec=None, version_id=None, use_ssl=True)</code>","text":"<p>Recommended method for securely retrieving objects.</p> <p>Generates a short-lived presigned URL and accesses it with a dedicated client.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>Name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>Name of the object to retrieve.</p> required <code>expires</code> <code>timedelta</code> <p>Expiration time for the URL (default: 15 minutes).</p> <code>timedelta(minutes=15)</code> <code>ssec</code> <code>SseCustomerKey | None</code> <p>Server-side encryption key.</p> <code>None</code> <code>version_id</code> <code>str | None</code> <p>Specific version of the object.</p> <code>None</code> <code>use_ssl</code> <code>bool</code> <p>Whether to use SSL connection.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>BaseHTTPResponse</code> <code>HTTPResponse</code> <p>HTTP response containing object data.</p> Notes <p>This method is recommended over the traditional get_object method. The expiration time is intentionally short for improved security.</p>"},{"location":"usage/cli/","title":"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u6a5f\u80fd\u306b\u3064\u3044\u3066","text":""},{"location":"usage/cli/#init","title":"init: \u30b9\u30bf\u30fc\u30c8\u30a2\u30c3\u30d7\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u4f5c\u6210","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u3001RDE\u69cb\u9020\u5316\u51e6\u7406\u306e\u30b9\u30bf\u30fc\u30c8\u30a2\u30c3\u30d7\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> Unix/macOSWindows <pre><code>python3 -m rdetoolkit init\n</code></pre> <pre><code>py -m rdetoolkit init\n</code></pre> <p>\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u30d5\u30a1\u30a4\u30eb\u7fa4\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002</p> <pre><code>container\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 invoice\n\u2502   \u2502   \u2514\u2500\u2500 invoice.json\n\u2502   \u2514\u2500\u2500 tasksupport\n\u2502       \u251c\u2500\u2500 invoice.schema.json\n\u2502       \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 modules\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>\u5404\u30d5\u30a1\u30a4\u30eb\u306e\u8aac\u660e\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <ul> <li>requirements.txt<ul> <li>\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u69cb\u7bc9\u3067\u4f7f\u7528\u3057\u305f\u3044Python\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066<code>pip install</code>\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> </li> <li>modules<ul> <li>\u69cb\u9020\u5316\u51e6\u7406\u3067\u4f7f\u7528\u3057\u305f\u3044\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u683c\u7d0d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5225\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u8aac\u660e\u3057\u307e\u3059\u3002</li> </ul> </li> <li>main.py<ul> <li>\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u3092\u5b9a\u7fa9</li> </ul> </li> <li>data/inputdata<ul> <li>\u69cb\u9020\u5316\u51e6\u7406\u5bfe\u8c61\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> </li> <li>data/invoice<ul> <li>\u30ed\u30fc\u30ab\u30eb\u5b9f\u884c\u3055\u305b\u308b\u305f\u3081\u306b\u306f\u7a7a\u30d5\u30a1\u30a4\u30eb\u3067\u3082\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</li> </ul> </li> <li>data/tasksupport<ul> <li>\u69cb\u9020\u5316\u51e6\u7406\u306e\u88dc\u52a9\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> </li> </ul> <p>Tip</p> <p>\u3059\u3067\u306b\u5b58\u5728\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u306f\u4e0a\u66f8\u304d\u3084\u751f\u6210\u304c\u30b9\u30ad\u30c3\u30d7\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"usage/cli/#excelinvoice","title":"ExcelInvoice\u306e\u751f\u6210\u6a5f\u80fd\u306b\u3064\u3044\u3066","text":"<p><code>make_excelinvoice</code>\u3067\u3001<code>invoic.schema.json</code>\u304b\u3089Excelinvoice\u3092\u751f\u6210\u53ef\u80fd\u3067\u3059\u3002\u5229\u7528\u53ef\u80fd\u306a\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> \u30aa\u30d7\u30b7\u30e7\u30f3 \u8aac\u660e \u5fc5\u9808 -o(--output) \u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3002\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u306e\u672b\u5c3e\u306f<code>_excel_invoice.xlsx</code>\u3092\u4ed8\u4e0e\u3059\u308b\u3053\u3068\u3002 o -m \u30e2\u30fc\u30c9\u306e\u9078\u629e\u3002\u767b\u9332\u30e2\u30fc\u30c9\u306e\u9078\u629e\u3002\u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9<code>file</code>\u304b\u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9<code>folder</code>\u3092\u9078\u629e\u53ef\u80fd\u3002 - Unix/macOSWindows <pre><code>python3 -m rdetoolkit make_excelinvoice &lt;invoice.schema.json path&gt; -o &lt;save file path&gt; -m &lt;file or folder&gt;\n</code></pre> <pre><code>py -m rdetoolkit make_excelinvoice &lt;invoice.schema.json path&gt; -o &lt;save file path&gt; -m &lt;file or folder&gt;\n</code></pre> <p>Tip</p> <p><code>-o</code>\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u306f\u3001<code>template_excel_invoice.xlsx</code>\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u540d\u3067\u3001\u5b9f\u884c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u306b\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"usage/cli/#version","title":"version: \u30d0\u30fc\u30b8\u30e7\u30f3\u78ba\u8a8d","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u3001rdetoolkit\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> Unix/macOSWindows <pre><code>python3 -m rdetoolkit version\n</code></pre> <pre><code>py -m rdetoolkit version\n</code></pre>"},{"location":"usage/cli/#artifact-rde","title":"artifact: RDE\u63d0\u51fa\u7528\u30a2\u30fc\u30ab\u30a4\u30d6\u306e\u4f5c\u6210","text":"<p><code>artifact</code>\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001RDE\u306b\u63d0\u51fa\u3059\u308b\u305f\u3081\u306e\u30a2\u30fc\u30ab\u30a4\u30d6\uff08.zip\uff09\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u6307\u5b9a\u3057\u305f\u30bd\u30fc\u30b9\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u5727\u7e2e\u3057\u3001\u9664\u5916\u30d1\u30bf\u30fc\u30f3\u306b\u4e00\u81f4\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9664\u5916\u3057\u307e\u3059\u3002</p> Unix/macOSWindows <pre><code>python3 -m rdetoolkit artifact --source-dir &lt;\u30bd\u30fc\u30b9\u30c7\u30a3\u30ec\u30af\u30c8\u30ea&gt; --output-archive &lt;\u51fa\u529b\u30a2\u30fc\u30ab\u30a4\u30d6\u30d5\u30a1\u30a4\u30eb&gt; --exclude &lt;\u9664\u5916\u30d1\u30bf\u30fc\u30f3&gt;\n</code></pre> <pre><code>py -m rdetoolkit artifact --source-dir &lt;\u30bd\u30fc\u30b9\u30c7\u30a3\u30ec\u30af\u30c8\u30ea&gt; --output-archive &lt;\u51fa\u529b\u30a2\u30fc\u30ab\u30a4\u30d6\u30d5\u30a1\u30a4\u30eb&gt; --exclude &lt;\u9664\u5916\u30d1\u30bf\u30fc\u30f3&gt;\n</code></pre> <p>\u5229\u7528\u53ef\u80fd\u306a\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> \u30aa\u30d7\u30b7\u30e7\u30f3 \u8aac\u660e \u5fc5\u9808 -s(--source-dir) \u5727\u7e2e\u30fb\u30b9\u30ad\u30e3\u30f3\u5bfe\u8c61\u306e\u30bd\u30fc\u30b9\u30c7\u30a3\u30ec\u30af\u30c8\u30ea o -o(--output-archive) \u51fa\u529b\u30a2\u30fc\u30ab\u30a4\u30d6\u30d5\u30a1\u30a4\u30eb\uff08\u4f8b\uff1arde_template.zip\uff09 - -e(--exclude) \u9664\u5916\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f 'venv' \u3068 'site-packages' \u304c\u9664\u5916\u3055\u308c\u307e\u3059 - <p>\u30a2\u30fc\u30ab\u30a4\u30d6\u304c\u4f5c\u6210\u3055\u308c\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9f\u884c\u30ec\u30dd\u30fc\u30c8\u304c\u751f\u6210\u3055\u308c\u307e\u3059\uff1a</p> <ul> <li>Dockerfile\u3084requirements.txt\u306e\u5b58\u5728\u78ba\u8a8d</li> <li>\u542b\u307e\u308c\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u30d5\u30a1\u30a4\u30eb\u306e\u30ea\u30b9\u30c8</li> <li>\u30b3\u30fc\u30c9\u30b9\u30ad\u30e3\u30f3\u7d50\u679c\uff08\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u30ea\u30b9\u30af\u306e\u691c\u51fa\uff09</li> <li>\u5916\u90e8\u901a\u4fe1\u30c1\u30a7\u30c3\u30af\u7d50\u679c</li> </ul> <p>\u4ee5\u4e0b\u306f\u5b9f\u884c\u30ec\u30dd\u30fc\u30c8\u306e\u30b5\u30f3\u30d7\u30eb\u3067\u3059\uff1a</p> <pre><code># Execution Report\n\n**Execution Date:** 2025-04-08 02:58:44\n\n- **Dockerfile:** [Exists]: \ud83d\udc33\u3000container/Dockerfile\n- **Requirements:** [Exists]: \ud83d\udc0d container/requirements.txt\n\n## Included Directories\n\n- container/requirements.txt\n- container/Dockerfile\n- container/vuln.py\n- container/external.py\n\n## Code Scan Results\n\n### container/vuln.py\n\n**Description**: Usage of eval() poses the risk of arbitrary code execution.\n\n```python\ndef insecure():\n\n    value = eval(\"1+2\")\n\n    print(value)\n</code></pre>"},{"location":"usage/cli/#external-communication-check-results","title":"External Communication Check Results","text":""},{"location":"usage/cli/#containerexternalpy","title":"container/external.py","text":"<p><pre><code>1:\n2: import requests\n3: def fetch():\n4:     response = requests.get(\"https://example.com\")\n5:     return response.text\n</code></pre> ```</p> <p>Tip</p> <p><code>--output-archive</code>\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u3067\u30a2\u30fc\u30ab\u30a4\u30d6\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002 <code>--exclude</code>\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u8907\u6570\u56de\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff08\u4f8b\uff1a<code>--exclude venv --exclude .git</code>\uff09\u3002</p>"},{"location":"usage/docker/","title":"Using RDEToolKit with Docker","text":"<p>rdetoolkit\u3092\u4f7f\u3063\u305f\u69cb\u9020\u5316\u51e6\u7406\u3092Docker\u4e0a\u3067\u52d5\u4f5c\u3055\u305b\u308b\u624b\u9806\u3092\u307e\u3068\u3081\u307e\u3059\u3002</p>"},{"location":"usage/docker/#_1","title":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020","text":"<pre><code>(\u69cb\u9020\u5316\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea)\n\u251c\u2500\u2500 container\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 input1\n\u2502   \u2514\u2500\u2500 input2\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 template\n    \u251c\u2500\u2500 batch.yaml\n    \u251c\u2500\u2500 catalog.schema.json\n    \u251c\u2500\u2500 invoice.schema.json\n    \u251c\u2500\u2500 jobs.template.yaml\n    \u251c\u2500\u2500 metadata-def.json\n    \u2514\u2500\u2500 tasksupport\n</code></pre>"},{"location":"usage/docker/#dockerfile","title":"Dockerfile","text":"<p><code>container/Dockerfile</code>\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u4e0b\u8a18\u306fDockerfile\u306e\u4f5c\u6210\u4f8b\u3067\u3059\u3002</p> <p>\u4f7f\u7528\u3059\u308bdocker\u30a4\u30e1\u30fc\u30b8\u3084\u5404\u7a2e\u5b9f\u884c\u6587\u306f\u3001\u5404\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u81ea\u7531\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>FROM python:3.11.9\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY main.py /app\nCOPY modules/ /app/modules/\n</code></pre> <p>Reference</p> <p>Docker Hub Container Image Library | App Containerization</p>"},{"location":"usage/docker/#_2","title":"\u30a4\u30e1\u30fc\u30b8\u306e\u4f5c\u6210","text":"<p><code>Dockerfile</code>\u304c\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304f\u3060\u3055\u3044\u3002docker build\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u30d3\u30eb\u30c9\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\u30b3\u30de\u30f3\u30c9\u306e\u5f62\u5f0f\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059</p> <pre><code># \u30b3\u30de\u30f3\u30c9\n$ docker build -t \u30a4\u30e1\u30fc\u30b8\u540d:\u30bf\u30b0 \u30d1\u30b9\n# \u5b9f\u884c\u4f8b\n$ docker build -t sample_tif:v1 .\n</code></pre> <ul> <li><code>-t</code>\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u30a4\u30e1\u30fc\u30b8\u540d\u3068\u30bf\u30b0\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u30a4\u30e1\u30fc\u30b8\u540d\u306f\u4efb\u610f\u306e\u540d\u524d\u3067\u69cb\u3044\u307e\u305b\u3093\u304c\u3001\u4e00\u610f\u3067\u3042\u308b\u3053\u3068\u304c\u671b\u307e\u3057\u3044\u3067\u3059\u3002</li> <li>\u30d1\u30b9\u306b\u306f<code>Dockerfile</code>\u304c\u5b58\u5728\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d1\u30b9\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306bDockerfile\u304c\u3042\u308b\u5834\u5408\u306f<code>.</code>\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002</li> <li>\u30d7\u30ed\u30ad\u30b7\u74b0\u5883\u5316\u306e\u5834\u5408\u3001<code>--build-arg http_proxy=</code>, <code>--build-arg https_proxy=</code>\u3092\u8a2d\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul>"},{"location":"usage/docker/#pip","title":"\u3082\u3057pip\u30b3\u30de\u30f3\u30c9\u3067\u5931\u6557\u3059\u308b\u5834\u5408","text":"<p>Dockefile\u3068\u540c\u3058\u968e\u5c64\u306b\u3001<code>pip.conf</code>\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4ee5\u4e0b\u306e\u5185\u5bb9\u3067\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u540c\u6642\u306b<code>Dockerfile</code>\u3082\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>[install]\ntrusted-host =\n    pypi.python.org\n    files.pythonhosted.org\n    pypi.org\n</code></pre> <p>\u4fee\u6b63\u5f8c\u306eDockerfile</p> <pre><code>FROM python:3.11\n\nWORKDIR /app\n\nCOPY requirements.txt .\nCOPY pip.conf /etc/pip.conf\n\nRUN pip install -r requirements.txt\n\nCOPY main.py /app\nCOPY modules/ /app/modules/\n</code></pre>"},{"location":"usage/docker/#docker","title":"docker\u30a4\u30e1\u30fc\u30b8\u3092\u8d77\u52d5","text":"<p>\u30d3\u30eb\u30c9\u3057\u305f\u30a4\u30e1\u30fc\u30b8\u3092\u5b9f\u884c\u3059\u308b\u306b\u306f\u3001<code>docker run</code>\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 Docker\u4e0a\u3067\u69cb\u9020\u5316\u51e6\u7406\u3092\u30c6\u30b9\u30c8\u3059\u308b\u305f\u3081\u3001\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(data\u306a\u3069)\u3092\u30de\u30a6\u30f3\u30c8\u3057\u307e\u3059\u3002</p> <pre><code>$ docker run [\u30aa\u30d7\u30b7\u30e7\u30f3] \u30a4\u30e1\u30fc\u30b8\u540d [\u30b3\u30de\u30f3\u30c9]\n# \u5b9f\u884c\u4f8b\n$ docker run -it -v ${HOME}/sample_tif/container/data:/app2/data --name \"sample_tifv1\" sample_tif:v1 \"/bin/bash\"\n</code></pre> <ul> <li><code>-it</code>: \u3053\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u3001\u5bfe\u8a71\u7684\u306a\u30e2\u30fc\u30c9\u3067\u30b3\u30f3\u30c6\u30ca\u3092\u5b9f\u884c\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\u30b3\u30f3\u30c6\u30ca\u3068\u306e\u5bfe\u8a71\u304c\u53ef\u80fd\u306b\u306a\u308a\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u3084\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u3092\u5229\u7528\u3067\u304d\u307e\u3059\u3002</li> <li><code>-v ${HOME}/sample_tif/container/data:/app2/data</code>\uff1a\u3053\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u3001\u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u9593\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002<code>${HOME}/sample_tif/container/data</code>\u306f\u30db\u30b9\u30c8\u5074\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u3057\u3001<code>/app2/data</code>\u306f\u30b3\u30f3\u30c6\u30ca\u5185\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u3057\u307e\u3059\u3002</li> <li><code>--name \"sample_tifv1\"</code>\uff1a\u3053\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u306b\u540d\u524d\u3092\u4ed8\u3051\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u540d\u524d\u3092sample_tifv1\u3068\u3057\u3066\u6307\u5b9a\u3057\u307e\u3059\u3002</li> <li><code>sample_tif:v1</code>\uff1a\u3053\u306e\u90e8\u5206\u306f\u3001\u5b9f\u884c\u3059\u308bDocker\u30a4\u30e1\u30fc\u30b8\u306e\u540d\u524d\u3068\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002</li> <li><code>\"/bin/bash\"</code>\uff1a\u6700\u5f8c\u306e\u90e8\u5206\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u5185\u3067\u5b9f\u884c\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001Bash\u30b7\u30a7\u30eb(/bin/bash)\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002</li> </ul> <p>\u5b9f\u884c\u3059\u308b\u3068\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u304croot@(\u30b3\u30f3\u30c6\u30caID):\u306e\u3088\u3046\u306b\u5909\u5316\u3059\u308b\u3068\u601d\u3044\u307e\u3059\u3002</p>"},{"location":"usage/docker/#_3","title":"\u30b3\u30f3\u30c6\u30ca\u4e0a\u3067\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u52d5\u4f5c\u3055\u305b\u308b","text":"<p>\u958b\u767a\u3057\u305f\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8d77\u52d5\u3055\u305b\u307e\u3059\u3002</p> <pre><code>$ cd /app2\n$ python3 /app/main.py\n</code></pre>"},{"location":"usage/docker/#_4","title":"\u30b3\u30f3\u30c6\u30ca\u3092\u51fa\u308b","text":"<p>\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30b3\u30f3\u30c6\u30ca\u3092\u7d42\u4e86\u3055\u305b\u307e\u3059\u3002</p> <pre><code>exit\n</code></pre>"},{"location":"usage/metadata_definition_file/","title":"\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb","text":""},{"location":"usage/metadata_definition_file/#_2","title":"\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066","text":"<p>RDE\u3067\u306f\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u6271\u3044\u307e\u3059\u3002RDE\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u3068\u304d\u3001\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u5fc5\u8981\u306b\u306a\u308b\u6642\u304c\u3042\u308a\u307e\u3059\u3002</p> <ul> <li>invoice.schema.json</li> <li>invoice.json</li> <li>metadata-def.json</li> <li>metadata.json</li> </ul>"},{"location":"usage/metadata_definition_file/#invoiceschemajson","title":"invoice.schema.json\u306b\u3064\u3044\u3066","text":"<p>\u9001\u308a\u72b6\u306e\u30b9\u30ad\u30fc\u30de\u3092\u5b9a\u7fa9\u3059\u308b\u3002\u30b9\u30ad\u30fc\u30de\u306e\u5f62\u5f0f\u306fJSON Schema\u306e\u6a19\u6e96\u4ed5\u69d8\u306b\u6e96\u62e0\u3057\u307e\u3059\u3002\u3053\u306e JSON\u30b9\u30ad\u30fc\u30de\u306f\u9001\u308a\u72b6\u306e\u753b\u9762\u3092\u751f\u6210\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u304c\u3001RDEToolKit\u3092\u4f7f\u3046\u3053\u3068\u3067\u9001\u308a\u72b6\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3068\u3057\u3066\u4f7f\u308f\u308c\u307e\u3059\u3002</p> <p>Tip</p> <p>Creating your first schema - json-schema.org</p>"},{"location":"usage/metadata_definition_file/#invoiceschemajson_1","title":"invoice.schema.json\u306e\u69cb\u7bc9\u4f8b","text":"invoice.shcema.json\u306e\u69cb\u7bc9\u4f8b <pre><code>{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"$id\": \"https://rde.nims.go.jp/rde/dataset-templates/dataset_template_custom_sample/invoice.schema.json\",\n  \"description\": \"RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30b5\u30f3\u30d7\u30eb\u56fa\u6709\u60c5\u5831invoice\",\n  \"type\": \"object\",\n  \"required\": [\n    \"custom\",\n    \"sample\"\n  ],\n  \"properties\": {\n    \"custom\": {\n      \"type\": \"object\",\n      \"label\": {\n        \"ja\": \"\u56fa\u6709\u60c5\u5831\",\n        \"en\": \"Custom Information\"\n      },\n      \"required\": [\n        \"sample1\",\n        \"sample2\"\n      ],\n      \"properties\": {\n        \"sample1\": {\n          \"label\": {\n            \"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff11\",\n            \"en\": \"sample1\"\n          },\n          \"type\": \"string\",\n          \"format\": \"date\",\n          \"options\": {\n            \"unit\": \"A\"\n          }\n        },\n        \"sample2\": {\n          \"label\": {\n            \"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff12\",\n            \"en\": \"sample2\"\n          },\n          \"type\": \"number\",\n          \"options\": {\n            \"unit\": \"b\"\n          }\n        },\n        \"sample3\": {\n          \"label\": {\n            \"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff13\",\n            \"en\": \"sample3\"\n          },\n          \"type\": \"integer\",\n          \"options\": {\n            \"unit\": \"c\",\n            \"placeholder\": {\n              \"ja\": \"Please Enter text\",\n              \"en\": \"Please Enter text\"\n            }\n          }\n        },\n      }\n    },\n    \"sample\": {\n      \"type\": \"object\",\n      \"label\": {\n        \"ja\": \"\u8a66\u6599\u60c5\u5831\",\n        \"en\": \"Sample Information\"\n      },\n      \"properties\": {\n        \"generalAttributes\": {\n          \"type\": \"array\",\n          \"items\": [\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"e2d20d02-2e38-2cd3-b1b3-66fdb8a11057\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"efcf34e7-4308-c195-6691-6f4d28ffc9bb\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"1e70d11d-cbdd-bfd1-9301-9612c29b4060\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"5e166ac4-bfcd-457a-84bc-8626abe9188f\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"0d0417a3-3c3b-496a-b0fb-5a26f8a74166\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"termId\"\n              ],\n              \"properties\": {\n                \"termId\": {\n                  \"const\": \"efc6a0d5-313e-1871-190c-baaff7d1bf6c\"\n                }\n              }\n            }\n          ]\n        },\n        \"specificAttributes\": {\n          \"type\": \"array\",\n          \"items\": [\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"classId\",\n                \"termId\"\n              ],\n              \"properties\": {\n                \"classId\": {\n                  \"const\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\"\n                },\n                \"termId\": {\n                  \"const\": \"3250c45d-0ed6-1438-43b5-eb679918604a\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"classId\",\n                \"termId\"\n              ],\n              \"properties\": {\n                \"classId\": {\n                  \"const\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\"\n                },\n                \"termId\": {\n                  \"const\": \"70c2c751-5404-19b7-4a5e-981e6cebbb15\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"classId\",\n                \"termId\"\n              ],\n              \"properties\": {\n                \"classId\": {\n                  \"const\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\"\n                },\n                \"termId\": {\n                  \"const\": \"e2d20d02-2e38-2cd3-b1b3-66fdb8a11057\"\n                }\n              }\n            },\n            {\n              \"type\": \"object\",\n              \"required\": [\n                \"classId\",\n                \"termId\"\n              ],\n              \"properties\": {\n                \"classId\": {\n                  \"const\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\"\n                },\n                \"termId\": {\n                  \"const\": \"518e26a0-4262-86f5-3598-80e18e6ff2af\"\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"usage/metadata_definition_file/#invoiceschemajson_2","title":"invoice.schema.json\u306e\u5b9a\u7fa9","text":"\u9805\u76ee\u540d   (JSON\u30dd\u30a4\u30f3\u30bf) \u578b \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 \u5fc5\u9808 \u56fa\u5b9a\u5024 \u8aac\u660e (\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u30eb\u30fc\u30c8) object - \u25cb - JSON\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u30eb\u30fc\u30c8\u3002 /$schema string uri \u25cb <code>https://json-schema.org/draft/2020-12/schema</code> \u30e1\u30bf\u30b9\u30ad\u30fc\u30de(\u30b9\u30ad\u30fc\u30de\u306e\u30b9\u30ad\u30fc\u30de)\u306eID\u3002 /$id string uri \u25cb - \u3053\u306e\u30b9\u30ad\u30fc\u30de\u306eID\u3002\u30e6\u30cb\u30fc\u30af\u3067\u3042\u308b\u3053\u3068 /description string - - - \u30b9\u30ad\u30fc\u30de\u306e\u8aac\u660e /type string - \u25cb \"object\" \u5024\u306f\u56fa\u5b9a\u3002 /required array - \u25cb - \u56fa\u6709\u60c5\u5831\u3092\u5165\u529b\u3055\u305b\u308b\u5834\u5408\u306f\"custom\"\u3092\u542b\u3081\u308b\u3002\u8a66\u6599\u60c5\u5831\u3092\u5165\u529b\u3055\u305b\u308b\u5834\u5408\u306f\"sample\"\u3092\u542b\u3081\u308b\u3002 /properties object - \u25cb - \u2002\u2002/custom object - - - \u9001\u308a\u72b6\u306e\u56fa\u6709\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\u56fa\u6709\u60c5\u5831\u3092\u5165\u529b\u3055\u305b\u306a\u3044\u5834\u5408\u306f\u7701\u304f\u3002 \u2002\u2002\u2002\u2002/type string - \u25cb \"object\" \u5024\u306f\u56fa\u5b9a\u3002 \u2002\u2002\u2002\u2002/label object - \u25cb - \u56fa\u6709\u60c5\u5831\u306e\u898b\u51fa\u3057\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u6587\u5b57\u5217\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/ja string - \u25cb - \u898b\u51fa\u3057\u306e\u65e5\u672c\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/en string - \u25cb - \u898b\u51fa\u3057\u306e\u82f1\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002/required object - \u25cb - \u5fc5\u9808\u306e\u30ad\u30fc\u540d\u3092\u6307\u5b9a\u3059\u308b\u3002\u8907\u6570\u6307\u5b9a\u53ef\u3002 \u2002\u2002\u2002\u2002/properties object - \u25cb - \u56fa\u6709\u60c5\u5831\u9805\u76ee\u306e\u30de\u30c3\u30d7\u3002\u8868\u793a\u3084\u5165\u529b\u3059\u308b\u969b\u306e\u9805\u76ee\u306e\u9806\u5e8f\u306f\u3001\u3053\u306e\u30b9\u30ad\u30fc\u30de\u3067\u306e\u8a18\u8ff0\u9806\u306b\u5f93\u3046\u3002 \u2002\u2002\u2002\u2002\u2002/{\u6700\u521d\u306e\u30ad\u30fc\u306e\u540d\u524d} object - - - \u6700\u521d\u306e\u9805\u76ee\u306e\u30ad\u30fc\u540d\u3002\u30ad\u30fc\u306e\u540d\u524d\u306f\u30d5\u30a1\u30a4\u30eb\u5168\u4f53\u3067\u30e6\u30cb\u30fc\u30af\u3067\u3042\u308b\u3053\u3068\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/type string - \u25cb - \u9805\u76ee\u306e\u5024\u306e\u30c7\u30fc\u30bf\u578b\u3002\"boolean\", \"integer\", \"number\", \"string\"\u306e\u3044\u305a\u308c\u304b1\u3064\u3092\u6307\u5b9a\u3059\u308b\u3002\"boolean\",\"integer\", \"number\", \"string\"\u306e\u3044\u305a\u308c\u306e\u5834\u5408\u3082null\u3092\u8a31\u5bb9\u3057\u306a\u3044\u3002\u203b\u6ce81 \u2002\u2002\u2002\u2002\u2002\u2002/description string - - - \u9805\u76ee\u306e\u8aac\u660e\u3002\u753b\u9762\u306b\u306f\u8868\u793a\u3057\u306a\u3044\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/examples array - - - \u5024\u306e\u4f8b\u3002\u753b\u9762\u306b\u306f\u8868\u793a\u3057\u306a\u3044\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/default \u4efb\u610f - - - \u521d\u671f\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/{\u6700\u521d\u306e\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9} \u30ad\u30fc\u30ef\u30fc\u30c9\u306b\u4f9d\u5b58 - - - \u9805\u76ee\u306e\u5024\u306b\u95a2\u3059\u308b\u5236\u7d04\u3092\u6307\u5b9a\u3059\u308b\u30ad\u30fc\u30ef\u30fc\u30c9\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/{2\u756a\u76ee\u306e\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9} \u30ad\u30fc\u30ef\u30fc\u30c9\u306b\u4f9d\u5b58 - - - \u540c\u4e0a \u2002\u2002\u2002\u2002\u2002\u2002/... - - - - \u2002\u2002\u2002\u2002\u2002\u2002/label object - \u25cb - \u753b\u9762\u306b\u8868\u793a\u3059\u308b\u9805\u76ee\u306e\u30e9\u30d9\u30eb\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/ja string - \u25cb - \u65e5\u672c\u8a9e\u8868\u793a\u6642\u306e\u30e9\u30d9\u30eb\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/en string - \u25cb - \u82f1\u8a9e\u8868\u793a\u6642\u306e\u30e9\u30d9\u30eb\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/options object - - - \u9805\u76ee\u306b\u95a2\u3059\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u6307\u5b9a\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/widget string - - - \u753b\u9762\u90e8\u54c1\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\u3059\u308b\u5834\u5408\u306b\u4f7f\u3046\u3002\"textarea\"\u306e\u307f\u6307\u5b9a\u53ef\u3002\u901a\u5e38\u306ftype\u306e\u5024\u306b\u5fdc\u3058\u305f\u753b\u9762\u90e8\u54c1\u304c\u751f\u6210\u3055\u308c\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/rows integer - - - \u753b\u9762\u90e8\u54c1\u304ctextarea\u306e\u5834\u5408\u306e\u884c\u6570\u3092\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/unit string - - - \u753b\u9762\u306b\u8868\u793a\u3059\u308b\u5358\u4f4d\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/placeholder object - - - \u753b\u9762\u90e8\u54c1\u306b\u8a2d\u5b9a\u3059\u308b\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002\u7701\u7565\u53ef\u80fd\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/ja string - - - \u65e5\u672c\u8a9e\u8868\u793a\u6642\u306e\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/en string - - - \u82f1\u8a9e\u8868\u793a\u6642\u306e\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002 \u2002\u2002\u2002\u2002\u2002/{2\u756a\u76ee\u306e\u30ad\u30fc\u306e\u540d\u524d} object - - - 2\u756a\u76ee\u306e\u9805\u76ee\u306e\u30ad\u30fc\u540d\u3002 \u2002\u2002\u2002\u2002\u2002(\u4ee5\u4e0b\u7e70\u308a\u8fd4\u3057) - - - - \u2002\u2002/sample object - - - \u9001\u308a\u72b6\u306e\u8a66\u6599\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\u8a66\u6599\u60c5\u5831\u3092\u5165\u529b\u3055\u305b\u306a\u3044\u5834\u5408\u306f\u7701\u304f\u3002 \u2002\u2002\u2002\u2002/type string - \u25cb \"object\" \u5024\u306f\u56fa\u5b9a\u3002 \u2002\u2002\u2002\u2002/label object - \u25cb - \u8a66\u6599\u60c5\u5831\u306e\u898b\u51fa\u3057\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u6587\u5b57\u5217\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/ja string - \u25cb - \u898b\u51fa\u3057\u306e\u65e5\u672c\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/en string - \u25cb - \u898b\u51fa\u3057\u306e\u82f1\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002/properties object - \u25cb - \u8a66\u6599\u306e\u30d7\u30ed\u30d1\u30c6\u30a3 \u2002\u2002\u2002\u2002\u2002\u2002/generalAttributes object - - - \u4e00\u822c\u9805\u76ee\u3002\u4e00\u822c\u9805\u76ee\u3092\u5165\u529b\u3057\u306a\u3044\u5834\u5408\u306f\u7701\u7565\u53ef\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/type string - \u25cb \"array\" \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/items array - \u25cb - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/0 object - - - \u6700\u521d\u306e\u4e00\u822c\u9805\u76ee\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/type string - \u25cb \"object\" \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/required array - \u25cb [\"termId\"] \u4e00\u822c\u9805\u76ee\u304c\u6301\u3064\u5fc5\u9808\u30d7\u30ed\u30d1\u30c6\u30a3\u3002\u56fa\u5b9a\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/properties object - \u25cb - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/termId object - \u25cb - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/const string - \u25cb - \u3053\u306e\u4e00\u822c\u9805\u76ee\u306e\u7528\u8a9eID\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/1 object - - - 2\u756a\u76ee\u306e\u4e00\u822c\u9805\u76ee\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002(\u4ee5\u4e0b\u7e70\u308a\u8fd4\u3057) - - - - \u2002\u2002\u2002\u2002\u2002\u2002/specificAttributes object - - - \u5206\u985e\u5225\u9805\u76ee\u3002\u5206\u985e\u5225\u9805\u76ee\u3092\u5165\u529b\u3057\u306a\u3044\u5834\u5408\u306f\u7701\u7565\u53ef\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/type string - \u25cb \"array\" \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/items array - \u25cb \"string\" \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/0 object - - - \u6700\u521d\u306e\u5206\u985e\u5225\u9805\u76ee\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/type string - \u25cb \"object\" \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/required array - \u25cb [\"classId\",\"termId\"] \u5206\u985e\u5225\u9805\u76ee\u304c\u6301\u3064\u5fc5\u9808\u30d7\u30ed\u30d1\u30c6\u30a3\u3002\u56fa\u5b9a\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/properties object - \u25cb - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/classId object - \u25cb - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/const string uuid \u25cb - \u3053\u306e\u5206\u985e\u5225\u9805\u76ee\u306e\u8a66\u6599\u5206\u985eID\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/termId object - \u25cb - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/const string - \u25cb - \u3053\u306e\u5206\u985e\u5225\u9805\u76ee\u306e\u7528\u8a9eID\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/1 object - - - 2\u756a\u76ee\u306e\u5206\u985e\u5225\u9805\u76ee\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002(\u4ee5\u4e0b\u7e70\u308a\u8fd4\u3057) - - - -"},{"location":"usage/metadata_definition_file/#invoiceschemajson_3","title":"invoice.schema.json\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9\u4e00\u89a7","text":"<p>\u9805\u76ee\u306e\u5024\u306b\u95a2\u3059\u308b\u5236\u7d04\u3068\u3057\u3066\u6307\u5b9a\u53ef\u80fd\u306a\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u4e0b\u8868\u306b\u793a\u3059\u3002</p> type \u30ad\u30fc\u30ef\u30fc\u30c9 \u5024\u306e\u578b \u8aac\u660e \u5024\u306e\u5236\u7d04 \u3059\u3079\u3066 type string \u5024\u306e\u578b\u3092\u6307\u5b9a\u3059\u308b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\"boolean\", \"integer\", \"number\", \"string\"\u306e\u3044\u305a\u308c\u304b\u3002 \u6307\u5b9a\u3067\u304d\u308b\u578b\u306f1\u3064\u306e\u307f const type\u306b\u4f9d\u5b58 \u5b9a\u6570\u3092\u6307\u5b9a\u3059\u308b\u3002\u3053\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u5165\u529b\u30fb\u7de8\u96c6\u4e0d\u53ef\u3002 enum array \u53d6\u308a\u5f97\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 number\u307e\u305f\u306f integer maximum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 exclusiveMaximum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u672a\u6e80\u3067\u3042\u308b\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 minimum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u4ee5\u4e0a\u3067\u3042\u308b\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 exclusiveMinimum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u3088\u308a\u5927\u304d\u3044\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 string maxLength integer \u6587\u5b57\u5217\u306e\u9577\u3055\u306e\u6700\u5927\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 \u5024\u306f2,147,483,647\u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068\u3002 minLength integer \u6587\u5b57\u5217\u306e\u9577\u3055\u306e\u6700\u5c0f\u5024\u3092\u6307\u5b9a\u3059\u308b\u30020\u4ee5\u4e0a\u3002 pattern string \u6b63\u898f\u8868\u73fe\u3067\u6307\u5b9a\u3057\u305f\u30d1\u30bf\u30fc\u30f3\u3092\u6301\u3064\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 \u958b\u767a\u8a00\u8a9e\u306b\u4f9d\u5b58\u3057\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u306b\u9650\u5b9a format string \u6587\u5b57\u5217\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6307\u5b9a\u3002\u6307\u5b9a\u53ef\u80fd\u306a\u5024\u306f<code>\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u4e00\u89a7</code>\u3092\u53c2\u7167\u306e\u3053\u3068\u3002"},{"location":"usage/metadata_definition_file/#invoiceschemajson_4","title":"invoice.schema.json\u3067\u5229\u7528\u53ef\u80fd\u306a\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u4e00\u89a7","text":"<p>\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9format\u304c\u53d6\u308a\u5f97\u308b\u5024\u3092\u4e0b\u8868\u306b\u793a\u3059\u3002</p> type \u30ad\u30fc\u30ef\u30fc\u30c9 date \u65e5\u4ed8\u3002RFC 3339\u306efull-date\u3002 time \u6642\u523b\u3002RFC 3339\u306efull-time\u3002 uri URI uuid UUID\u3002URN\u5f62\u5f0f\u3067\u306f\u306a\u304f\u7d20\u306eUUID markdown Markdown\u5f62\u5f0f\u306e\u6587\u5b57\u5217\u3002\u3053\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306fJSON\u30b9\u30ad\u30fc\u30de\u306e\u6a19\u6e96\u4ed5\u69d8\u306b\u306f\u5b58\u5728\u3057\u306a\u3044\u3002"},{"location":"usage/metadata_definition_file/#invoiceschemajson_5","title":"invoice.schema.json\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u3064\u3044\u3066","text":"<p>\u9805\u76ee\u306b\u95a2\u3059\u308b\u5404\u7a2e\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306foptions\u30ad\u30fc\u30ef\u30fc\u30c9\u306b\u3088\u3063\u3066\u6307\u5b9a\u3067\u304d\u308b\u3002\u30aa\u30d7\u30b7\u30e7\u30f3\u3068\u3057\u3066\u6307\u5b9a\u53ef\u80fd\u306a\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u4e0b\u8868\u306b\u793a\u3059\u3002</p> \u30ad\u30fc\u30ef\u30fc\u30c9 \u5024\u306e\u578b \u8aac\u660e format string \u751f\u6210\u3059\u308b\u753b\u9762\u90e8\u54c1\u306e\u7a2e\u985e\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\u3059\u308b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\u201dtextarea\u201d\u306e\u307f\u3068\u3059\u308b\u3002 widget string \u751f\u6210\u3059\u308b\u753b\u9762\u90e8\u54c1\u306e\u7a2e\u985e\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\u3059\u308b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\u201dtextarea\u201d\u306e\u307f\u3068\u3059\u308b\u3002 rows integer widget\u306e\u5024\u304c\u201dtextarea\u201d\u306e\u5834\u5408\u306erows\u5c5e\u6027\u306e\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 unit string \u5358\u4f4d\u306e\u8868\u793a\u5185\u5bb9\u3092\u6307\u5b9a\u3059\u308b\u3002 placeholder object \u753b\u9762\u90e8\u54c1\u306b\u8a2d\u5b9a\u3059\u308b\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002\u65e5\u672c\u8a9e\u3068\u82f1\u8a9e\u3092\u6307\u5b9a\u3067\u304d\u308b\u3002"},{"location":"usage/metadata_definition_file/#invoicejson","title":"invoice.json\u306b\u3064\u3044\u3066","text":"<p>invoice.schema.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306f\u3001\u5fc5\u8981\u306a\u30d5\u30a3\u30fc\u30eb\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3059\u3002</p>"},{"location":"usage/metadata_definition_file/#invoicejson_1","title":"invoice.json\u306e\u69cb\u7bc9\u4f8b","text":"invoice.json\u306e\u69cb\u7bc9\u4f8b <pre><code>{\n  \"datasetId\": \"1s1199df4-0d1v-41b0-1dea-23bf4dh09g12\",\n  \"basic\": {\n    \"dateSubmitted\": \"\",\n    \"dataOwnerId\": \"0c233ef274f28e611de4074638b4dc43e737ab993132343532343430\",\n    \"dataName\": \"test-dataset\",\n    \"instrumentId\": null,\n    \"experimentId\": null,\n    \"description\": null\n  },\n  \"custom\": {\n    \"sample1\": \"2023-01-01\",\n    \"sample2\": 1.0,\n    \"sample3\": 1\n  },\n  \"sample\": {\n    \"sampleId\": \"\",\n    \"names\": [\"test\"],\n    \"composition\": null,\n    \"referenceUrl\": null,\n    \"description\": null,\n    \"generalAttributes\": [\n      {\n        \"termId\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\",\n        \"value\": null\n      },\n      {\n        \"termId\": \"e2d20d02-2e38-2cd3-b1b3-66fdb8a11057\",\n        \"value\": null\n      },\n      {\n        \"termId\": \"efcf34e7-4308-c195-6691-6f4d28ffc9bb\",\n        \"value\": null\n      },\n      {\n        \"termId\": \"1e70d11d-cbdd-bfd1-9301-9612c29b4060\",\n        \"value\": null\n      },\n      {\n        \"termId\": \"5e166ac4-bfcd-457a-84bc-8626abe9188f\",\n        \"value\": null\n      },\n      {\n        \"termId\": \"0d0417a3-3c3b-496a-b0fb-5a26f8a74166\",\n        \"value\": null\n      },\n      {\n        \"termId\": \"efc6a0d5-313e-1871-190c-baaff7d1bf6c\",\n        \"value\": null\n      }\n    ],\n    \"specificAttributes\": [\n      {\n        \"classId\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\",\n        \"termId\": \"3250c45d-0ed6-1438-43b5-eb679918604a\",\n        \"value\": null\n      },\n      {\n        \"classId\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\",\n        \"termId\": \"70c2c751-5404-19b7-4a5e-981e6cebbb15\",\n        \"value\": null\n      },\n      {\n        \"classId\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\",\n        \"termId\": \"e2d20d02-2e38-2cd3-b1b3-66fdb8a11057\",\n        \"value\": null\n      },\n      {\n        \"classId\": \"01cb3c01-37a4-5a43-d8ca-f523ca99a75b\",\n        \"termId\": \"518e26a0-4262-86f5-3598-80e18e6ff2af\",\n        \"value\": null\n      }\n    ],\n    \"ownerId\": \"de17c7b3f0ff5126831c2d519f481055ba466ddb6238666132316439\"\n  }\n}\n</code></pre>"},{"location":"usage/metadata_definition_file/#invoicejson_2","title":"invoice.json\u306e\u5b9a\u7fa9","text":"\u9805\u76ee (JSON\u30dd\u30a4\u30f3\u30bf) \u30d0\u30ea\u30e5\u30fc\u578b \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 \u5fc5\u9808 \u8aac\u660e (\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u30eb\u30fc\u30c8) object - \u25cb /datasetId string uuid \u25cb \u30c7\u30fc\u30bf\u306e\u767b\u9332\u5148\u3068\u306a\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306eID\u3002 /basic object - \u25cb \u9001\u308a\u72b6\u306e\u57fa\u672c\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002 \u2002\u2002/dateSubmitted string date \u25cb \u9001\u308a\u72b6\u304c\u63d0\u51fa\u3055\u308c\u305f\u65e5\u3002\u8aad\u307f\u53d6\u308a\u5c02\u7528\u3002 \u2002\u2002/dataOwnerId string - - \u30c7\u30fc\u30bf\u3092\u6240\u6709\u3059\u308b\u30e6\u30fc\u30b6\u306eID\u3002 \u2002\u2002/dataName string - \u25cb \u30c7\u30fc\u30bf\u306e\u540d\u524d\u3002 \u2002\u2002/instrumentId string uuid - \u88c5\u7f6eID\u3002 \u2002\u2002/experimentId string - - \u5b9f\u9a13ID\u3002\u30e6\u30fc\u30b6\u304c\u81ea\u7531\u306b\u63a1\u756a\u3059\u308b\u3002 \u2002\u2002/description string - - \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aac\u660e\u3002 /custom object - - \u9001\u308a\u72b6\u306e\u56fa\u6709\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u542b\u307e\u308c\u308b\u30d7\u30ed\u30d1\u30c6\u30a3\u306f\u9001\u308a\u72b6\u30b9\u30ad\u30fc\u30de\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u3002 \u2002\u2002\u2026 - - - /sample object - - \u9001\u308a\u72b6\u306e\u8a66\u6599\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\u30d7\u30ed\u30d1\u30c6\u30a3\u306fsampleId, ownerId\u3092\u9664\u3044\u3066\u8a66\u6599API\u306e\u8a66\u6599\u5c5e\u6027\u3068\u4e00\u81f4\u3002\u8a66\u6599\u3078\u306e\u95b2\u89a7\u6a29\u9650\u304c\u7121\u3044\u5834\u5408\u306f\u3001\u5b50\u306e\u30d7\u30ed\u30d1\u30c6\u30a3\u3092\u542b\u3081\u3066\u51fa\u529b\u9805\u76ee\u306b\u542b\u3081\u306a\u3044\u3002 \u2002\u2002/sampleId string uuid - \u8a66\u6599\u306eID\u3002\u9001\u308a\u72b6\u306e\u521d\u56de\u63d0\u51fa\u6642\u306b\u6307\u5b9a\u3057\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u30d7\u30ed\u30d1\u30c6\u30a3\u306f\u4e0d\u8981\u3002 \u2002\u2002/names array - \u25cb \u8a66\u6599\u540d\u306e\u30ea\u30b9\u30c8\u3002 \u2002\u2002\u2002\u2002/0 string - \u25cb \u8a66\u6599\u306e\u4e3b\u305f\u308b\u540d\u524d\u3002 \u2002\u2002\u2002\u2002\u2026 - - - 2\u756a\u76ee\u4ee5\u964d\u306e\u540d\u524d\u3002 \u2002\u2002/composition string - - \u8a66\u6599\u306e\u7d44\u6210\u3002 \u2002\u2002/referenceUrl string uri - \u8a66\u6599\u306e\u53c2\u8003URL\u3002 \u2002\u2002/description string - - \u8a66\u6599\u306e\u8aac\u660e\u3002 \u2002\u2002/generalAttributes array - - \u4e00\u822c\u8a66\u6599\u5c5e\u6027\u306e\u30ea\u30b9\u30c8\u3002\u753b\u9762\u306e\u4e00\u822c\u9805\u76ee\u306b\u8a72\u5f53\u3059\u308b\u3002 \u2002\u2002\u2002\u2002/0 object - - \u6700\u521d\u306e\u5c5e\u6027\u3002\u203b\u6ce81 \"boolean\",\"integer\", \"number\", \"string\"\u306f\u3001\u5024\u306e\u8a2d\u5b9a\u304c\u306a\u3044\u5834\u5408\u306f\u51fa\u529b\u3057\u306a\u3044\u3002\u4ee5\u4e0b\u3002\u540c\u69d8\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/termId string uuid \u25cb \u5c5e\u6027\u306e\u540d\u524d\u3068\u3057\u3066\u306e\u7528\u8a9eID\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/value string - - \u5c5e\u6027\u306e\u5024\u3002 \u2002\u2002\u2002\u2002\u2026 - - \u25cb 2\u756a\u76ee\u4ee5\u964d\u306e\u5c5e\u6027\u3002 \u2002\u2002/specificAttributes array - - \u7279\u5b9a\u8a66\u6599\u5c5e\u6027\u306e\u30ea\u30b9\u30c8\u3002\u753b\u9762\u306e\u5206\u985e\u5225\u9805\u76ee\u306b\u8a72\u5f53\u3059\u308b\u3002 \u2002\u2002\u2002\u2002/0 object - - \u6700\u521d\u306e\u5c5e\u6027\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/classId string uuid \u25cb \u8a66\u6599\u5206\u985e\u306eID\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/termId string uuid \u25cb \u5c5e\u6027\u306e\u540d\u524d\u3068\u3057\u3066\u306e\u7528\u8a9eID\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/value string - - \u5c5e\u6027\u306e\u5024\u3002 \u2002\u2002\u2002\u2002\u2026 - - - 2\u756a\u76ee\u4ee5\u964d\u306e\u5c5e\u6027\u3002 \u2002\u2002/ownerId string - - \u8a66\u6599\u7ba1\u7406\u8005\u306eID\u3002"},{"location":"usage/metadata_definition_file/#metadata-defjson","title":"metadata-def.json","text":"<p>\u30c7\u30fc\u30bf\u69cb\u9020\u5316\u304c\u51fa\u529b\u3059\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u540d\u524d\u3084\u30c7\u30fc\u30bf\u578b\u3092\u5ba3\u8a00\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3002\u9001\u308a\u72b6\u7b49\u306b\u5165\u529b\u3055\u308c\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u306f\u3001<code>metadata-def.json</code>\u306b\u5b9a\u7fa9\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"usage/metadata_definition_file/#metadata-defjson_1","title":"metadata-def.json\u306e\u69cb\u7bc9\u4f8b","text":"metadata-def.json\u306e\u69cb\u7bc9\u4f8b <pre><code>{\n    \"operator_identifier\": {\n        \"name\": {\n            \"ja\": \"\u6e2c\u5b9a\u8005\",\n            \"en\": \"Operator identifier\"\n        },\n        \"schema\": {\n            \"type\": \"string\"\n        },\n        \"order\": 1,\n        \"originalName\": \"Operator\"\n    },\n    \"comment\": {\n        \"name\": {\n            \"ja\": \"\u30b3\u30e1\u30f3\u30c8\",\n            \"en\": \"Comment\"\n        },\n        \"schema\": {\n            \"type\": \"string\"\n        },\n        \"order\": 2,\n        \"originalName\": \"Comment\"\n    },\n    \"memo\": {\n        \"name\": {\n            \"ja\": \"\u30e1\u30e2\",\n            \"en\": \"Memo\"\n        },\n        \"schema\": {\n            \"type\": \"string\"\n        },\n        \"order\": 3,\n        \"originalName\": \"Memo\",\n        \"variable\": 1\n    },\n    \"measurement_operator\": {\n        \"name\": {\n            \"ja\": \"\u6e2c\u5b9a\u5b9f\u65bd\u8005\",\n            \"en\": \"Measurement Operator\"\n        },\n        \"schema\": {\n            \"type\": \"string\"\n        },\n        \"order\": 4,\n        \"originalName\": \"Operator\",\n        \"variable\": 1\n    },\n    \"specimen\": {\n        \"name\": {\n            \"ja\": \"\u8a66\u6599\",\n            \"en\": \"Specimen\"\n        },\n        \"schema\": {\n            \"type\": \"string\"\n        },\n        \"order\": 5,\n        \"originalName\": \"SampleName\",\n        \"variable\": 1\n    },\n    \"peak\": {\n        \"name\": {\n            \"ja\": \"\u30d4\u30fc\u30af\u5024\",\n            \"en\": \"peak value\"\n        },\n        \"schema\": {\n            \"type\": \"number\"\n        },\n        \"unit\": \"V\"\n        \"order\": 6,\n        \"variable\": 1\n    }\n}\n</code></pre>"},{"location":"usage/metadata_definition_file/#metadata-defjson_2","title":"metadata-def.json\u306e\u5b9a\u7fa9","text":"\u9805\u76ee (JSON\u30dd\u30a4\u30f3\u30bf) \u30d0\u30ea\u30e5\u30fc\u578b \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 \u5fc5\u9808 \u8aac\u660e (\u30eb\u30fc\u30c8) object - \u25cb JSON\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u30eb\u30fc\u30c8\u3002 /{\u6700\u521d\u306e\u30ad\u30fc\u306e\u540d\u524d} object - \u25cb \u6700\u521d\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u306e\u30ad\u30fc\u540d\u3002\u5168\u3066\u306e\u30ad\u30fc\u540d\u306f\u30d5\u30a1\u30a4\u30eb\u5185\u3067\u30e6\u30cb\u30fc\u30af\u3067\u3042\u308b\u3053\u3068\u3002 \u2002\u2002/name object - \u25cb - \u2002\u2002\u2002\u2002/ja string - \u25cb \u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u540d\u306e\u65e5\u672c\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002/en string - \u25cb \u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u540d\u306e\u82f1\u8a9e\u8868\u8a18\u3002 \u2002\u2002/schema object - \u25cb JSON Schema (2020-12)\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u3067\u3042\u308btype\u3068format\u3092\u4f7f\u7528\u3059\u308b\u3002\u3053\u308c\u3089\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u306e\u5b9a\u7fa9\u306fJSON Schema\u306b\u5f93\u3046\u3002 \u2002\u2002\u2002\u2002/type string - \u25cb \"s \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u5024\u306e\u578b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\"array\", \"boolean\"\u3001\"integer\"\u3001\"number\"\u3001\"string\"\u3002\"array\"\u306e\u5834\u5408\u3001\u8981\u7d20\u306e\u578b\u306f\u898f\u5b9a\u3057\u306a\u3044\u3002 \u2002\u2002\u2002\u2002/format string - - \"d \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u5024\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\"date-time\"\u3001\"duration\"\u3002 \u2002\u2002/unit string - - \u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u306e\u5024\u306b\u4ed8\u52a0\u3059\u308b\u5358\u4f4d\u3002\u5358\u4f4d\u304c\u7121\u3044\u5834\u5408\u306f\u7701\u7565\u3059\u308b\u3002 \u2002\u2002/description string - - \u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u306e\u8aac\u660e\u3002 \u2002\u2002/uri string uri - \u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u306e\u30ad\u30fc\u306b\u7d10\u3065\u304fURI/URL\u3002 \u2002\u2002/mode string - - \"S \u3053\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u304c\u6709\u52b9\u3067\u3042\u308b\u8a08\u6e2c\u30e2\u30fc\u30c9\u3002\u8a08\u6e2c\u30e2\u30fc\u30c9\u306e\u6307\u5b9a\u304c\u306a\u3044\u5834\u5408\u306f\u7701\u7565\u53ef\u3002 \u2002\u2002/order integer - - \u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u306e\u8868\u793a\u9806\u5e8f\u3002\u5024\u306e\u6607\u9806\u306b\u8868\u793a\u3059\u308b\u3002\u540c\u5024\u306e\u5834\u5408\u306e\u8868\u793a\u9806\u306f\u4e0d\u5b9a\u3002 /{2\u756a\u76ee\u306e\u30ad\u30fc\u306e\u540d\u524d} object - - 2\u756a\u76ee\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u9805\u76ee\u306e\u30ad\u30fc\u540d\u3002 \u2002\u2002(\u4ee5\u4e0b\u7e70\u308a\u8fd4\u3057) - - <p>Note</p> <p>\u69cb\u7bc9\u4f8b\u3067\u63d0\u793a\u3057\u305fmetadata-def.json\u306b<code>variable</code>\u3068\u3044\u3046\u5b9a\u7fa9\u306b\u306a\u3044\u5c5e\u6027\u304c\u3042\u308b\u3002\u3053\u306e\u5834\u5408\u3001RDE\u3067\u306f\u3001<code>variable</code>\u3092\u7121\u8996\u3057\u3066\u53d6\u8fbc\u306f\u884c\u308f\u306a\u3044\u3002</p>"},{"location":"usage/metadata_definition_file/#metadatajson","title":"metadata.json","text":"<p>metadata-def.jso\u306f\u3001\u30c7\u30fc\u30bf\u69cb\u9020\u5316\u51e6\u7406\u304c\u62bd\u51fa\u3057\u305f\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u3002</p>"},{"location":"usage/metadata_definition_file/#metadatajson_1","title":"metadata.json\u306e\u69cb\u7bc9\u4f8b","text":"metadata.json\u306e\u69cb\u7bc9\u4f8b <pre><code>{\n  \"constatn\": {\n    \"operator_identifier\": {\n      \"value\": \"Mike\",\n    },\n    \"comment\": {\n      \"value\": \"sample data\",\n    },\n    \"memo\": {\n      \"value\": \"test\",\n    },\n    \"measurement_operator\": {\n      \"value\": \"Alice\",\n    },\n  },\n  \"variable\": [\n    {\n      \"specimen\": {\n        \"value\": \"C\",\n      },\n      \"peak\": {\n        \"value\": 120,\n        \"unit\": \"V\"\n      }\n    },\n    {\n      \"specimen\": {\n        \"value\": \"H\",\n      },\n      \"peak\": {\n        \"value\": 58,\n        \"unit\": \"V\"\n      }\n    },\n    {\n      \"specimen\": {\n        \"value\": \"O\",\n      },\n      \"peak\": {\n        \"value\": 190,\n        \"unit\": \"V\"\n      }\n    },\n  ]\n}\n</code></pre>"},{"location":"usage/metadata_definition_file/#metadatajson_2","title":"metadata.json\u306e\u5b9a\u7fa9","text":"\u9805\u76ee (JSON\u30dd\u30a4\u30f3\u30bf) \u30d0\u30ea\u30e5\u30fc\u578b \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 \u5fc5\u9808 \u8aac\u660e /constant object - \u25cb \u5168\u3066\u306e\u8a08\u6e2c\u306b\u5171\u901a\u306a\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u96c6\u5408\u3002\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u5b9a\u7fa9\u3067\u306e\u300c\u8a08\u6e2c\u300d\u306b\u306f\u8a08\u7b97\u306a\u3069\u3092\u542b\u3080\u3002 \u2002\u2002/{\u30ad\u30fc\u306e\u540d\u524d} object - \u25cb \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ad\u30fc\u306e\u540d\u524d\u3002 \u2002\u2002\u2002\u2002/value \u30ad\u30fc\u306b\u4f9d\u5b58 - \u25cb \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u5024\u3002 \u2002\u2002\u2002\u2002/unit string - - \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u5024\u306e\u5358\u4f4d\u3002\u5358\u4f4d\u304c\u7121\u3044\u5834\u5408\u306f\u7701\u7565\u53ef\u3002 \u2002\u2002/{\u30ad\u30fc\u306e\u540d\u524d} object - \u25cb \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ad\u30fc\u306e\u540d\u524d\u3002 ... - - \u25cb /variable array - \u25cb \u8a08\u6e2c\u3054\u3068\u306b\u7570\u306a\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u914d\u5217\u3002 \u2002\u2002/0 object - \u25cb \u6700\u521d\u306e\u8a08\u6e2c\u306b\u56fa\u6709\u306a\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u96c6\u5408\u3002 \u2002\u2002\u2002\u2002/{\u30ad\u30fc\u306e\u540d\u524d} object - - \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ad\u30fc\u306e\u540d\u524d\u3002\u914d\u5217\u306e\u5404\u8981\u7d20\u3067\u30ad\u30fc\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u7701\u7565\u53ef\u80fd\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/value \u30ad\u30fc\u306b\u4f9d\u5b58 - \u25cb \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u5024\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/unit string - - \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u5024\u306e\u5358\u4f4d\u3002\u5358\u4f4d\u304c\u7121\u3044\u5834\u5408\u306f\u7701\u7565\u53ef\u3002 \u2002\u2002\u2002\u2002/{\u30ad\u30fc\u306e\u540d\u524d} object - - \u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u30ad\u30fc\u306e\u540d\u524d\u3002 \u2002\u2002\u2002\u2002... - - \u25cb \u2002\u2002/1 object - \u25cb 2\u756a\u76ee\u306e\u8a08\u6e2c\u306b\u56fa\u6709\u306a\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u96c6\u5408\u3002 \u2002\u2002(\u4ee5\u4e0b\u7e70\u308a\u8fd4\u3057) - - \u25cb"},{"location":"usage/metadata_definition_file/#_3","title":"\u7e70\u308a\u8fd4\u3057\u30e1\u30bf\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066","text":"<p>RDE\u306b\u306f\u3001\u8a08\u6e2c\u3054\u3068\u306b\u7570\u306a\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u3057\u3066\u7e70\u308a\u8fd4\u3057\u30e1\u30bf\u30c7\u30fc\u30bf\u304c\u5b9a\u7fa9\u53ef\u80fd\u3067\u3059\u3002\u7e70\u308a\u8fd4\u3057\u30e1\u30bf\u30c7\u30fc\u30bf\u3068\u3057\u3066\u767b\u9332\u3059\u308b\u5834\u5408\u3001<code>variable: 1</code>\u3068\u3044\u3046\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002RDE\u306e\u30b7\u30b9\u30c6\u30e0\u306b\u306f\u3001<code>variable</code>\u306f\u53d6\u8fbc\u306f\u884c\u308f\u308c\u307e\u305b\u3093\u3002</p> <p>\u3057\u304b\u3057\u3001<code>metadata-def.json</code>\u3067<code>variable: 1</code>\u306b\u30bb\u30c3\u30c8\u3057\u305f\u30e1\u30bf\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u306f\u3001<code>metadata.json</code>\u3067<code>variable</code>\u306b\u8ffd\u52a0\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"usage/metadata_definition_file/#catalogschemajson","title":"catalog.schema.json","text":"<p>\u30c7\u30fc\u30bf\u30ab\u30bf\u30ed\u30b0\u306e\u30b9\u30ad\u30fc\u30de\u30d5\u30a1\u30a4\u30eb\u3002\u30b9\u30ad\u30fc\u30de\u306e\u5f62\u5f0f\u306fJSON Schema\u306e\u6a19\u6e96\u4ed5\u69d8\u306b\u6e96\u62e0\u3057\u307e\u3059\u3002</p>"},{"location":"usage/metadata_definition_file/#catalogschemajson_1","title":"catalog.schema.json\u69cb\u7bc9\u4f8b","text":"<pre><code>{\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://rde.nims.go.jp/rde/dataset-templates/dataset_template_custom_sample/catalog.schema.json\",\n    \"type\": \"object\",\n    \"required\": [\n        \"catalog\"\n    ],\n    \"description\": \"dataset_template_custom_sample\",\n    \"properties\": {\n        \"catalog\": {\n            \"type\": \"object\",\n            \"label\": {\n                \"ja\": \"RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30b5\u30f3\u30d7\u30eb\u56fa\u6709\u60c5\u5831\",\n                \"en\": \"dataset_template__custom_sample\"\n            },\n            \"required\": [],\n            \"properties\": {\n                \"dataset_title\": {\n                    \"label\": {\n                        \"ja\": \"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u540d\",\n                        \"en\": \"Dataset Title\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"abstract\": {\n                    \"label\": {\n                        \"ja\": \"\u6982\u8981\",\n                        \"en\": \"Abstract\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"data_creator\": {\n                    \"label\": {\n                        \"ja\": \"\u4f5c\u6210\u8005\",\n                        \"en\": \"Data Creator\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"language\": {\n                    \"label\": {\n                        \"ja\": \"\u8a00\u8a9e\",\n                        \"en\": \"Language\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"experimental_apparatus\": {\n                    \"label\": {\n                        \"ja\": \"\u4f7f\u7528\u88c5\u7f6e\",\n                        \"en\": \"Experimental Apparatus\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"data_distribution\": {\n                    \"label\": {\n                        \"ja\": \"\u30c7\u30fc\u30bf\u306e\u518d\u914d\u5e03\",\n                        \"en\": \"Data Distribution\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"raw_data_type\": {\n                    \"label\": {\n                        \"ja\": \"\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\",\n                        \"en\": \"Raw Data Type\"\n                    },\n                    \"type\": \"string\"\n                },\n                \"stored_data\": {\n                    \"label\": {\n                        \"ja\": \"\u683c\u7d0d\u30c7\u30fc\u30bf\",\n                        \"en\": \"Stored Data\"\n                    },\n                    \"type\": \"string\",\n                    \"options\": {\n                        \"widget\": \"textarea\",\n                        \"rows\": 5\n                    }\n                },\n                \"remarks\": {\n                    \"label\": {\n                        \"ja\": \"\u5099\u8003\",\n                        \"en\": \"Remarks\"\n                    },\n                    \"type\": \"string\",\n                    \"options\": {\n                        \"widget\": \"textarea\",\n                        \"rows\": 5\n                    }\n                },\n                \"references\": {\n                    \"label\": {\n                        \"ja\": \"\u53c2\u8003\u8ad6\u6587\",\n                        \"en\": \"References\"\n                    },\n                    \"type\": \"string\"\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"usage/metadata_definition_file/#catalogschemajson_2","title":"catalog.schema.json\u306e\u5b9a\u7fa9","text":"\u9805\u76ee (JSON\u30dd\u30a4\u30f3\u30bf\u8868\u73fe) \u30d0\u30ea\u30e5\u30fc\u578b \u30d5\u30a9\u30fc\u30de\u30c3\u30c8 \u5fc5\u9808 \u8aac\u660e (\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u30eb\u30fc\u30c8) object - \u25cb /$schema string uri \u25cb \u30e1\u30bf\u30b9\u30ad\u30fc\u30de(\u30b9\u30ad\u30fc\u30de\u306e\u30b9\u30ad\u30fc\u30de)\u306eID\u3002\u56fa\u5b9a\u6587\u5b57\u5217<code>https://json-schema.org/draft/2020-12/schema</code>\u3092\u6307\u5b9a\u3002 /$id string uri \u25cb \u3053\u306e\u30b9\u30ad\u30fc\u30de\u306eID\u3002\u30e6\u30cb\u30fc\u30af\u3067\u3042\u308b\u3053\u3068\u3002 /description string - - \u3053\u306e\u30b9\u30ad\u30fc\u30de\u306e\u8aac\u660e\u3002 /type string - \u25cb \u5024\u306f\u56fa\u5b9a\u3002 /required array - - \u5024\u306f\u56fa\u5b9a\u3002 /properties object - \u25cb \u2002\u2002/catalog object - \u25cb \u30c7\u30fc\u30bf\u30ab\u30bf\u30ed\u30b0\u9805\u76ee\u3092\u683c\u7d0d\u3059\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002 \u2002\u2002\u2002\u2002/type string - \u25cb \u5024\u306f\u56fa\u5b9a\u3002 \u2002\u2002\u2002\u2002/label object - \u25cb \u898b\u51fa\u3057\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u6587\u5b57\u5217\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/ja string - \u25cb \u898b\u51fa\u3057\u306e\u65e5\u672c\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/en string - \u25cb \u898b\u51fa\u3057\u306e\u82f1\u8a9e\u8868\u8a18\u3002 \u2002\u2002\u2002\u2002/required object - \u25cb \u5fc5\u9808\u306e\u30ad\u30fc\u540d\u3092\u6307\u5b9a\u3059\u308b\u3002\u8907\u6570\u6307\u5b9a\u53ef\u3002 \u2002\u2002\u2002\u2002/properties object - \u25cb \u30c7\u30fc\u30bf\u30ab\u30bf\u30ed\u30b0\u9805\u76ee\u306e\u30de\u30c3\u30d7\u3002\u8868\u793a\u3084\u5165\u529b\u3059\u308b\u969b\u306e\u9805\u76ee\u306e\u9806\u5e8f\u306f\u3001\u3053\u306e\u30b9\u30ad\u30fc\u30de\u3067\u306e\u8a18\u8ff0\u9806\u306b\u5f93\u3046\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/{\u6700\u521d\u306e\u30ad\u30fc\u306e\u540d\u524d} object - \u25cb \u6700\u521d\u306e\u9805\u76ee\u306e\u30ad\u30fc\u540d\u3002\u30ad\u30fc\u306e\u540d\u524d\u306f\u30d5\u30a1\u30a4\u30eb\u5168\u4f53\u3067\u30e6\u30cb\u30fc\u30af\u3067\u3042\u308b\u3053\u3068\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/type string - \u25cb \u9805\u76ee\u306e\u5024\u306e\u30c7\u30fc\u30bf\u578b\u3002\"boolean\", \"integer\", \"number\", \"string\"\u306e\u3044\u305a\u308c\u304b1\u3064\u3092\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/description string - - \u9805\u76ee\u306e\u8aac\u660e\u3002\u753b\u9762\u306b\u306f\u8868\u793a\u3057\u306a\u3044\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/examples array - - \u5024\u306e\u4f8b\u3002\u753b\u9762\u306b\u306f\u8868\u793a\u3057\u306a\u3044\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/default \u4efb\u610f - - \u521d\u671f\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/{\u6700\u521d\u306e\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9} \u30ad\u30fc\u30ef\u30fc\u30c9\u306b\u4f9d\u5b58 - - \u9805\u76ee\u306e\u5024\u306b\u95a2\u3059\u308b\u5236\u7d04\u3092\u6307\u5b9a\u3059\u308b\u30ad\u30fc\u30ef\u30fc\u30c9\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/{2\u756a\u76ee\u306e\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9} \u30ad\u30fc\u30ef\u30fc\u30c9\u306b\u4f9d\u5b58 - - \u540c\u4e0a \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2026       - - - \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/label object - \u25cb \u753b\u9762\u306b\u8868\u793a\u3059\u308b\u9805\u76ee\u306e\u30e9\u30d9\u30eb\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/ja string - \u25cb \u65e5\u672c\u8a9e\u8868\u793a\u6642\u306e\u30e9\u30d9\u30eb\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/en string - \u25cb \u82f1\u8a9e\u8868\u793a\u6642\u306e\u30e9\u30d9\u30eb\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/options object - - \u9805\u76ee\u306b\u95a2\u3059\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u6307\u5b9a\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/widget string - - \u753b\u9762\u90e8\u54c1\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\u3059\u308b\u5834\u5408\u306b\u4f7f\u3046\u3002\"textarea\"\u306e\u307f\u6307\u5b9a\u53ef\u3002\u901a\u5e38\u306ftype\u306e\u5024\u306b\u5fdc\u3058\u305f\u753b\u9762\u90e8\u54c1\u304c\u751f\u6210\u3055\u308c\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/rows integer - - \u753b\u9762\u90e8\u54c1\u304ctextarea\u306e\u5834\u5408\u306e\u884c\u6570\u3092\u6307\u5b9a\u3059\u308b\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/unit string - - \u753b\u9762\u306b\u8868\u793a\u3059\u308b\u5358\u4f4d\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/placeholder object - - \u753b\u9762\u90e8\u54c1\u306b\u8a2d\u5b9a\u3059\u308b\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002\u8a00\u8a9e\u5225\u306b\u6307\u5b9a\u3059\u308b\u3002\u7701\u7565\u53ef\u80fd\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/ja string - - \u65e5\u672c\u8a9e\u8868\u793a\u6642\u306e\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002/en string - - \u82f1\u8a9e\u8868\u793a\u6642\u306e\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002 \u2002\u2002\u2002\u2002\u2002\u2002/{2\u756a\u76ee\u306e\u30ad\u30fc\u306e\u540d\u524d} object - \u25cb 2\u756a\u76ee\u306e\u9805\u76ee\u306e\u30ad\u30fc\u540d\u3002 \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002(\u4ee5\u4e0b\u7e70\u308a\u8fd4\u3057)- - -"},{"location":"usage/metadata_definition_file/#catalogschemajson_3","title":"catalog.schema.json\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9\u4e00\u89a7","text":"<p>\u9805\u76ee\u306e\u5024\u306b\u95a2\u3059\u308b\u5236\u7d04\u3068\u3057\u3066\u6307\u5b9a\u53ef\u80fd\u306a\u30b9\u30ad\u30fc\u30de\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u4e0b\u8868\u306b\u793a\u3059\u3002</p> type \u30ad\u30fc\u30ef\u30fc\u30c9 \u5024\u306e\u578b \u8aac\u660e \u5024\u306e\u5236\u7d04 \u3059\u3079\u3066 type string \u5024\u306e\u578b\u3092\u6307\u5b9a\u3059\u308b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\"boolean\", \"integer\", \"number\", \"string\"\u306e\u3044\u305a\u308c\u304b\u3002 \u6307\u5b9a\u3067\u304d\u308b\u578b\u306f1\u3064\u306e\u307f const type\u306b\u4f9d\u5b58 \u5b9a\u6570\u3092\u6307\u5b9a\u3059\u308b\u3002\u3053\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u5165\u529b\u30fb\u7de8\u96c6\u4e0d\u53ef\u3002 enum array \u53d6\u308a\u5f97\u308b\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 number\u307e\u305f\u306f integer maximum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 exclusiveMaximum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u672a\u6e80\u3067\u3042\u308b\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 minimum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u4ee5\u4e0a\u3067\u3042\u308b\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 exclusiveMinimum number \u6570\u5024\u304c\u6307\u5b9a\u3055\u308c\u305f\u5024\u3088\u308a\u5927\u304d\u3044\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 string maxLength integer \u6587\u5b57\u5217\u306e\u9577\u3055\u306e\u6700\u5927\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 \u5024\u306f2,147,483,647\u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068\u3002 minLength integer \u6587\u5b57\u5217\u306e\u9577\u3055\u306e\u6700\u5c0f\u5024\u3092\u6307\u5b9a\u3059\u308b\u30020\u4ee5\u4e0a\u3002 pattern string \u6b63\u898f\u8868\u73fe\u3067\u6307\u5b9a\u3057\u305f\u30d1\u30bf\u30fc\u30f3\u3092\u6301\u3064\u3053\u3068\u3092\u5ba3\u8a00\u3059\u308b\u3002 \u958b\u767a\u8a00\u8a9e\u306b\u4f9d\u5b58\u3057\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u306b\u9650\u5b9a format string \u6587\u5b57\u5217\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6307\u5b9a\u3002\u6307\u5b9a\u53ef\u80fd\u306a\u5024\u306f<code>\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u4e00\u89a7</code>\u3092\u53c2\u7167\u306e\u3053\u3068\u3002"},{"location":"usage/metadata_definition_file/#catalogschemajson_4","title":"catalog.schema.json\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u3064\u3044\u3066","text":"<p>\u9805\u76ee\u306b\u95a2\u3059\u308b\u5404\u7a2e\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306foptions\u30ad\u30fc\u30ef\u30fc\u30c9\u306b\u3088\u3063\u3066\u6307\u5b9a\u3067\u304d\u308b\u3002\u30aa\u30d7\u30b7\u30e7\u30f3\u3068\u3057\u3066\u6307\u5b9a\u53ef\u80fd\u306a\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u4e0b\u8868\u306b\u793a\u3059\u3002</p> \u30ad\u30fc\u30ef\u30fc\u30c9 \u5024\u306e\u578b \u8aac\u660e format string \u751f\u6210\u3059\u308b\u753b\u9762\u90e8\u54c1\u306e\u7a2e\u985e\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\u3059\u308b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\u201dtextarea\u201d\u306e\u307f\u3068\u3059\u308b\u3002 widget string \u751f\u6210\u3059\u308b\u753b\u9762\u90e8\u54c1\u306e\u7a2e\u985e\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\u3059\u308b\u3002\u53d6\u308a\u5f97\u308b\u5024\u306f\u201dtextarea\u201d\u306e\u307f\u3068\u3059\u308b\u3002 rows integer widget\u306e\u5024\u304c\u201dtextarea\u201d\u306e\u5834\u5408\u306erows\u5c5e\u6027\u306e\u5024\u3092\u6307\u5b9a\u3059\u308b\u3002 unit string \u5358\u4f4d\u306e\u8868\u793a\u5185\u5bb9\u3092\u6307\u5b9a\u3059\u308b\u3002 placeholder object \u753b\u9762\u90e8\u54c1\u306b\u8a2d\u5b9a\u3059\u308b\u30d7\u30ec\u30a4\u30b9\u30db\u30eb\u30c0\u3002\u65e5\u672c\u8a9e\u3068\u82f1\u8a9e\u3092\u6307\u5b9a\u3067\u304d\u308b\u3002"},{"location":"usage/object_storage/","title":"rdetolkit\u3067\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8(MinIO)\u3092\u5229\u7528\u3059\u308b\u65b9\u6cd5","text":""},{"location":"usage/object_storage/#_1","title":"\u6982\u8981","text":"<p>MinIOStorage\u306f\u3001\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30b9\u30c8\u30ec\u30fc\u30b8\u30b5\u30fc\u30d3\u30b9\u3067\u3042\u308bMinIO\u3068\u306e\u9023\u643a\u3092\u7c21\u5358\u306b\u884c\u3046\u305f\u3081\u306ePython\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067\u3059\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3001\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u53d6\u5f97\u306a\u3069\u3001MinIO\u306e\u4e3b\u8981\u306a\u6a5f\u80fd\u3092\u7c21\u5358\u306b\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"usage/object_storage/#_2","title":"\u524d\u63d0\u6761\u4ef6","text":"<ul> <li>Python 3.9\u4ee5\u4e0a</li> <li>MinIO\u30b5\u30fc\u30d0\u30fc\u3078\u306e\u30a2\u30af\u30bb\u30b9\uff08\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8URL\u3001\u30a2\u30af\u30bb\u30b9\u30ad\u30fc\u3001\u30b7\u30fc\u30af\u30ec\u30c3\u30c8\u30ad\u30fc\uff09</li> </ul>"},{"location":"usage/object_storage/#_3","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5","text":"<p>rdetoolkit\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u4e00\u90e8\u3068\u3057\u3066\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>pip install rdetoolkit[minio]\n</code></pre>"},{"location":"usage/object_storage/#_4","title":"\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9","text":""},{"location":"usage/object_storage/#1-miniostorage","title":"1. MinIOStorage\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316","text":"<pre><code>from rdetoolkit.storage.minio import MinIOStorage\n\n# \u76f4\u63a5\u8a8d\u8a3c\u60c5\u5831\u3092\u6307\u5b9a\u3059\u308b\u65b9\u6cd5\nstorage = MinIOStorage(\n    endpoint=\"minio.example.com:9000\",\n    access_key=\"your-access-key\",\n    secret_key=\"your-secret-key\",\n    secure=True  # HTTPS\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306fTrue\n)\n\n# \u74b0\u5883\u5909\u6570\u304b\u3089\u8a8d\u8a3c\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u65b9\u6cd5\n# \u74b0\u5883\u5909\u6570 MINIO_ACCESS_KEY \u3068 MINIO_SECRET_KEY \u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u5834\u5408\nimport os\nos.environ[\"MINIO_ACCESS_KEY\"] = \"your-access-key\"\nos.environ[\"MINIO_SECRET_KEY\"] = \"your-secret-key\"\n\nstorage = MinIOStorage(\n    endpoint=\"minio.example.com:9000\",\n    # access_key\u3068secret_key\u3092\u7701\u7565\u3059\u308b\u3068\u74b0\u5883\u5909\u6570\u304b\u3089\u8aad\u307f\u8fbc\u3080\n)\n</code></pre>"},{"location":"usage/object_storage/#2","title":"2. \u30d0\u30b1\u30c3\u30c8\u64cd\u4f5c","text":""},{"location":"usage/object_storage/#_5","title":"\u30d0\u30b1\u30c3\u30c8\u306e\u4f5c\u6210","text":"<pre><code>storage.make_bucket(\"my-bucket\", location=\"us-east-1\")\n</code></pre>"},{"location":"usage/object_storage/#_6","title":"\u30d0\u30b1\u30c3\u30c8\u4e00\u89a7\u306e\u53d6\u5f97","text":"<pre><code>buckets = storage.list_buckets()\nfor bucket in buckets:\n    print(f\"\u30d0\u30b1\u30c3\u30c8\u540d: {bucket['name']}, \u4f5c\u6210\u65e5: {bucket['creation_date']}\")\n</code></pre>"},{"location":"usage/object_storage/#_7","title":"\u30d0\u30b1\u30c3\u30c8\u306e\u5b58\u5728\u78ba\u8a8d","text":"<pre><code>if storage.bucket_exists(\"my-bucket\"):\n    print(\"\u30d0\u30b1\u30c3\u30c8\u304c\u5b58\u5728\u3057\u307e\u3059\")\nelse:\n    print(\"\u30d0\u30b1\u30c3\u30c8\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\")\n</code></pre>"},{"location":"usage/object_storage/#_8","title":"\u30d0\u30b1\u30c3\u30c8\u306e\u524a\u9664","text":"<pre><code>storage.remove_bucket(\"my-bucket\")  # \u30d0\u30b1\u30c3\u30c8\u304c\u7a7a\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\n</code></pre>"},{"location":"usage/object_storage/#3","title":"3. \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u64cd\u4f5c","text":""},{"location":"usage/object_storage/#_9","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\uff08\u30e1\u30e2\u30ea\u4e0a\u306e\u30c7\u30fc\u30bf\u304b\u3089\uff09","text":"<pre><code># \u6587\u5b57\u5217\u304b\u3089\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\ndata = \"Hello, MinIO!\"\nstorage.put_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"hello.txt\",\n    data=data,\n    length=len(data),\n    content_type=\"text/plain\"\n)\n\n# \u30d0\u30a4\u30ca\u30ea\u30c7\u30fc\u30bf\u304b\u3089\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\nbinary_data = b\"\\x00\\x01\\x02\\x03\"\nstorage.put_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"binary-file\",\n    data=binary_data,\n    length=len(binary_data),\n    content_type=\"application/octet-stream\"\n)\n</code></pre>"},{"location":"usage/object_storage/#_10","title":"\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9","text":"<pre><code>storage.fput_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"document.pdf\",\n    file_path=\"/path/to/local/document.pdf\",\n    content_type=\"application/pdf\"\n)\n</code></pre>"},{"location":"usage/object_storage/#_11","title":"\u30e1\u30bf\u30c7\u30fc\u30bf\u4ed8\u304d\u3067\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9","text":"<pre><code>metadata = {\n    \"Author\": \"\u5c71\u7530\u592a\u90ce\",\n    \"Version\": \"1.0\",\n    \"Department\": \"\u958b\u767a\u90e8\"\n}\n\nstorage.fput_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"document.pdf\",\n    file_path=\"/path/to/local/document.pdf\",\n    content_type=\"application/pdf\",\n    metadata=metadata\n)\n</code></pre>"},{"location":"usage/object_storage/#_12","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\uff08\u30e1\u30e2\u30ea\u4e0a\u306b\uff09","text":"<pre><code>response = storage.get_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"hello.txt\"\n)\n\n# \u30ec\u30b9\u30dd\u30f3\u30b9\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\ndata = response.read()\nprint(data.decode('utf-8'))  # \"Hello, MinIO!\"\n\n# \u4f7f\u3044\u7d42\u308f\u3063\u305f\u3089\u30ea\u30bd\u30fc\u30b9\u3092\u89e3\u653e\nresponse.close()\n</code></pre>"},{"location":"usage/object_storage/#_13","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9","text":"<pre><code>storage.fget_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"document.pdf\",\n    file_path=\"/path/to/save/document.pdf\"\n)\n</code></pre>"},{"location":"usage/object_storage/#_14","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30e1\u30bf\u30c7\u30fc\u30bf\u53d6\u5f97","text":"<pre><code>object_info = storage.stat_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"document.pdf\"\n)\n\nprint(f\"\u30b5\u30a4\u30ba: {object_info.size} bytes\")\nprint(f\"\u6700\u7d42\u66f4\u65b0\u65e5: {object_info.last_modified}\")\nprint(f\"ETag: {object_info.etag}\")\nprint(f\"\u30b3\u30f3\u30c6\u30f3\u30c4\u30bf\u30a4\u30d7: {object_info.content_type}\")\nprint(f\"\u30e1\u30bf\u30c7\u30fc\u30bf: {object_info.metadata}\")\n</code></pre>"},{"location":"usage/object_storage/#_15","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u524a\u9664","text":"<pre><code>storage.remove_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"document.pdf\"\n)\n</code></pre>"},{"location":"usage/object_storage/#4-urlpresigned-url","title":"4. \u7f72\u540d\u4ed8\u304dURL\uff08presigned URL\uff09\u306e\u751f\u6210","text":""},{"location":"usage/object_storage/#url","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u53d6\u5f97\u7528\u306e\u7f72\u540d\u4ed8\u304dURL","text":"<pre><code>from datetime import timedelta\n\n# 1\u6642\u9593\u6709\u52b9\u306a\u7f72\u540d\u4ed8\u304dURL\u3092\u751f\u6210\nurl = storage.presigned_get_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"private-document.pdf\",\n    expires=timedelta(hours=1)\n)\n\nprint(f\"\u6b21\u306eURL\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u307e\u3059: {url}\")\n# \u3053\u306eURL\u306f\u8a8d\u8a3c\u306a\u3057\u30671\u6642\u9593\u3060\u3051\u30a2\u30af\u30bb\u30b9\u53ef\u80fd\n</code></pre>"},{"location":"usage/object_storage/#url_1","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u7528\u306e\u7f72\u540d\u4ed8\u304dURL","text":"<pre><code># 1\u65e5\u6709\u52b9\u306a\u7f72\u540d\u4ed8\u304dURL\u3092\u751f\u6210\nurl = storage.presigned_put_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"upload-here.zip\",\n    expires=timedelta(days=1)\n)\n\nprint(f\"\u6b21\u306eURL\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u53ef\u80fd\u3067\u3059: {url}\")\n# \u3053\u306eURL\u306b\u5bfe\u3057\u3066PUT\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u9001\u308b\u3053\u3068\u3067\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u53ef\u80fd\n</code></pre>"},{"location":"usage/object_storage/#5","title":"5. \u30bb\u30ad\u30e5\u30a2\u306a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u53d6\u5f97","text":"<p>\u901a\u5e38\u306e<code>get_object</code>\u3088\u308a\u3082\u30bb\u30ad\u30e5\u30a2\u306a\u65b9\u6cd5\u3067\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u53d6\u5f97\u3057\u307e\u3059\uff1a</p> <pre><code>response = storage.secure_get_object(\n    bucket_name=\"my-bucket\",\n    object_name=\"sensitive-document.pdf\",\n    expires=timedelta(minutes=5)  # \u975e\u5e38\u306b\u77ed\u3044\u6709\u52b9\u671f\u9650\u3092\u8a2d\u5b9a\n)\n\n# \u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\ndata = response.read()\n\n# \u4f7f\u3044\u7d42\u308f\u3063\u305f\u3089\u30ea\u30bd\u30fc\u30b9\u3092\u89e3\u653e\nresponse.close()\n</code></pre>"},{"location":"usage/object_storage/#_16","title":"\u30d7\u30ed\u30ad\u30b7\u74b0\u5883\u3067\u306e\u5229\u7528","text":"<p>\u30d7\u30ed\u30ad\u30b7\u74b0\u5883\u4e0b\u3067MinIOStorage\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u304b\u3001\u660e\u793a\u7684\u306bHTTP\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"usage/object_storage/#_17","title":"\u74b0\u5883\u5909\u6570\u3067\u30d7\u30ed\u30ad\u30b7\u3092\u8a2d\u5b9a","text":"<pre><code>import os\n\n# \u74b0\u5883\u5909\u6570\u3067\u30d7\u30ed\u30ad\u30b7\u3092\u8a2d\u5b9a\nos.environ[\"HTTP_PROXY\"] = \"http://proxy.example.com:8080\"\nos.environ[\"HTTPS_PROXY\"] = \"http://proxy.example.com:8080\"\n\n# \u901a\u5e38\u901a\u308a\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nstorage = MinIOStorage(\n    endpoint=\"minio.example.com:9000\",\n    access_key=\"your-access-key\",\n    secret_key=\"your-secret-key\"\n)\n</code></pre>"},{"location":"usage/object_storage/#http","title":"\u30ab\u30b9\u30bf\u30e0HTTP\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u8a2d\u5b9a","text":"<pre><code>from rdetoolkit.storage.minio import MinIOStorage\n\n# \u30ab\u30b9\u30bf\u30e0\u30d7\u30ed\u30ad\u30b7\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\nproxy_client = MinIOStorage.create_proxy_client(\n    proxy_url=\"http://proxy.example.com:8080\"\n)\n\n# \u30d7\u30ed\u30ad\u30b7\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f7f\u7528\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nstorage = MinIOStorage(\n    endpoint=\"minio.example.com:9000\",\n    access_key=\"your-access-key\",\n    secret_key=\"your-secret-key\",\n    http_client=proxy_client\n)\n</code></pre>"},{"location":"usage/object_storage/#_18","title":"\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0","text":""},{"location":"usage/object_storage/#_19","title":"\u4e00\u822c\u7684\u306a\u30a8\u30e9\u30fc","text":"<ol> <li>\u8a8d\u8a3c\u30a8\u30e9\u30fc</li> <li>\u30a2\u30af\u30bb\u30b9\u30ad\u30fc\u3068\u30b7\u30fc\u30af\u30ec\u30c3\u30c8\u30ad\u30fc\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> <li> <p>\u74b0\u5883\u5909\u6570\u304c\u6b63\u3057\u304f\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>\u63a5\u7d9a\u30a8\u30e9\u30fc</p> </li> <li>\u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> <li>MinIO\u30b5\u30fc\u30d0\u30fc\u304c\u7a3c\u50cd\u3057\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> <li>\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u63a5\u7d9a\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> <li> <p>\u30d7\u30ed\u30ad\u30b7\u8a2d\u5b9a\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u6b63\u3057\u304f\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>\u6a29\u9650\u30a8\u30e9\u30fc</p> </li> <li> <p>\u30d0\u30b1\u30c3\u30c8\u3084\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3078\u306e\u64cd\u4f5c\u6a29\u9650\u304c\u3042\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>\u30d0\u30b1\u30c3\u30c8\u304c\u898b\u3064\u304b\u3089\u306a\u3044\u30a8\u30e9\u30fc</p> </li> <li>\u30d0\u30b1\u30c3\u30c8\u540d\u306e\u30b9\u30da\u30eb\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> <li>\u30d0\u30b1\u30c3\u30c8\u304c\u5b58\u5728\u3059\u308b\u304b<code>bucket_exists()</code>\u3067\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> </ol>"},{"location":"usage/object_storage/#_20","title":"\u30ed\u30b0\u306e\u78ba\u8a8d","text":"<p>\u554f\u984c\u89e3\u6c7a\u306e\u305f\u3081\u306b\u3001\u3088\u308a\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u6709\u52b9\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>import logging\n\n# MinIO\u306e\u30ed\u30b0\u3092\u6709\u52b9\u306b\u3059\u308b\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>"},{"location":"usage/object_storage/#_21","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9","text":""},{"location":"usage/object_storage/#_22","title":"\u57fa\u672c\u7684\u306a\u30d5\u30a1\u30a4\u30eb\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0","text":"<pre><code>from rdetoolkit.storage.minio import MinIOStorage\nfrom datetime import timedelta\nimport os\n\n# MinIOStorage\u306e\u521d\u671f\u5316\nstorage = MinIOStorage(\n    endpoint=\"minio.example.com:9000\",\n    access_key=\"your-access-key\",\n    secret_key=\"your-secret-key\"\n)\n\n# \u4f5c\u696d\u7528\u30d0\u30b1\u30c3\u30c8\u306e\u4f5c\u6210\nbucket_name = \"my-documents\"\nif not storage.bucket_exists(bucket_name):\n    storage.make_bucket(bucket_name)\n    print(f\"\u30d0\u30b1\u30c3\u30c8 '{bucket_name}' \u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\")\n\n# \u30d5\u30a1\u30a4\u30eb\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\nlocal_file = \"/path/to/important-doc.pdf\"\nobject_name = os.path.basename(local_file)\n\nstorage.fput_object(\n    bucket_name=bucket_name,\n    object_name=object_name,\n    file_path=local_file,\n    content_type=\"application/pdf\",\n    metadata={\"CreatedBy\": \"User123\"}\n)\nprint(f\"\u30d5\u30a1\u30a4\u30eb '{object_name}' \u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f\")\n\n# \u4e00\u6642\u7684\u306a\u5171\u6709\u30ea\u30f3\u30af\u306e\u4f5c\u6210\nshare_url = storage.presigned_get_object(\n    bucket_name=bucket_name,\n    object_name=object_name,\n    expires=timedelta(hours=24)\n)\nprint(f\"24\u6642\u9593\u6709\u52b9\u306a\u5171\u6709\u30ea\u30f3\u30af: {share_url}\")\n\n# \u30d5\u30a1\u30a4\u30eb\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\ndownload_path = f\"/path/to/downloads/{object_name}\"\nstorage.fget_object(\n    bucket_name=bucket_name,\n    object_name=object_name,\n    file_path=download_path\n)\nprint(f\"\u30d5\u30a1\u30a4\u30eb\u3092 '{download_path}' \u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f\")\n</code></pre>"},{"location":"usage/object_storage/#_23","title":"\u307e\u3068\u3081","text":"<p>MinIOStorage\u30af\u30e9\u30b9\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001MinIO\u30b5\u30fc\u30d0\u30fc\u3068\u306e\u9023\u643a\u304c\u975e\u5e38\u306b\u7c21\u5358\u306b\u306a\u308a\u307e\u3059\u3002\u4e3b\u306a\u6a5f\u80fd\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\uff1a</p> <ul> <li>\u30d0\u30b1\u30c3\u30c8\u306e\u4f5c\u6210\u3001\u4e00\u89a7\u53d6\u5f97\u3001\u524a\u9664</li> <li>\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff08\u30d5\u30a1\u30a4\u30eb\uff09\u306e\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3068\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9</li> <li>\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u7ba1\u7406</li> <li>\u7f72\u540d\u4ed8\u304dURL\uff08\u671f\u9650\u4ed8\u304d\u30a2\u30af\u30bb\u30b9\u30ea\u30f3\u30af\uff09\u306e\u751f\u6210</li> <li>\u30d7\u30ed\u30ad\u30b7\u74b0\u5883\u3078\u306e\u5bfe\u5fdc</li> </ul> <p>\u8a73\u7d30\u306a\u60c5\u5831\u3084\u9ad8\u5ea6\u306a\u4f7f\u3044\u65b9\u306b\u3064\u3044\u3066\u306f\u3001MinIO Python SDK\u306e\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3082\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"usage/quickstart/","title":"\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8","text":"<p>RDE\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u69cb\u7bc9\u306e\u4e00\u4f8b\u3067\u3059\u3002</p>"},{"location":"usage/quickstart/#_2","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b","text":"<p>\u307e\u305a\u3001RDE\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u5fc5\u8981\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u6e96\u5099\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u30bf\u30fc\u30df\u30ca\u30eb\u3084\u30b7\u30a7\u30eb\u4e0a\u3067\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> Unix/macOSWindows <pre><code>python3 -m rdetoolkit init\n</code></pre> <pre><code>py -m rdetoolkit init\n</code></pre> <p>\u30b3\u30de\u30f3\u30c9\u304c\u6b63\u3057\u304f\u52d5\u4f5c\u3059\u308b\u3068\u3001\u4e0b\u8a18\u3067\u793a\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u30fb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002</p> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001<code>container</code>\u3068\u3044\u3046\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u3001\u958b\u767a\u3092\u9032\u3081\u307e\u3059\u3002</p> <ul> <li>requirements.txt<ul> <li>\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u69cb\u7bc9\u3067\u4f7f\u7528\u3057\u305f\u3044Python\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066<code>pip install</code>\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> </li> <li>modules<ul> <li>\u69cb\u9020\u5316\u51e6\u7406\u3067\u4f7f\u7528\u3057\u305f\u3044\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u683c\u7d0d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5225\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u8aac\u660e\u3057\u307e\u3059\u3002</li> </ul> </li> <li>main.py<ul> <li>\u69cb\u9020\u5316\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u8d77\u52d5\u51e6\u7406\u3092\u5b9a\u7fa9</li> </ul> </li> <li>data/inputdata<ul> <li>\u69cb\u9020\u5316\u51e6\u7406\u5bfe\u8c61\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> </li> <li>data/invoice<ul> <li>\u30ed\u30fc\u30ab\u30eb\u5b9f\u884c\u3055\u305b\u308b\u305f\u3081\u306b\u306f\u7a7a\u30d5\u30a1\u30a4\u30eb\u3067\u3082\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</li> </ul> </li> <li>data/tasksupport<ul> <li>\u69cb\u9020\u5316\u51e6\u7406\u306e\u88dc\u52a9\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> </ul> </li> </ul> <pre><code>container\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 invoice\n\u2502   \u2502   \u2514\u2500\u2500 invoice.json\n\u2502   \u2514\u2500\u2500 tasksupport\n\u2502       \u251c\u2500\u2500 invoice.schema.json\n\u2502       \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 modules\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"usage/quickstart/#_3","title":"\u69cb\u9020\u5316\u51e6\u7406\u306e\u5b9f\u88c5","text":"<p>RDE\u69cb\u9020\u5316\u51e6\u7406\u306f\u3001\u5927\u304d\u304f\u5206\u3051\u3066\u3001\u4ee5\u4e0b\u306e3\u3064\u306e\u30d5\u30a7\u30fc\u30ba\u306b\u5206\u3051\u3089\u308c\u307e\u3059\u3002</p> <pre><code>graph LR\n    \u8d77\u52d5\u51e6\u7406 --&gt; \u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\n    \u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406 --&gt; \u7d42\u4e86\u51e6\u7406\n</code></pre> <p>\u8d77\u52d5\u51e6\u7406\u3001\u7d42\u4e86\u51e6\u7406\u306f\u3001rdetoolkit\u3092\u4f7f\u3046\u3053\u3068\u3067\u7c21\u5358\u306b\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u30e6\u30fc\u30b6\u30fc\u81ea\u8eab\u306f\u3001\u3054\u81ea\u8eab\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b \u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406 \u3092\u5b9a\u7fa9\u3059\u308b\u3060\u3051\u3067\u3059\u3002</p> <p>Documents</p> <p>\u30ab\u30b9\u30bf\u30e0\u7528\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\u306e\u4f5c\u6210</p>"},{"location":"usage/quickstart/#_4","title":"\u30ab\u30b9\u30bf\u30e0\u7528\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\u306e\u4f5c\u6210","text":"<p>rdetoolkit\u3067\u306f\u3001\u72ec\u81ea\u306e\u51e6\u7406\u3092RDE\u306e\u69cb\u9020\u5316\u51e6\u7406\u306e\u30d5\u30ed\u30fc\u306b\u7d44\u307f\u8fbc\u307f\u8fbc\u3080\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u306f\u3001\u5165\u529b\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30c7\u30fc\u30bf\u52a0\u5de5\u30fb\u30b0\u30e9\u30d5\u5316\u30fb\u6a5f\u68b0\u5b66\u7fd2\u7528\u306ecsv\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u306a\u3069\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u56fa\u6709\u306e\u51e6\u7406\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u3001RDE\u3078\u67d4\u8edf\u306b\u30c7\u30fc\u30bf\u3092\u767b\u9332\u53ef\u80fd\u3067\u3059\u3002</p> <p>\u4eee\u306b\u3001rdetoolkit\u3078\u6e21\u3059\u72ec\u81ea\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u95a2\u6570\u3092\u3001<code>dataset()</code>\u3068\u3057\u307e\u3059\u3002<code>dataset()</code>\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u5f15\u6570\u3092\u6e21\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>srcpaths (RdeInputDirPaths): \u51e6\u7406\u306e\u305f\u3081\u306e\u5165\u529b\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30d1\u30b9</li> <li>resource_paths (RdeOutputResourcePath): \u51e6\u7406\u7d50\u679c\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u51fa\u529b\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30d1\u30b9</li> </ul> <pre><code>def dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    ...\n</code></pre> <p>Reference</p> <ul> <li>API Documentation: RdeInputDirPaths - rde2types</li> <li>API Documentation: RdeOutputResourcePath - rde2types</li> </ul> <p>\u4eca\u56de\u306e\u4f8b\u3067\u306f\u3001<code>modules</code>\u4ee5\u4e0b\u306b\u3001<code>display_messsage()</code>, <code>custom_graph()</code>, <code>custom_extract_metadata()</code>\u3068\u3044\u3046\u30c0\u30df\u30fc\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u3001\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u95a2\u6570\u306f\u3001<code>modules/modules.py</code>\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306e2\u3064\u306e\u5f15\u6570\u3092\u6e21\u3059\u95a2\u6570\u3067\u306a\u3051\u308c\u3070\u3001rdetoolkit\u306f\u6b63\u3057\u304f\u51e6\u7406\u304c\u5b9f\u884c\u3067\u304d\u307e\u305b\u3093\u3002</p> <pre><code># modules/modules.py\ndef display_messsage(path):\n    print(f\"Test Message!: {path}\")\n\ndef custom_graph():\n    print(\"graph\")\n\ndef custom_extract_metadata():\n    print(\"extract metadata\")\n\ndef dataset(srcpaths, resource_paths):\n    display_messsage(srcpaths)\n    display_messsage(resource_paths)\n    custom_graph()\n    custom_extract_metadata()\n</code></pre> <p>\u4e0a\u8a18\u306e<code>dataset()</code>\u3092\u6b21\u306e\u30bb\u30af\u30b7\u30e7\u30f3\u306e\u8d77\u52d5\u51e6\u7406\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002</p>"},{"location":"usage/quickstart/#_5","title":"\u8d77\u52d5\u51e6\u7406\u306b\u3064\u3044\u3066","text":"<p>\u7d9a\u3044\u3066\u3001<code>rdetoolkit.workflow.run()</code>\u3092\u4f7f\u3063\u3066\u3001\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\u8d77\u52d5\u51e6\u7406\u3067\u4e3b\u306b\u5b9f\u884c\u51e6\u7406\u306f\u3001</p> <ul> <li>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u30c1\u30a7\u30c3\u30af</li> <li>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3068RDE\u69cb\u9020\u5316\u3067\u898f\u5b9a\u3059\u308b\u5404\u7a2e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b</li> <li>\u30e6\u30fc\u30b6\u30fc\u3054\u3068\u3067\u5b9a\u7fa9\u3057\u305f\u5177\u4f53\u7684\u306a\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c(\u4e0a\u8a18\u30bb\u30af\u30b7\u30e7\u30f3\u3067\u5b9a\u7fa9\u3057\u305f<code>dataset()</code>\u306a\u3069)</li> <li>\u5404\u7a2e\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3</li> </ul> <p>Reference</p> <ul> <li>API Documentation: run - workflows</li> </ul> <p>\u4eca\u56de\u306e\u4f8b\u3067\u306f\u3001<code>main.py</code>\u3092\u4f5c\u6210\u3057\u3001<code>modules/modules.py</code>\u3067\u5b9a\u7fa9\u3057\u305f<code>dataset()</code>\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</p> <pre><code>import rdetoolkit\nfrom modules.modules import dataset #\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\n\n#\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\u3092\u5f15\u6570\u3068\u3057\u3066\u6e21\u3059\nrdetoolkit.workflows.run(custom_dataset_function=dataset)\n</code></pre> <p>\u3082\u3057\u3001\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u6e21\u3055\u306a\u3044\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>import rdetoolkit\n\nrdetoolkit.workflows.run()\n</code></pre>"},{"location":"usage/quickstart/#_6","title":"\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u69cb\u9020\u5316\u51e6\u7406\u3092\u52d5\u4f5c\u3055\u305b\u308b","text":"<p>\u4e0a\u8a18\u306e\u624b\u9806\u3067\u5b9a\u7fa9\u3057\u305f<code>main.py</code>\u3092\u3001\u5404\u81ea\u306e\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u3001\u30c7\u30d0\u30c3\u30b0\u3084\u30c6\u30b9\u30c8\u7684\u306bRDE\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u3001<code>data</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3001<code>tasksupport</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5fc5\u8981\u306a\u5165\u529b\u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\u3053\u3068\u3067\u3001\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u5b9f\u884c\u53ef\u80fd\u3067\u3059\u3002\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001main.py\u3068\u540c\u3058\u968e\u5c64\u306bdata\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u914d\u7f6e\u3057\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u52d5\u4f5c\u3057\u307e\u3059\u3002</p> <pre><code>container/\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 modules/\n\u2502   \u2514\u2500\u2500 modules.py\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 inputdata/\n    \u2502   \u2514\u2500\u2500 &lt;\u51e6\u7406\u3057\u305f\u3044\u5b9f\u9a13\u30c7\u30fc\u30bf&gt;\n    \u251c\u2500\u2500 invoice/\n    \u2502   \u2514\u2500\u2500 invoice.json\n    \u2514\u2500\u2500 tasksupport/\n        \u251c\u2500\u2500 metadata-def.json\n        \u2514\u2500\u2500 invoice.schema.json\n</code></pre> <p>\u4e0a\u8a18\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u306f\u3001\u3042\u304f\u307e\u3067\u4e00\u4f8b\u3067\u3059\u3002data/inputdata\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3001tasksupport\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001\u5fc5\u8981\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u9069\u5b9c\u8ffd\u52a0/\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u4e0b\u8a18\u306e\u3088\u3046\u306b\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>cd container\npython3 main.py\n</code></pre>"},{"location":"usage/quickstart/#_7","title":"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7","text":"<ul> <li>\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u88c5\u3059\u308b</li> <li>RDEToolKit\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3068\u5236\u5fa1\u3067\u304d\u308b\u6a5f\u80fd\u3092\u77e5\u308b</li> <li>RDE\u306e\u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9\u3092\u6307\u5b9a\u3059\u308b</li> <li>\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u77e5\u308b</li> </ul>"},{"location":"usage/validation/","title":"\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u6a5f\u80fd","text":"<p>RDEToolKit\u306b\u306f\u3001\u7279\u5b9a\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u6a5f\u80fd\u3092\u5b9f\u88c5\u3057\u3066\u3044\u307e\u3059\u3002\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u958b\u767a\u3059\u308b\u5834\u5408\u3001<code>invoice.json</code>\u3084<code>invocie.schema.json</code>\u306a\u3069\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u3001\u4e8b\u524d\u306b\u4f5c\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u958b\u767a\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u304c\u6b63\u3057\u304fRDE\u306b\u767b\u9332\u3055\u308c\u308b\u305f\u3081\u306b\u3001\u4e8b\u524d\u306b\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"usage/validation/#_2","title":"\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5bfe\u8c61\u30d5\u30a1\u30a4\u30eb","text":"<p>RDEToolKit\u3067\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5bfe\u8c61\u3068\u306a\u308b\u30d5\u30a1\u30a4\u30eb\u306f\u3001\u4ee5\u4e0b\u306e4\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u7406\u7531\u306f\u3001\u69cb\u9020\u5316\u51e6\u7406\u5185\u3067\u4e0b\u8a18\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u3092\u5909\u66f4\u3067\u304d\u3066\u3057\u307e\u3046\u305f\u3081\u3067\u3059\u3002\u69cb\u9020\u5316\u51e6\u7406\u69cb\u7bc9\u306e\u969b\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001\u4e0b\u8a18\u306e\u60c5\u5831\u3092\u53c2\u8003\u306b\u30d5\u30a1\u30a4\u30eb\u3092\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>invoice.schema.json</li> <li>invoice.json</li> <li>metadata-def.json</li> <li>metadata.json</li> </ul> <p>Documents</p> <ul> <li>\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066</li> </ul>"},{"location":"usage/validation/#invoiceschemajson","title":"invoice.schema.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3","text":"<p><code>invoice.schema.json</code>\u3092\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002invoie.schema.json\u306f\u3001RDE\u306e\u753b\u9762\u3092\u69cb\u6210\u3059\u308b\u30b9\u30ad\u30fc\u30de\u30d5\u30a1\u30a4\u30eb\u3067\u3059\u304c\u3001\u69cb\u9020\u5316\u51e6\u7406\u4e2d\u3067\u5909\u66f4\u3001\u30ed\u30fc\u30ab\u30eb\u3067\u5b9a\u7fa9\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b\u70b9\u304b\u3089\u3001\u5fc5\u8981\u306a\u30d5\u30a3\u30fc\u30eb\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306e\u30c1\u30a7\u30c3\u30af\u6a5f\u80fd\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u6a5f\u80fd\u306f\u3001<code>rdetoolkit.workflows.run()</code>\u306b\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p> <p><code>invoice.schema.json</code>\u306e\u5404\u30d5\u30a3\u30fc\u30eb\u30c9\u306e\u30c1\u30a7\u30c3\u30af\u306f\u3001<code>rdetoolkit.validation.InvoiceValidator</code>\u3067\u5b9f\u884c\u3057\u307e\u3059\u3002</p> <pre><code>import json\nfrom pydantic import ValidationError\n\nfrom rdetoolkit.validation import InvoiceValidator\nfrom rdetoolkit.exceptions import InvoiceSchemaValidationError\n\nschema = {\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://rde.nims.go.jp/rde/dataset-templates/dataset_template_custom_sample/invoice.schema.json\",\n    \"description\": \"RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30b5\u30f3\u30d7\u30eb\u56fa\u6709\u60c5\u5831invoice\",\n    \"type\": \"object\",\n    \"required\": [\"custom\", \"sample\"],\n    \"properties\": {\n        \"custom\": {\n            \"type\": \"object\",\n            \"label\": {\"ja\": \"\u56fa\u6709\u60c5\u5831\", \"en\": \"Custom Information\"},\n            \"required\": [\"sample1\"],\n            \"properties\": {\n                \"sample1\": {\"label\": {\"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff11\", \"en\": \"sample1\"}, \"type\": \"string\", \"format\": \"date\", \"options\": {\"unit\": \"A\"}},\n                \"sample2\": {\"label\": {\"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff12\", \"en\": \"sample2\"}, \"type\": \"number\", \"options\": {\"unit\": \"b\"}},\n            },\n        },\n        \"sample\": {\n            \"type\": \"object\",\n            \"label\": {\"ja\": \"\u8a66\u6599\u60c5\u5831\", \"en\": \"Sample Information\"},\n            \"properties\": {\n                \"generalAttributes\": {\n                    \"type\": \"array\",\n                    \"items\": [\n                        {\"type\": \"object\", \"required\": [\"termId\"], \"properties\": {\"termId\": {\"const\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\"}}}\n                    ],\n                },\n                \"specificAttributes\": {\"type\": \"array\", \"items\": []},\n            },\n        },\n    },\n}\n\ndata = {\n    \"datasetId\": \"1s1199df4-0d1v-41b0-1dea-23bf4dh09g12\",\n    \"basic\": {\n        \"dateSubmitted\": \"\",\n        \"dataOwnerId\": \"0c233ef274f28e611de4074638b4dc43e737ab993132343532343430\",\n        \"dataName\": \"test-dataset\",\n        \"instrumentId\": None,\n        \"experimentId\": None,\n        \"description\": None,\n    },\n    \"custom\": {\"sample1\": \"2023-01-01\", \"sample2\": 1.0},\n    \"sample\": {\n        \"sampleId\": \"\",\n        \"names\": [\"test\"],\n        \"composition\": None,\n        \"referenceUrl\": None,\n        \"description\": None,\n        \"generalAttributes\": [{\"termId\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\", \"value\": None}],\n        \"specificAttributes\": [],\n        \"ownerId\": \"de17c7b3f0ff5126831c2d519f481055ba466ddb6238666132316439\",\n    },\n}\n\nwith open(\"temp/invoice.schema.json\", \"w\") as f:\n    json.dump(schema, f, ensure_ascii=False, indent=2)\n\nvalidator = InvoiceValidator(\"temp/invoice.schema.json\")\ntry:\n    validator.validate(obj=data)\nexcept ValidationError as validation_error:\n    raise InvoiceSchemaValidationError from validation_error\n</code></pre>"},{"location":"usage/validation/#invoiceschemajson_1","title":"invoice.schema.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc","text":"<p><code>invoice.schema.json</code>\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001<code>pydantic_core._pydantic_core.ValidationError</code>\u304c\u767a\u751f\u3057\u307e\u3059\u3002</p> <p>Reference</p> <ul> <li>pydantic_core._pydantic_core.ValidationError - Pydantic</li> </ul> <p>invoice.schema.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u306f\u3001<code>invoice.schema.json</code>\u306e\u5c5e\u6027\u304c\u4e0d\u6b63\u3001\u6b20\u640d\u304c\u751f\u3058\u305f\u5834\u5408\u306b\u767a\u751f\u3057\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u518d\u5ea6\u5b9a\u7fa9\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002<code>invoice.schema.json</code>\u306e\u5b9a\u7fa9\u306b\u3064\u3044\u3066\u306f\u3001invoice.schema.json\u306b\u3064\u3044\u3066 - \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p> <p>\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u3001\u4ee5\u4e0b\u306e\u60c5\u5831\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\u30a8\u30e9\u30fc\u539f\u56e0\u3068\u306a\u3063\u305f\u30d5\u30a3\u30fc\u30eb\u30c9</li> <li>\u30a8\u30e9\u30fc\u30bf\u30a4\u30d7</li> <li>\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8</li> </ul> <pre><code>1. Field: required.0\n   Type: literal_error\n   Context: Input should be 'custom' or 'sample'\n</code></pre> <p>\u3053\u306e\u4f8b\u3067\u306f\u3001<code>Input should be 'custom' or 'sample'</code>\u3068\u66f8\u304b\u308c\u3066\u3044\u308b\u901a\u308a\u3001<code>required</code>\u306b\u3001<code>custom</code>\u304b<code>sample</code>\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</p> <pre><code>schema = {\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://rde.nims.go.jp/rde/dataset-templates/dataset_template_custom_sample/invoice.schema.json\",\n    \"description\": \"RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30b5\u30f3\u30d7\u30eb\u56fa\u6709\u60c5\u5831invoice\",\n    \"type\": \"object\",\n    \"required\": [\"custom\"], # sample\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u306b\u3082\u304b\u304b\u308f\u3089\u305a\"required\"\u306b\u542b\u307e\u308c\u3066\u3044\u306a\u3044\n    \"properties\": {\n        \"custom\": {\n            ...\n            },\n        \"sample\": {\n            ...\n        }\n    },\n}\n</code></pre> <p>Tip</p> <p>\u8a73\u3057\u3044\u4fee\u6b63\u65b9\u6cd5\u306f\u3001invoice.schema.json - \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066 \u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"usage/validation/#invoicejson","title":"invoice.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3","text":"<p>invoice.schema.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306f\u3001\u5fc5\u8981\u306a\u30d5\u30a3\u30fc\u30eb\u30c9\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3059\u3002invoice.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306b\u306f\u3001<code>invoice.schema.json</code>\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>import json\nfrom pydantic import ValidationError\n\nfrom rdetoolkit.validation import InvoiceValidator\nfrom rdetoolkit.exceptions import InvoiceSchemaValidationError\n\nschema = {\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"$id\": \"https://rde.nims.go.jp/rde/dataset-templates/dataset_template_custom_sample/invoice.schema.json\",\n    \"description\": \"RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30b5\u30f3\u30d7\u30eb\u56fa\u6709\u60c5\u5831invoice\",\n    \"type\": \"object\",\n    \"required\": [\"custom\", \"sample\"],\n    \"properties\": {\n        \"custom\": {\n            \"type\": \"object\",\n            \"label\": {\"ja\": \"\u56fa\u6709\u60c5\u5831\", \"en\": \"Custom Information\"},\n            \"required\": [\"sample1\"],\n            \"properties\": {\n                \"sample1\": {\"label\": {\"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff11\", \"en\": \"sample1\"}, \"type\": \"string\", \"format\": \"date\", \"options\": {\"unit\": \"A\"}},\n                \"sample2\": {\"label\": {\"ja\": \"\u30b5\u30f3\u30d7\u30eb\uff12\", \"en\": \"sample2\"}, \"type\": \"number\", \"options\": {\"unit\": \"b\"}},\n            },\n        },\n        \"sample\": {\n            \"type\": \"object\",\n            \"label\": {\"ja\": \"\u8a66\u6599\u60c5\u5831\", \"en\": \"Sample Information\"},\n            \"properties\": {\n                \"generalAttributes\": {\n                    \"type\": \"array\",\n                    \"items\": [\n                        {\"type\": \"object\", \"required\": [\"termId\"], \"properties\": {\"termId\": {\"const\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\"}}}\n                    ],\n                },\n                \"specificAttributes\": {\"type\": \"array\", \"items\": []},\n            },\n        },\n    },\n}\n\ndata = {\n    \"datasetId\": \"1s1199df4-0d1v-41b0-1dea-23bf4dh09g12\",\n    \"basic\": {\n        \"dateSubmitted\": \"\",\n        \"dataOwnerId\": \"0c233ef274f28e611de4074638b4dc43e737ab993132343532343430\",\n        \"dataName\": \"test-dataset\",\n        \"instrumentId\": None,\n        \"experimentId\": None,\n        \"description\": None,\n    },\n    \"custom\": {\"sample1\": \"2023-01-01\", \"sample2\": 1.0},\n    \"sample\": {\n        \"sampleId\": \"\",\n        \"names\": [\"test\"],\n        \"composition\": None,\n        \"referenceUrl\": None,\n        \"description\": None,\n        \"generalAttributes\": [{\"termId\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\", \"value\": None}],\n        \"specificAttributes\": [],\n        \"ownerId\": \"de17c7b3f0ff5126831c2d519f481055ba466ddb6238666132316439\",\n    },\n}\n\nwith open(\"temp/invoice.schema.json\", \"w\") as f:\n    json.dump(schema, f, ensure_ascii=False, indent=2)\n\nvalidator = InvoiceValidator(\"temp/invoice.schema.json\")\ntry:\n    validator.validate(obj=data)\nexcept ValidationError as validation_error:\n    print(validation_error)\n</code></pre>"},{"location":"usage/validation/#_3","title":"\u8a66\u6599\u60c5\u5831\u306e\u5b9a\u7fa9\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306b\u3064\u3044\u3066","text":"<p>\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u69cb\u9020\u5316\u51e6\u7406\u3092\u958b\u767a\u3059\u308b\u5834\u5408\u3001invoice.json(\u9001\u308a\u72b6)\u3092\u4e8b\u524d\u306b\u7528\u610f\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002\u9001\u308a\u72b6\u306b\u8a66\u6599\u60c5\u5831\u3092\u5b9a\u7fa9\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u5b9a\u7fa9\u304c\u60f3\u5b9a\u3055\u308c\u307e\u3059\u3002</p> <ol> <li>\u8a66\u6599\u60c5\u5831\u3092\u65b0\u898f\u306b\u8ffd\u52a0\u3059\u308b\u5834\u5408</li> <li>\u65e2\u5b58\u306e\u8a66\u6599\u60c5\u5831\u3092\u53c2\u7167\u3059\u308b\u5834\u5408</li> </ol> <p>\u4e0a\u8a18\u306e2\u3064\u306e\u30b1\u30fc\u30b9\u3067\u306f\u5fc5\u9808\u9805\u76ee\u304c\u7570\u306a\u308b\u305f\u3081\u3001\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u4f5c\u6210\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u3063\u3066\u30c7\u30d0\u30c3\u30b0\u3059\u308b\u969b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059\u3002\u4e0a\u8a18\u306e\u3069\u3061\u3089\u304b\u306e\u5fc5\u9808\u9805\u76ee\u3092\u6e80\u305f\u305b\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</p>"},{"location":"usage/validation/#_4","title":"\u8a66\u6599\u60c5\u5831\u3092\u65b0\u898f\u306b\u8ffd\u52a0\u3059\u308b\u5834\u5408","text":"<p>\u3053\u306e\u5834\u5408\u3001<code>sample</code>\u30d5\u30a3\u30fc\u30eb\u30c9\u306e<code>sampleId</code>\u3001<code>names</code>\u3001<code>ownerId</code>\u304c\u5fc5\u9808\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>\"sample\": {\n        \"sampleId\": \"de1132316439\",\n        \"names\": [\"test\"],\n        \"composition\": null,\n        \"referenceUrl\": null,\n        \"description\": null,\n        \"generalAttributes\": [{\"termId\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\", \"value\": null}],\n        \"specificAttributes\": [],\n        \"ownerId\": \"de17c7b3f0ff5126831c2d519f481055ba466ddb6238666132316439\",\n    },\n</code></pre>"},{"location":"usage/validation/#_5","title":"\u65e2\u5b58\u306e\u8a66\u6599\u60c5\u5831\u3092\u53c2\u7167\u3059\u308b\u5834\u5408","text":"<p>\u3053\u306e\u5834\u5408\u3001<code>sample</code>\u30d5\u30a3\u30fc\u30eb\u30c9\u306e<code>sampleId</code>\u304c\u5fc5\u9808\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>\"sample\": {\n        \"sampleId\": \"de1132316439\",\n        \"names\": [],\n        \"composition\": null,\n        \"referenceUrl\": null,\n        \"description\": null,\n        \"generalAttributes\": [{\"termId\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\", \"value\": null}],\n        \"specificAttributes\": [],\n        \"ownerId\": \"de17c7b3f0ff5126831c2d519f481055ba466ddb6238666132316439\",\n    },\n</code></pre>"},{"location":"usage/validation/#_6","title":"\u8a66\u6599\u60c5\u5831\u306b\u95a2\u3059\u308b\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc","text":"<p>\u4e0a\u8a18\u306e2\u3064\u306e\u30b1\u30fc\u30b9\u3069\u3061\u3089\u304b\u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3051\u308c\u3070\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u3001\u4ee5\u4e0b\u306e\u60c5\u5831\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\u30a8\u30e9\u30fc\u539f\u56e0\u3068\u306a\u3063\u305f\u30d5\u30a3\u30fc\u30eb\u30c9</li> <li>\u30a8\u30e9\u30fc\u30bf\u30a4\u30d7</li> <li>\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8</li> </ul> <pre><code>Error: Error in validating system standard field.\nPlease correct the following fields in invoice.json\nField: sample\nType: anyOf\nContext: {'sampleId': '', 'names': 'test', 'generalAttributes': [{'termId': '3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e'}, {'termId': 'e2d20d02-2e38-2cd3-b1b3-66fdb8a11057'}, {'termId': 'efcf34e7-4308-c195-6691-6f4d28ffc9bb'}, {'termId': '7cc57dfb-8b70-4b3a-5315-fbce4cbf73d0'}, {'termId': '1e70d11d-cbdd-bfd1-9301-9612c29b4060'}, {'termId': '5e166ac4-bfcd-457a-84bc-8626abe9188f'}, {'termId': '0d0417a3-3c3b-496a-b0fb-5a26f8a74166'}, {'termId': '0d0417a3-3c3b-496a-b0fb-5a26f8a74166'}], 'specificAttributes': [], 'ownerId': ''} is not valid under any of the given schemas\n</code></pre>"},{"location":"usage/validation/#invoicejson_1","title":"\u305d\u306e\u4ed6invoice.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc","text":"<p><code>invoice.json</code>\u306e<code>basic</code>\u9805\u76ee\u306b\u904e\u4e0d\u8db3\u3084\u5024\u304c\u4e0d\u6b63\u306a\u5834\u5408\u3001<code>jsonschema</code>\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</p> <pre><code>data = {\n    \"datasetId\": \"1s1199df4-0d1v-41b0-1dea-23bf4dh09g12\",\n    \"basic\": {\n        \"dateSubmitted\": \"\",\n        \"dataOwnerId\": \"0c233ef274f28e611de4074638b4dc43e737ab9931323435323434\",\n        \"dataName\": \"test-dataset\",\n        \"instrumentId\": None,\n        \"experimentId\": None,\n        \"description\": None,\n    },\n    \"custom\": {\"sample1\": \"2023-01-01\", \"sample2\": 1.0},\n}\n</code></pre> <p>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u3001\u4ee5\u4e0b\u306e\u60c5\u5831\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\u8a72\u5f53\u30d5\u30a3\u30fc\u30eb\u30c9</li> <li>\u30a8\u30e9\u30fc\u7a2e\u5225</li> <li>\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8</li> </ul> <p>\u4ee5\u4e0b\u306e\u30b1\u30fc\u30b9\u306e\u5834\u5408\u3001\u57fa\u672c\u60c5\u5831\u3067\u3042\u308b<code>basic.dataOwnerId</code>\u304c\u6b63\u3057\u3044\u30d1\u30bf\u30fc\u30f3\u3067\u8a18\u8ff0\u3055\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u30a8\u30e9\u30fc\u5185\u5bb9\u306e\u6307\u793a\u306b\u6cbf\u3063\u3066\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>Error: Error in validating system standard item in invoice.schema.json.\nPlease correct the following fields in invoice.json\nField: basic.dataOwnerId\nType: pattern\nContext: '153cbe4798cb8c' does not match '^([0-9a-zA-Z]{56})$'\n</code></pre> <p>Tip</p> <p>\u8a73\u3057\u3044\u4fee\u6b63\u65b9\u6cd5\u306f\u3001invoice.json - \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066 \u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"usage/validation/#metadatajson","title":"metadata.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3","text":"<p>\u30c7\u30fc\u30bf\u69cb\u9020\u5316\u304c\u51fa\u529b\u3059\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u540d\u524d\u3084\u30c7\u30fc\u30bf\u578b\u3092\u5ba3\u8a00\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3002\u9001\u308a\u72b6\u7b49\u306b\u5165\u529b\u3055\u308c\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u306f\u3001<code>metadata-def.json</code>\u306b\u5b9a\u7fa9\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <pre><code>import json\n\nfrom rdetoolkit.exceptions import MetadataValidationError\nfrom rdetoolkit.validation import metadata_validate\n\nmetadata = {\n    \"constant\": {\"meta1\": {\"value\": \"sample_meta\"}, \"meta2\": {\"value\": 1000, \"unit\": \"mV\"}},\n    \"variable\": [\n        {\"meta3\": {\"value\": 100, \"unit\": \"V\"}, \"meta4\": {\"value\": 200, \"unit\": \"V\"}},\n        {\"meta3\": {\"value\": 300, \"unit\": \"V\"}, \"meta4\": {\"value\": 400, \"unit\": \"V\"}},\n    ],\n}\n\nwith open(\"temp/metadata.json\", \"w\") as f:\n    json.dump(metadata, f, ensure_ascii=False, indent=2)\n\ntry:\n    metadata_validate(\"temp/metadata.json\")\nexcept ValidationError as validation_error:\n    raise MetadataValidationError from validation_error\n</code></pre>"},{"location":"usage/validation/#metadatajson_1","title":"metadata.json\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc","text":"<p><code>metadata.json</code>\u306b\u3001<code>constant</code>, <code>variable</code>\u304c\u6b63\u3057\u304f\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</p> <pre><code>metadata = {\n    \"constant\": {\"value\": \"sample_meta\"},\n    \"variable\": [\n        {\"meta3\": {\"value\": 100, \"unit\": \"V\"}, \"meta4\": {\"value\": 200, \"unit\": \"V\"}},\n        {\"meta3\": {\"value\": 300, \"unit\": \"V\"}, \"meta4\": {\"value\": 400, \"unit\": \"V\"}},\n    ],\n}\n</code></pre> <p>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\u4ee5\u4e0b\u306e\u30b1\u30fc\u30b9\u3067\u306f\u3001\u5b9a\u7fa9\u3055\u308c\u305f\u578b\u304c\u60f3\u5b9a\u3068\u7570\u306a\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002</p> <pre><code>Error: Validation Errors in metadata.json. Please correct the following fields\n1. Field: constant.key\n   Type: model_type\n   Context: Input should be a valid dictionary or instance of MetaValue\n2. Field: variable\n   Type: list_type\n   Context: Input should be a valid list\n</code></pre> <p>Tip</p> <p>\u8a73\u3057\u3044\u4fee\u6b63\u65b9\u6cd5\u306f\u3001metadata.json - \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066 \u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"usage/config/config/","title":"RDEToolKit\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb","text":"<p>rdetoolkit\u3067\u306f\u3001\u8d77\u52d5\u6642\u306e\u6319\u52d5\u3092\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3067\u5236\u5fa1\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u3067\u3059\u3002</p> <p>Reference</p> <p>API Documents: rdetoolkit.config.parse_config_file</p>"},{"location":"usage/config/config/#_1","title":"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306f\u3001<code>tasksupport</code>\u3082\u3057\u304f\u306f\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u76f4\u4e0b\u306b\u683c\u7d0d\u3055\u308c\u305f\u3001\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8aad\u307f\u8fbc\u307f\u307e\u3059\u3002</p>"},{"location":"usage/config/config/#_2","title":"\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u540d","text":"<ul> <li>rdeconfig.yaml</li> <li>rdeconfig.yaml</li> <li>rdeconfig.yml</li> <li>rdeconfig.yaml</li> <li>pyproject.toml</li> </ul>"},{"location":"usage/config/config/#_3","title":"\u8a2d\u5b9a\u53ef\u80fd\u306a\u30aa\u30d7\u30b7\u30e7\u30f3","text":""},{"location":"usage/config/config/#extendeds-mode","title":"Extendeds Mode","text":"<p>rdetoolkit\u3067\u306f\u30014\u3064\u306e\u8d77\u52d5\u30e2\u30fc\u30c9\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>invoice\u30e2\u30fc\u30c9</li> <li>ExcelInvoice\u30e2\u30fc\u30c9</li> <li>\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb</li> <li>RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9</li> </ul> <p>Documents</p> <p>\u53c2\u8003: \u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9\u306b\u3064\u3044\u3066</p> <p>\u3053\u306e\u3046\u3061\u3001\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u3068RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9\u306f\u62e1\u5f35\u30e2\u30fc\u30c9(<code>extended_mode</code>)\u3067\u3042\u308b\u305f\u3081\u3001\u4e0a\u8a182\u3064\u306e\u30e2\u30fc\u30c9\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3001<code>mode_type</code>\u306e\u6307\u5b9a\u304c\u5fc5\u8981\u3067\u3059\u3002<code>mode_type</code>\u3092\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067invoice\u30e2\u30fc\u30c9\u3068\u306a\u308a\u307e\u3059\u3002</p> \u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30ebRDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9 <pre><code>system:\n    extended_mode: 'MultiDataTile'\n</code></pre> <pre><code>system:\n    extended_mode: 'rdeformat'\n</code></pre>"},{"location":"usage/config/config/#_4","title":"\u8d77\u52d5\u6761\u4ef6","text":"\u30e2\u30fc\u30c9\u540d \u8d77\u52d5\u6761\u4ef6 invoice\u30e2\u30fc\u30c9 \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u8d77\u52d5 Excelinvoice\u30e2\u30fc\u30c9 \u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306b<code>*._excel_invoice.xlsx</code>\u3092\u683c\u7d0d \u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b<code>extended_mode: 'MultiDataTile'</code>\u3092\u8ffd\u52a0 RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9 \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b<code>extended_mode: 'rdeformat'</code>\u3092\u8ffd\u52a0"},{"location":"usage/config/config/#_5","title":"\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u81ea\u52d5\u4fdd\u5b58","text":"<p>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u81ea\u52d5\u4fdd\u5b58\u3092\u6709\u52b9\u5316\u3059\u308b\u3068\u3001\u81ea\u52d5\u7684\u306b<code>raw</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u3057\u304f\u306f\u3001<code>nonshared_raw</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3057\u307e\u3059\u3002\u4fdd\u5b58\u5148\u306f\u3001RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u516c\u958b\u3068\u5171\u306b\u30c7\u30fc\u30bf\u304c\u5171\u6709\u3055\u308c\u308b<code>raw</code>, RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u516c\u958b\u3055\u308c\u3066\u3082\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u304c\u5171\u6709\u3055\u308c\u306a\u3044<code>nonshared_raw</code>\u304c\u3042\u308a\u3001\u5229\u7528\u72b6\u6cc1\u306b\u5fdc\u3058\u3066\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3088\u308a\u8a2d\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u8a2d\u5b9a\u5024 \u5024 \u8aac\u660e save_raw <code>true</code> / <code>false</code> <code>raw</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3059\u308b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f<code>false</code> save_nonshared_raw <code>true</code> / <code>false</code> <code>nonshared_raw</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3059\u308b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f<code>true</code> \u5165\u529b\u30c7\u30fc\u30bf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u6709\u52b9\u5316(raw)\u5165\u529b\u30c7\u30fc\u30bf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u7121\u52b9\u5316(raw)\u5165\u529b\u30c7\u30fc\u30bf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u6709\u52b9\u5316(nonshared_raw)\u5165\u529b\u30c7\u30fc\u30bf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u7121\u52b9\u5316(nonshared_raw) <pre><code>system:\n    save_raw: true\n</code></pre> <pre><code>system:\n    save_raw: false\n</code></pre> <pre><code>system:\n    save_nonshared_raw: true\n</code></pre> <pre><code>system:\n    save_nonshared_raw: false\n</code></pre>"},{"location":"usage/config/config/#magic-variable","title":"magic variable","text":"<p>\u3053\u306e\u30e2\u30fc\u30c9\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5165\u529b\u30e2\u30fc\u30c9<code>invoice\u30e2\u30fc\u30c9</code>\u306e\u307f\u3067\u5b9f\u884c\u3055\u308c\u308b\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\u4e0b\u8a18\u306e\u3088\u3046\u306b\u3001\u30c7\u30fc\u30bf\u767b\u9332\u6642\u306b<code>${filename}</code>\u3068\u3044\u3046\u540d\u79f0\u3067\u30c7\u30fc\u30bf\u540d\u3092\u767b\u9332\u3059\u308b\u3068\u3001\u81ea\u52d5\u7684\u306b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30c7\u30fc\u30bf\u540d\u306b\u8ee2\u8a18\u3059\u308b\u30e2\u30fc\u30c9\u3067\u3059\u3002 \u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u3001\u30c7\u30fc\u30bf\u540d\u306b\u3001\u300c<code>${filename}</code>\u300d\u3092\u5165\u529b\u3057\u3001\u30d5\u30a1\u30a4\u30ebxrd_CI0034.rasx\u3092\u767b\u9332\u3059\u308b\u3068\u3001\u30c7\u30fc\u30bf\u540d\u304c\u3001xrd_CI0034.rasx\u306b\u7f6e\u63db\u3055\u308c\u307e\u3059\u3002</p> <p></p> magic variable\u306e\u6709\u52b9\u5316magic variable\u306e\u7121\u52b9\u5316 <pre><code>system:\n    magic_variable: true\n</code></pre> <pre><code>system:\n    magic_variable: false\n</code></pre>"},{"location":"usage/config/config/#_6","title":"\u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u306e\u81ea\u52d5\u4fdd\u5b58","text":"<p>\u30b5\u30e0\u30cd\u30a4\u30eb\u306b\u4f7f\u7528\u3059\u308b\u753b\u50cf\u3092\u81ea\u52d5\u7684\u4fdd\u5b58\u3059\u308b\u6a5f\u80fd\u3067\u3059\u3002\u3053\u306e\u30e2\u30fc\u30c9\u3092\u6709\u52b9\u5316\u3059\u308b\u3068\u3001Main\u753b\u50cf(main_image)\u30d5\u30a9\u30eb\u30c0\u306e\u753b\u50cf\u3092\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30bf\u30a4\u30eb\u306e\u30b5\u30e0\u30cd\u30a4\u30eb\u30d5\u30a9\u30eb\u30c0\u3078\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002</p> \u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u6709\u52b9\u5316\u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u7121\u52b9\u5316 <pre><code>system:\n    save_thumbnail_image: ture\n</code></pre> <pre><code>system:\n    save_thumbnail_image: false\n</code></pre>"},{"location":"usage/config/config/#_7","title":"\u72ec\u81ea\u306e\u8a2d\u5b9a\u5024\u3092\u8a2d\u5b9a\u3059\u308b","text":"<p><code>rdeconfig.yaml</code>\u7b49\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u72ec\u81ea\u306e\u8a2d\u5b9a\u5024\u3092\u8a18\u8ff0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u30b5\u30e0\u30cd\u30a4\u30eb\u306e\u753b\u50cf\u306b\u3069\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u3059\u308b\u304b\u6307\u5b9a\u3059\u308b\u5834\u5408\u3001<code>thumbnail_image_name</code>\u3068\u3044\u3046\u8a2d\u5b9a\u5024\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a18\u8ff0\u3057\u307e\u3059\u3002</p> yml\u30fbyamlpyproject.toml <pre><code>custom:\n    thumbnail_image_name: \"inputdata/sample_image.png\"\n</code></pre> <pre><code>[tool.rdetoolkit.custom]\n thumbnail_image_name: \"inputdata/sample_image.png\"\n</code></pre> <p>\u8a2d\u5b9a\u5024\u306e\u66f8\u304d\u65b9\u306b\u3064\u3044\u3066\u306f\u3001YAML\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5f93\u3063\u3066\u8a18\u8ff0\u3057\u3066\u304f\u3060\u3055\u3044\u3002: YAML Ain\u2019t Markup Language (YAML\u2122) version 1.2</p>"},{"location":"usage/config/config/#multidatatile","title":"MultiDataTile\u30e2\u30fc\u30c9\u3067\u30a8\u30e9\u30fc\u306b\u3088\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u7d42\u4e86\u3092\u30b9\u30ad\u30c3\u30d7\u3059\u308b","text":"<p><code>MultiDataTile</code>\u306f\u3001\u4e00\u5ea6\u306b\u8907\u6570\u306e\u30c7\u30fc\u30bf\u3092\u767b\u9332\u3067\u304d\u307e\u3059\u304c\u3001\u9014\u4e2d\u3067\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u3068\u3001\u51e6\u7406\u304c\u505c\u6b62\u3057\u3001\u6700\u5f8c\u307e\u3067\u69cb\u9020\u5316\u51e6\u7406\u304c\u5b9f\u884c\u3055\u308c\u307e\u305b\u3093\u3002\u3053\u306e\u3068\u304d\u3001\u5165\u529b\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u306b\u5bfe\u3057\u3066\u51e6\u7406\u3092\u6700\u5f8c\u307e\u3067\u5b9f\u884c\u3057\u305f\u3044\u5834\u5408\u3001<code>multidata_tile</code>\u3068\u3044\u3046\u30bb\u30af\u30b7\u30e7\u30f3\u306e<code>ignore_errors</code>\u3092\u6709\u52b9\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f\u3001<code>false</code>\u3068\u306a\u3063\u3066\u304a\u308a\u3001\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u3068\u51e6\u7406\u304c\u7d42\u4e86\u3057\u307e\u3059\u3002</p> \u30a8\u30e9\u30fc\u51e6\u7406\u30b9\u30ad\u30c3\u30d7\u6a5f\u80fd\u3092\u6709\u52b9\u5316\u30a8\u30e9\u30fc\u51e6\u7406\u30b9\u30ad\u30c3\u30d7\u6a5f\u80fd\u3092\u7121\u52b9\u5316 <pre><code>multidata_tile:\n    ignore_errors: true\n</code></pre> <pre><code>multidata_tile:\n    ignore_errors: false\n</code></pre> <p>\u8a2d\u5b9a\u5024\u306e\u66f8\u304d\u65b9\u306b\u3064\u3044\u3066\u306f\u3001YAML\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306b\u5f93\u3063\u3066\u8a18\u8ff0\u3057\u3066\u304f\u3060\u3055\u3044\u3002: YAML Ain\u2019t Markup Language (YAML\u2122) version 1.2</p>"},{"location":"usage/config/config/#_8","title":"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u8a2d\u5b9a\u4f8b","text":"rdeconfig.ymlpyproject.toml <pre><code>system:\n    extended_mode: 'MultiDataTile'\n    save_raw: false\n    save_nonshared_raw: true\n    magic_variable: false\n    save_thumbnail_image: true\n</code></pre> <pre><code>[tool.rdetoolkit.system]\nextended_mode = 'MultiDataTile'\nsave_raw = true\nsave_nonshared_raw=true\nmagic_variable = false\nsave_thumbnail_image = true\n</code></pre>"},{"location":"usage/config/config/#_9","title":"\u69cb\u9020\u5316\u51e6\u7406\u304b\u3089\u8a2d\u5b9a\u5024\u3092\u53c2\u7167\u3059\u308b","text":"<p>\u69cb\u9020\u5316\u51e6\u7406\u5185\u3067\u3001<code>tasksupport</code>\u306b\u683c\u7d0d\u3057\u305f\u8a2d\u5b9a\u5024\u3092\u53c2\u7167\u3059\u308b\u65b9\u6cd5\u306f\u3001<code>rdetoolkit.models.rde2types.RdeInputDirPaths.config</code>\u3067\u8a2d\u5b9a\u5024\u3092\u53c2\u7167\u3067\u304d\u307e\u3059\u3002</p> <pre><code>```python\ndef dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    # \u3053\u306e\u95a2\u6570\u5185\u3067\u30e6\u30fc\u30b6\u81ea\u8eab\u304c\u5b9a\u7fa9\u3057\u305f\u30af\u30e9\u30b9\u3084\u95a2\u6570\u3092\u8a18\u8ff0\u3059\u308b\n    ... #\u4efb\u610f\u306e\u51e6\u7406\n\n    # Extendeds Mode\u306e\u8a2d\u5b9a\u5024\u3092\u53d6\u5f97\u3059\u308b\n    print(srcpaths.config.system.extended_mode)\n\n    # \u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u8a2d\u5b9a\u5024\u3092\u53d6\u5f97\u3059\u308b\n    print(srcpaths.config.system.save_raw)\n\n    # \u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u306e\u81ea\u52d5\u4fdd\u5b58\u306e\u8a2d\u5b9a\u5024\u3092\u53c2\u7167\u3059\u308b\n    print(srcpaths.config.system.save_thumbnail_image)\n\n    # magic variable\u306e\u8a2d\u5b9a\u5024\u3092\u53c2\u7167\u3059\u308b\n    print(srcpaths.config.system.magic_variable)\n\n    # \u72ec\u81ea\u306e\u8a2d\u5b9a\u5024\u3092\u53c2\u7167\u3059\u308b\n    print(srcpaths.config[\"custom\"][\"thumbnail_image_name\"])\n```\n</code></pre>"},{"location":"usage/config/file_folder_mode/","title":"\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306e\u5f62\u5f0f\u306b\u3064\u3044\u3066","text":"<p>Excelinvoice\u30e2\u30fc\u30c9\u3067\u30c7\u30fc\u30bf\u3092\u767b\u9332\u3059\u308b\u3068\u304d\u3001RDE\u306b\u306f\u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9\u3068\u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9\u3068\u3044\u3046\u6982\u5ff5\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9\u3068\u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9\u306e\u8aac\u660e\u3068\u3001\u8a18\u8ff0\u30eb\u30fc\u30eb\u3001\u305d\u306e\u4ed6\u6ce8\u610f\u4e8b\u9805\u7b49\u306b\u3064\u3044\u3066\u307e\u3068\u3081\u307e\u3059\u3002</p>"},{"location":"usage/config/file_folder_mode/#_2","title":"\u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9","text":"<p>Excelinvoice\u306e\u30c7\u30fc\u30bf\u5217(A\u5217)\u306b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5217\u6319\u3057\u3001\u8a18\u8f09\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u767b\u9332\u3059\u308b\u30e2\u30fc\u30c9\u306e\u3053\u3068\u3092\u6307\u3057\u307e\u3059\u3002</p> <p>\u8d77\u52d5\u6761\u4ef6\u3084\u5236\u7d04\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <ul> <li>\u4e0b\u56f3\u306e\u3001A\u5217\u306e\u3088\u3046\u306b<code>data_file_names</code>\u306e\u6b21\u306e\u884c\u306b<code>name</code>\u3068\u8a18\u8f09\u3059\u308b\u3002(\u30c7\u30d5\u30a9\u30eb\u30c8)</li> <li>\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8a18\u8f09\u3059\u308b\u3002(zip\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3067\u306f\u306a\u3044\u3002)</li> </ul> <p></p> <p>\u3053\u306e\u6642\u306e\u5165\u529bzip\u30d5\u30a1\u30a4\u30eb\u306e\u69cb\u6210\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002zip\u30d5\u30a1\u30a4\u30eb\u540d\u306f\u4eee\u306e\u540d\u79f0\u3068\u3057\u307e\u3059\u3002 \u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9\u3067\u5165\u529b\u3059\u308bzip\u30d5\u30a1\u30a4\u30eb\u306b\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u304c\u5e73\u7f6e\u304d\u3067\u683c\u7d0d\u3055\u308c\u305f\u72b6\u614b\u3067\u5727\u7e2e\u3057\u307e\u3059\u3002</p> <pre><code>input.zip/\n|-- 20100131045801(2s).txt\n|-- 20100131052029(2p).txt\n</code></pre>"},{"location":"usage/config/file_folder_mode/#_3","title":"\u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9","text":"<p>Excelinvoice\u306e\u30c7\u30fc\u30bf\u5217(A\u5217)\u306b\u30d5\u30a9\u30eb\u30c0\u540d\u3092\u5217\u6319\u3057\u3001\u8a18\u8f09\u3057\u305f\u30d5\u30a9\u30eb\u30c0\u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u767b\u9332\u3059\u308b\u30e2\u30fc\u30c9\u306e\u3053\u3068\u3092\u6307\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u30e2\u30fc\u30c9\u306e\u5229\u7528\u60f3\u5b9a\u306f\u3001Excelinvoice\u3092\u5229\u7528\u3057\u3001\u8907\u6570\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u3064\u306e\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u306b\u767b\u9332\u3057\u305f\u3044\u5834\u5408\u3001\u3053\u306e\u30e2\u30fc\u30c9\u3067\u30c7\u30fc\u30bf\u767b\u9332\u3092\u884c\u3044\u307e\u3059\u3002</p> <p></p> <p>\u3053\u306e\u6642\u306e\u5165\u529bzip\u30d5\u30a1\u30a4\u30eb\u306e\u69cb\u6210\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002zip\u30d5\u30a1\u30a4\u30eb\u540d\u306f\u4eee\u306e\u540d\u79f0\u3068\u3057\u307e\u3059\u3002 \u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9\u3067\u5165\u529b\u3059\u308bzip\u30d5\u30a1\u30a4\u30eb\u306b\u306f\u3001\u30d5\u30a9\u30eb\u30c0\u304c\u683c\u7d0d\u3055\u308c\u305f\u72b6\u614b\u3067\u5727\u7e2e\u3057\u767b\u9332\u3057\u307e\u3059\u3002\u3053\u306e\u3068\u304d\u3001\u30d5\u30a9\u30eb\u30c0\u304c\u767b\u9332\u3055\u308c\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u30d5\u30a9\u30eb\u30c0\u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u767b\u9332\u3055\u308c\u308b\u305f\u3081\u3001\u3042\u3089\u304b\u3058\u3081\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\u3002</p> <pre><code>input.zip/\n|-- sample_folder_1/\n|   |-- file1.txt\n|   |-- file2.txt\n|\n|-- sample_folder_2/\n|   |-- file3.txt\n|   |-- file4.txt\n</code></pre>"},{"location":"usage/config/file_folder_mode/#_4","title":"\u305d\u306e\u4ed6","text":""},{"location":"usage/config/file_folder_mode/#_5","title":"\u8868\u8a18\u3057\u305f\u30e2\u30fc\u30c9\u3068\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306e\u69cb\u6210\u304c\u7570\u306a\u308b\u5834\u5408","text":"<p>Excelinvoice\u306b\u8868\u8a18\u3057\u305f\u30e2\u30fc\u30c9\u3068\u3001\u5165\u529b\u3059\u308bzip\u30d5\u30a1\u30a4\u30eb\u306e\u69cb\u6210\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u30a8\u30e9\u30fc\u3068\u306a\u308a\u51e6\u7406\u3092\u6b63\u3057\u304f\u5b9f\u884c\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"usage/config/file_folder_mode/#_6","title":"\u3053\u306e\u51e6\u7406\u306b\u3064\u3044\u3066","text":"<p>\u3053\u306e\u51e6\u7406\u306f\u3001RdeToolKit\u3067\u5b9a\u7fa9\u3057\u3066\u3044\u307e\u3059\u3002\u8a72\u5f53\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306f\u4ee5\u4e0b\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30ea\u30f3\u30af\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>Reference</p> <ul> <li>parse_compressedfile_mode - compressed_controller</li> </ul>"},{"location":"usage/config/magic_variable/","title":"Magic Variable\u306b\u3088\u308b\u30c7\u30fc\u30bf\u767b\u9332\u306b\u3064\u3044\u3066","text":"<ul> <li>\u8d77\u52d5\u6761\u4ef6: \u30c7\u30fc\u30bf\u767b\u9332\u6642\u306b<code>${filename}</code>\u3092\u5165\u529b\u3059\u308b\u3068\u5165\u529b\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u540d\u304c\u30c7\u30fc\u30bf\u540d\u306b\u767b\u9332\u3055\u308c\u308b</li> <li>\u5bfe\u8c61\u30e2\u30fc\u30c9: invoice\u30e2\u30fc\u30c9\u3001Excelinvoice\u30e2\u30fc\u30c9, \u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb</li> <li>\u5099\u8003: \u3053\u306e\u6a5f\u80fd\u306f<code>RDEToolKit v0.1.5</code>\u4ee5\u964d\u304b\u3089\u5229\u7528\u53ef\u80fd\u3067\u3059\u3002</li> </ul> <p>\u3053\u306e\u30e2\u30fc\u30c9\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5165\u529b\u30e2\u30fc\u30c9invoice\u30e2\u30fc\u30c9\u306e\u307f\u3067\u5b9f\u884c\u3055\u308c\u308b\u51e6\u7406\u306b\u306a\u308a\u307e\u3059\u3002\u4e0b\u8a18\u306e\u3088\u3046\u306b\u3001\u30c7\u30fc\u30bf\u767b\u9332\u6642\u306b<code>${filename}</code>\u3068\u3044\u3046\u540d\u79f0\u3067\u30c7\u30fc\u30bf\u540d\u3092\u767b\u9332\u3059\u308b\u3068\u3001\u81ea\u52d5\u7684\u306b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u30c7\u30fc\u30bf\u540d\u306b\u8ee2\u8a18\u3059\u308b\u30e2\u30fc\u30c9\u3067\u3059\u3002 \u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u3001\u30c7\u30fc\u30bf\u540d\u306b\u3001\u300c<code>${filename}</code>\u300d\u3092\u5165\u529b\u3057\u3001\u30d5\u30a1\u30a4\u30eb<code>xrd_CI0034.rasx</code>\u3092\u767b\u9332\u3059\u308b\u3068\u3001\u30c7\u30fc\u30bf\u540d\u304c\u3001<code>xrd_CI0034.rasx</code>\u306b\u7f6e\u63db\u3055\u308c\u307e\u3059\u3002</p> <p></p>"},{"location":"usage/config/magic_variable/#invoicejson","title":"\u5b9f\u884c\u524d\u306einvoice.json","text":"<pre><code>{\n    \"datasetId\": \"4c747c9a-ef13-4058-9e36-d76bb6531658\",\n    \"basic\": {\n        \"dateSubmitted\": \"2023-06-27\",\n        \"dataOwnerId\": \"222aaa4798cb8c1c3c19c66062c7e55a9b4255fe336461301233456\",\n        \"dataName\": \"${filename}\",\n        \"instrumentId\": null,\n        \"experimentId\": null,\n        \"description\": \"\"\n    },\n    \"custom\": null\n}\n</code></pre>"},{"location":"usage/config/magic_variable/#invoicejson_1","title":"\u5b9f\u884c\u5f8c\u306einvoice.json","text":"<pre><code>{\n    \"datasetId\": \"4c747c9a-ef13-4058-9e36-d76bb6531658\",\n    \"basic\": {\n        \"dateSubmitted\": \"2023-06-27\",\n        \"dataOwnerId\": \"222aaa4798cb8c1c3c19c66062c7e55a9b4255fe336461301233456\",\n        \"dataName\": \"data0000.dat\",\n        \"instrumentId\": null,\n        \"experimentId\": null,\n        \"description\": \"\"\n    },\n    \"custom\": null\n}\n</code></pre>"},{"location":"usage/config/mode/","title":"RDEToolKit\u306e\u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9","text":"<p>RDE\u306e\u69cb\u9020\u5316\u51e6\u7406\u3067\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u308b\u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9\u306f\u3001\u4ee5\u4e0b5\u3064\u306e\u30e2\u30fc\u30c9\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002</p> \u30e2\u30fc\u30c9\u540d \u8d77\u52d5\u6761\u4ef6 invoice\u30e2\u30fc\u30c9 \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9 Excelinvoice\u30e2\u30fc\u30c9 \u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306b<code>*._excel_invoice.xlsx</code>\u3092\u683c\u7d0d SmartTableInvoice\u30e2\u30fc\u30c9 \u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306b<code>smarttable_*.{xlsx,csv,tsv}</code>\u3092\u683c\u7d0d \u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b<code>extended_mode: 'MultiDataTile'</code>\u3092\u8ffd\u52a0 RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9 \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b<code>extended_mode: 'rdeformat'</code>\u3092\u8ffd\u52a0 <p>\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u3068\u3001RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9\u306f\u3001invoice\u30e2\u30fc\u30c9\u306e\u62e1\u5f35\u6a5f\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>flowchart LR\n  ModeA--&gt;ModeB\n  ModeA--&gt;ModeE\n  ModeA--&gt;ModeF\n  ModeB--&gt;ModeC\n  ModeB--&gt;ModeD\n  ModeA[RDE\u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9]\n  ModeB[invoice\u30e2\u30fc\u30c9]\n  subgraph extended mode\n  ModeC[\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb]\n  ModeD[RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9]\n  end\n  ModeE[Excelinvoice\u30e2\u30fc\u30c9]\n  ModeF[SmartTableInvoice\u30e2\u30fc\u30c9]\n</code></pre> <p>\u3053\u3053\u3067\u306f\u3001\u5404\u7a2e\u30e2\u30fc\u30c9\u306e\u8aac\u660e\u3068\u5b9f\u884c\u4f8b\u3092\u307e\u3068\u3081\u307e\u3057\u305f\u3002</p>"},{"location":"usage/config/mode/#invoice","title":"invoice\u30e2\u30fc\u30c9","text":""},{"location":"usage/config/mode/#_1","title":"\u8aac\u660e","text":"<p>\u3053\u306e\u30e2\u30fc\u30c9\u306f\u3001\u901a\u5e38\u306eRDE\u767b\u9332\u753b\u9762\u3067\u30c7\u30fc\u30bf\u3092\u767b\u9332\u3059\u308b\u30e2\u30fc\u30c9\u3092\u6307\u3057\u307e\u3059\u3002\u4e00\u756a\u3001\u57fa\u672c\u7684\u304b\u3064\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30e2\u30fc\u30c9\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u3001Entry\u753b\u9762\u304b\u3089\u3001\u30c7\u30fc\u30bf\u3092\u6295\u5165\u3059\u308b\u30e2\u30fc\u30c9\u3067\u3059\u3002</p> <p></p>"},{"location":"usage/config/mode/#_2","title":"\u8d77\u52d5\u6761\u4ef6","text":"<p>\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30c7\u30fc\u30bf\u767b\u9332\u30e2\u30fc\u30c9\u3002\u8a2d\u5b9a\u7b49\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"usage/config/mode/#invoice_1","title":"invoice\u30e2\u30fc\u30c9\u5b9f\u884c\u4f8b","text":""},{"location":"usage/config/mode/#_3","title":"\u6295\u5165\u30c7\u30fc\u30bf","text":"\u767b\u9332\u30d5\u30a1\u30a4\u30eb \u8aac\u660e <code>data/inputdata/test23_1.csv</code> \u767b\u9332\u3059\u308b\u30c7\u30fc\u30bf <code>data/invoice/invoice.json</code> \u30ed\u30fc\u30ab\u30eb\u3067\u4e8b\u524d\u306b\u6e96\u5099\u30fb\u4f5c\u6210/\u30b7\u30b9\u30c6\u30e0\u304c\u81ea\u52d5\u7684\u306b\u751f\u6210 <code>data/tasksupport/invoice.schema.json</code> \u30ed\u30fc\u30ab\u30eb\u3067\u4e8b\u524d\u306b\u6e96\u5099\u30fb\u4f5c\u6210/\u30b7\u30b9\u30c6\u30e0\u306b\u4e8b\u524d\u306b\u767b\u9332 <code>data/tasksupport/metadata-def.json</code> \u30ed\u30fc\u30ab\u30eb\u3067\u4e8b\u524d\u306b\u6e96\u5099\u30fb\u4f5c\u6210/\u30b7\u30b9\u30c6\u30e0\u306b\u4e8b\u524d\u306b\u767b\u9332 <pre><code>data\n\u251c\u2500\u2500 inputdata\n\u2502   \u2514\u2500\u2500 test23_1.csv\n\u251c\u2500\u2500 invoice\n\u2502   \u2514\u2500\u2500 invoice.json\n\u2514\u2500\u2500 tasksupport\n    \u251c\u2500\u2500 rdeconfig.yaml\n    \u251c\u2500\u2500 invoice.schema.json\n    \u2514\u2500\u2500 metadata-def.json\n</code></pre>"},{"location":"usage/config/mode/#config","title":"config\u30d5\u30a1\u30a4\u30eb","text":"<p>\u30e2\u30fc\u30c9\u306e\u6307\u5b9a\u7b49\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u306f\u3042\u308a\u307e\u305b\u3093\u3002</p> <pre><code>save_raw: true\nmagic_variable: false\nsave_thumbnail_image: ture\n</code></pre>"},{"location":"usage/config/mode/#_4","title":"\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c\u5b9f\u884c\u5f8c\u30d5\u30a1\u30a4\u30eb\u69cb\u6210","text":"<p>\u4e0a\u8a18\u306e\u8a2d\u5b9a\u3067\u3001\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <pre><code>data\n\u251c\u2500\u2500 inputdata\n\u2502   \u2514\u2500\u2500 test23_1.csv\n\u251c\u2500\u2500 invoice\n\u2502   \u2514\u2500\u2500 invoice.json\n\u251c\u2500\u2500 logs\n\u2502   \u2514\u2500\u2500 rdesys.log\n\u251c\u2500\u2500 main_image\n\u251c\u2500\u2500 meta\n\u251c\u2500\u2500 other_image\n\u251c\u2500\u2500 raw\n\u2502   \u2514\u2500\u2500 test23_1.csv\n\u251c\u2500\u2500 structured\n\u251c\u2500\u2500 tasksupport\n\u2502   \u251c\u2500\u2500 rdeconfig.yaml\n\u2502   \u251c\u2500\u2500 invoice.schema.json\n\u2502   \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 temp\n\u2514\u2500\u2500 thumbnail\n</code></pre>"},{"location":"usage/config/mode/#excelinvoice","title":"ExcelInvoice\u30e2\u30fc\u30c9","text":""},{"location":"usage/config/mode/#_5","title":"\u8aac\u660e","text":"<p>\u3053\u306e\u30e2\u30fc\u30c9\u306f\u3001\u4e00\u5ea6\u306b\u8907\u6570\u4ef6\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u767b\u9332\u3059\u308b\u3082\u306e\u30e2\u30fc\u30c9\u3067\u3059\u3002\u901a\u5e38\u306einvoice\u30e2\u30fc\u30c9\u3067\u306f\u3001\u4e00\u4ef6\u305a\u3064\u3057\u304b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u767b\u9332\u5b9f\u884c\u3067\u304d\u307e\u305b\u3093\u304c\u3001Excelinvoice\u30e2\u30fc\u30c9\u3092\u4f7f\u3046\u3068\u3001\u4e00\u5ea6\u306b\u8907\u6570\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u767b\u9332\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u3053\u306e\u30e2\u30fc\u30c9\u306e\u8d77\u52d5\u6761\u4ef6\u3068\u3057\u3066\u3001\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306b\u3001<code>*._excel_invoice.xlsx</code>\u3068\u3044\u3046\u547d\u540d\u898f\u5247\u3092\u6301\u3064Excel\u30d5\u30a1\u30a4\u30eb\u3092\u6295\u5165\u3059\u308b\u3068Excelinvoice\u3068\u3057\u3066\u767b\u9332\u3055\u308c\u307e\u3059\u3002</p> <p>\u3053\u306eExcelinvoice\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306fRDE\u3078\u554f\u3044\u5408\u308f\u305b\u304f\u3060\u3055\u3044\u3002</p> <p></p> <p>Documents</p> <p>ExcelInvoice\u306b\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9\u3068\u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9\u3068\u3044\u3046\u6982\u5ff5\u304c\u3042\u308a\u307e\u3059\u3002File Mode / Folder Mode\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"usage/config/mode/#_6","title":"\u8d77\u52d5\u6761\u4ef6","text":"<p>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306b<code>*._excel_invoice.xlsx</code>\u3092\u683c\u7d0d\u3059\u308b</p>"},{"location":"usage/config/mode/#excelinvoice_1","title":"ExcelInvoice\u30e2\u30fc\u30c9\u5b9f\u884c\u4f8b","text":""},{"location":"usage/config/mode/#_7","title":"\u6295\u5165\u30c7\u30fc\u30bf","text":"<ul> <li>\u767b\u9332\u30d5\u30a1\u30a4\u30eb(data/inputdata)</li> <li>data.zip (\u6295\u5165\u30d5\u30a1\u30a4\u30eb\u3092zip\u5727\u7e2e\u3057\u305f\u3082\u306e)</li> <li>sample_excel_invoice.xlsx (\u3053\u306e\u4e8b\u4f8b\u3067\u306f3\u884c3\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u5206\u3092\u8a18\u8f09)</li> <li>tasksupport</li> <li>\u8ffd\u52a0\u306a\u3057</li> </ul>"},{"location":"usage/config/mode/#_8","title":"\u5b9f\u884c\u524d\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u6210","text":"<p><code>data/invoice/invoice.json</code>\u306f\u3001\u7a7a\u306ejson\u30d5\u30a1\u30a4\u30eb\u3067\u3082\u69cb\u3044\u307e\u305b\u3093\u3002</p> <pre><code>container/\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 &lt;\u4efb\u610f\u306e\u69cb\u9020\u5316\u51e6\u7406\u30e2\u30b8\u30e5\u30fc\u30eb&gt;\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 inputdata\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 data.zip\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample_excel_invoice.xlsx\n    \u251c\u2500\u2500 invoice\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n    \u2514\u2500\u2500 tasksupport\n        \u251c\u2500\u2500 invoice.schema.json\n        \u2514\u2500\u2500 metadata-def.json\n</code></pre> <p>data.zip\u306e\u5185\u5bb9\u306f\u3001\u30a8\u30af\u30bb\u30eb\u30a4\u30f3\u30dc\u30a4\u30b9\u306b3\u884c\u8ffd\u52a0\u3059\u308b\u305f\u30813\u30d5\u30a1\u30a4\u30ebzip\u5316\u3059\u308b\u3002</p> <pre><code>$ unzip -t data.zip\nArchive:  data.zip\n    testing: data0000.dat\n    testing: data0001.dat\n    testing: data0002.dat\n</code></pre> <p></p>"},{"location":"usage/config/mode/#_9","title":"\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c\u5f8c\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020","text":"<p>\u4e0a\u8a18\u306e\u8a2d\u5b9a\u3067\u3001\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <ul> <li>data.zip\u306e\u5185\u5bb9\u306f\u5c55\u958b\u3055\u308c\u308b</li> <li>sample_excel_invoice.xlsx\u306e\u8a18\u5165\u5185\u5bb9\u306b\u5f93\u3063\u3066divided\u3092\u542b\u3080\u30d5\u30a9\u30eb\u30c0\u306b\u5c55\u958b</li> <li>\u5404invoice.json\u306f\u3001excel_invoice\u306e\u5404\u884c\u304b\u3089\u8aad\u307f\u51fa\u3057\u305f\u60c5\u5831\u304c\u5165\u529b\u3055\u308c\u308b</li> </ul> <pre><code>data\n\u251c\u2500\u2500 divided\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 0001\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 other_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 data0001.dat\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 structured\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 temp\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 thumbnail\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 0002\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 other_image\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 data0002.dat\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 structured\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 temp\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 thumbnail\n\u251c\u2500\u2500 inputdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data.zip\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sample_excel_invoice.xlsx\n\u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 rdesys.log\n\u251c\u2500\u2500 main_image\n\u251c\u2500\u2500 meta\n\u251c\u2500\u2500 other_image\n\u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 data0000.dat\n\u251c\u2500\u2500 structured\n\u251c\u2500\u2500 tasksupport\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice.schema.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 temp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data0000.dat\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data0001.dat\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data0002.dat\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice_org.json\n\u2514\u2500\u2500 thumbnail\n</code></pre>"},{"location":"usage/config/mode/#divided","title":"divided\u30d5\u30a9\u30eb\u30c0\u4ee5\u4e0b\u306e\u5185\u5bb9\u306b\u3064\u3044\u3066","text":"<p><code>divided/0001/invoice/invoice.json</code>\u306e\u5185\u5bb9\u306f\u3001\u4e8b\u524d\u306b\u914d\u7f6e\u3055\u308c\u305finvoice.json\u304c\u30b3\u30d4\u30fc\u3055\u308c<code>basic/dataName</code>, <code>basic/dataOwnerId</code> \u304c\u30a8\u30af\u30bb\u30eb\u30a4\u30f3\u30dc\u30a4\u30b9\u306e\u5185\u5bb9\u3067\u66f8\u304d\u63db\u3048\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u3002</p> <p>\u66f8\u304d\u63db\u3048\u5f8c\u306e\u3001<code>data/invoice/invoice.json</code></p> <pre><code>{\n    \"datasetId\": \"ab9536f2-5fe4-49c4-bb82-dd8212453d85\",\n    \"basic\": {\n        \"dateSubmitted\": \"2023-03-14\",\n        \"dataOwnerId\": \"153cbe4798cb8c1c3c0fc66062c7e55a9b4255fe3364613035643239\",\n        \"dataName\": \"dumm.dat\",\n        \"instrumentId\": null,\n        \"experimentId\": null,\n        \"description\": null\n    },\n    \"custom\": null\n}\n</code></pre> <p>\u66f8\u304d\u63db\u3048\u5f8c\u306e\u3001<code>data/divided/0001/invoice/invoice.json</code></p> <pre><code>{\n    \"datasetId\": \"e751fcc4-b926-4747-b236-cab40316fc49\",\n    \"basic\": {\n        \"dateSubmitted\": \"2023-03-14\",\n        \"dataOwnerId\": \"97e05f8b9ed6b4b5dd6fd50411a9c163a2d4e38d6264623666383663\",\n        \"dataName\": \"data0001.dat\",\n        \"instrumentId\": null,\n        \"experimentId\": null,\n        \"description\": null\n    }\n}\n</code></pre> <p>Warning</p> <ul> <li><code>smple.zip</code>\u306b\u4e0d\u8981\u306a\u30d5\u30a1\u30a4\u30eb\u304c\u542b\u307e\u308c\u3066\u3044\u306a\u3044\u304b\u78ba\u8a8d\u3059\u308b\u3002Mac\u7279\u6709\u306e<code>.DS_Store</code>\u30d5\u30a1\u30a4\u30eb\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u5b9f\u884c\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</li> <li>\u30a8\u30af\u30bb\u30eb\u30a4\u30f3\u30dc\u30a4\u30b9\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u305f\u307e\u307e\u5b9f\u884c\u3057\u3066\u3044\u308b\u5834\u5408\u3001Microsoft\u7279\u6709\u306e\u30d5\u30a1\u30a4\u30eb(<code>~$</code>\u304b\u3089\u59cb\u307e\u308b\u30d5\u30a1\u30a4\u30eb)\u304c\u6b8b\u3063\u3066\u3057\u307e\u3044\u3001\u5b9f\u884c\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</li> <li>\u30ed\u30fc\u30ab\u30eb\u3067\u5b9f\u884c\u3059\u308b\u5834\u5408\u3001temp\u30d5\u30a9\u30eb\u30c0\u306b\u524d\u56de\u306e\u5b9f\u884c\u7d50\u679c\u304c\u6b8b\u3063\u3066\u3044\u308b\u3068\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"usage/config/mode/#zipexcelinvoice","title":"\u30d5\u30a9\u30eb\u30c0\u3092\u542b\u3080zip\u306eExcelInvoice\u30e2\u30fc\u30c9\u5b9f\u884c\u4f8b","text":"<p>\u30d5\u30a9\u30eb\u30c0\u3092\u542b\u3080zip\u30d5\u30a1\u30a4\u30eb\u3092\u767b\u9332\u3059\u308b\u65b9\u6cd5\u306f\u3001zip\u30d5\u30a1\u30a4\u30eb\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <p>Documents</p> <p>ExcelInvoice\u306b\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u30e2\u30fc\u30c9\u3068\u30d5\u30a9\u30eb\u30c0\u30e2\u30fc\u30c9\u3068\u3044\u3046\u6982\u5ff5\u304c\u3042\u308a\u307e\u3059\u3002File Mode / Folder Mode\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p> <pre><code># \u30d5\u30a9\u30eb\u30c0\u3042\u308a\u3067zip\n$ zip data_folder.zip -r ./inputdata -x \\*/.DS_Store *\\.xlsx\n  adding: inputdata/ (stored 0%)\n  adding: inputdata/data0001.dat (stored 0%)\n  adding: inputdata/data0000.dat (stored 0%)\n  adding: inputdata/data0002.dat (stored 0%)\n</code></pre>"},{"location":"usage/config/mode/#smarttableinvoice","title":"SmartTableInvoice\u30e2\u30fc\u30c9","text":""},{"location":"usage/config/mode/#_10","title":"\u8aac\u660e","text":"<p>SmartTableInvoice\u30e2\u30fc\u30c9\u306f\u3001\u30c6\u30fc\u30d6\u30eb\u30d5\u30a1\u30a4\u30eb\uff08Excel/CSV/TSV\uff09\u304b\u3089\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u53d6\u308a\u3001\u81ea\u52d5\u7684\u306binvoice.json\u30d5\u30a1\u30a4\u30eb\u3092\u751f\u6210\u3059\u308b\u65b0\u3057\u3044\u767b\u9332\u30e2\u30fc\u30c9\u3067\u3059\u3002ExcelInvoice\u30e2\u30fc\u30c9\u3068\u540c\u69d8\u306b\u8907\u6570\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4e00\u5ea6\u306b\u767b\u9332\u3067\u304d\u307e\u3059\u304c\u3001\u3088\u308a\u67d4\u8edf\u306a\u30e1\u30bf\u30c7\u30fc\u30bf\u30de\u30c3\u30d4\u30f3\u30b0\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u30e2\u30fc\u30c9\u306e\u7279\u5fb4\uff1a - \u591a\u5f62\u5f0f\u5bfe\u5fdc: Excel (.xlsx)\u3001CSV\u3001TSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f - 2\u884c\u30d8\u30c3\u30c0\u30fc\u5f62\u5f0f: 1\u884c\u76ee\u306b\u8868\u793a\u540d\u30012\u884c\u76ee\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u30ad\u30fc\u3092\u914d\u7f6e - \u81ea\u52d5\u30e1\u30bf\u30c7\u30fc\u30bf\u30de\u30c3\u30d4\u30f3\u30b0: <code>basic/</code>\u3001<code>custom/</code>\u3001<code>sample/</code>\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\u306b\u3088\u308b\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u751f\u6210 - \u914d\u5217\u30c7\u30fc\u30bf\u30b5\u30dd\u30fc\u30c8: <code>generalAttributes</code>\u304a\u3088\u3073<code>specificAttributes</code>\u3078\u306e\u9069\u5207\u306a\u30de\u30c3\u30d4\u30f3\u30b0 - zip\u30d5\u30a1\u30a4\u30eb\u7d71\u5408: \u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u542b\u3080zip\u3068\u30c6\u30fc\u30d6\u30eb\u30d5\u30a1\u30a4\u30eb\u306e\u81ea\u52d5\u95a2\u9023\u4ed8\u3051</p> <p></p>"},{"location":"usage/config/mode/#_11","title":"\u8d77\u52d5\u6761\u4ef6","text":"<p>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306b<code>smarttable_*.{xlsx,csv,tsv}</code>\u3092\u683c\u7d0d\u3059\u308b</p>"},{"location":"usage/config/mode/#_12","title":"\u30c6\u30fc\u30d6\u30eb\u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f","text":""},{"location":"usage/config/mode/#_13","title":"\u30d8\u30c3\u30c0\u30fc\u69cb\u6210","text":"<p>\u30c6\u30fc\u30d6\u30eb\u30d5\u30a1\u30a4\u30eb\u306f\u4ee5\u4e0b\u306e\u5f62\u5f0f\u3067\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code># 1\u884c\u76ee: \u8868\u793a\u540d\uff08\u30e6\u30fc\u30b6\u30fc\u5411\u3051\u306e\u8aac\u660e\u3001\u51e6\u7406\u5bfe\u8c61\u5916\uff09\n\u30c7\u30fc\u30bf\u540d,\u5165\u529b\u30d5\u30a1\u30a4\u30eb1,\u5165\u529b\u30d5\u30a1\u30a4\u30eb2,\u30b5\u30a4\u30af\u30eb,\u539a\u3055,\u6e29\u5ea6,\u8a66\u6599\u540d,\u8a66\u6599ID,\u4e00\u822c\u9805\u76ee\n\n# 2\u884c\u76ee: \u30de\u30c3\u30d4\u30f3\u30b0\u30ad\u30fc\uff08\u5b9f\u969b\u306e\u51e6\u7406\u3067\u4f7f\u7528\uff09\nbasic/dataName,inputdata1,inputdata2,custom/cycle,custom/thickness,custom/temperature,sample/names,sample/sampleId,sample/generalAttributes.3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\n\n# 3\u884c\u76ee\u4ee5\u964d: \u30c7\u30fc\u30bf\n\u5b9f\u9a131,file1.txt,file2.txt,1,2mm,25,sample001,S001,value1\n\u5b9f\u9a132,file3.txt,file4.txt,2,3mm,30,sample002,S002,value2\n20250601_exp1,file1,sub/file11,file12,3,25,sample001,0,value1\n20250601_exp2,file2,sub/file21,file22,4,30,sample002,1,value2\n20250601_exp3,,,,5,50,sample003,2,value3\n</code></pre> <p></p>"},{"location":"usage/config/mode/#_14","title":"\u30de\u30c3\u30d4\u30f3\u30b0\u30ad\u30fc\u306e\u4ed5\u69d8","text":"<ul> <li><code>basic/xxxx</code>: invoice.json\u306e<code>basic</code>\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5185\u306e<code>xxxx</code>\u30ad\u30fc\u306b\u30de\u30c3\u30d4\u30f3\u30b0</li> <li><code>custom/xxxx</code>: invoice.json\u306e<code>custom</code>\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5185\u306e<code>xxxx</code>\u30ad\u30fc\u306b\u30de\u30c3\u30d4\u30f3\u30b0</li> <li><code>sample/xxxx</code>: invoice.json\u306e<code>sample</code>\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u5185\u306e<code>xxxx</code>\u30ad\u30fc\u306b\u30de\u30c3\u30d4\u30f3\u30b0</li> <li><code>sample/generalAttributes.&lt;termId&gt;</code>: <code>generalAttributes</code>\u914d\u5217\u5185\u306e\u8a72\u5f53\u3059\u308b<code>termId</code>\u306e<code>value</code>\u306b\u30de\u30c3\u30d4\u30f3\u30b0</li> <li><code>sample/specificAttributes.&lt;classId&gt;.&lt;termId&gt;</code>: <code>specificAttributes</code>\u914d\u5217\u5185\u306e\u8a72\u5f53\u3059\u308b<code>classId</code>\u3068<code>termId</code>\u306e<code>value</code>\u306b\u30de\u30c3\u30d4\u30f3\u30b0</li> <li><code>inputdataX</code>: zip\u30d5\u30a1\u30a4\u30eb\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u6307\u5b9a\uff08X=1,2,3...\uff09</li> <li><code>meta/xxxx</code>: \u30e1\u30bf\u60c5\u5831\u3068\u3057\u3066\u6271\u308f\u308c\u3001invoice.json\u306b\u306f\u51fa\u529b\u3055\u308c\u306a\u3044</li> </ul>"},{"location":"usage/config/mode/#smarttableinvoice_1","title":"SmartTableInvoice\u30e2\u30fc\u30c9\u5b9f\u884c\u4f8b","text":""},{"location":"usage/config/mode/#_15","title":"\u6295\u5165\u30c7\u30fc\u30bf","text":"\u30d5\u30a1\u30a4\u30eb \u8aac\u660e <code>data/inputdata/smarttable_experiment.xlsx</code> \u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u542b\u3080\u30c6\u30fc\u30d6\u30eb\u30d5\u30a1\u30a4\u30eb <code>data/inputdata/data.zip</code> \u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u542b\u3080zip\u30d5\u30a1\u30a4\u30eb\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09 <code>data/invoice/invoice.json</code> \u65e2\u5b58\u306einvoice.json\uff08\u5024\u304c\u7d99\u627f\u3055\u308c\u308b\uff09 <code>data/tasksupport/invoice.schema.json</code> \u30b9\u30ad\u30fc\u30de\u30d5\u30a1\u30a4\u30eb <pre><code>data\n\u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 smarttable_experiment.xlsx\n\u2502   \u2514\u2500\u2500 data.zip\n\u251c\u2500\u2500 invoice\n\u2502   \u2514\u2500\u2500 invoice.json\n\u2514\u2500\u2500 tasksupport\n    \u251c\u2500\u2500 invoice.schema.json\n    \u2514\u2500\u2500 metadata-def.json\n</code></pre>"},{"location":"usage/config/mode/#_16","title":"\u51e6\u7406\u30d5\u30ed\u30fc","text":"<ol> <li>SmartTable\u30d5\u30a1\u30a4\u30eb\u306e\u5404\u884c\u3092\u500b\u5225\u306eCSV\u30d5\u30a1\u30a4\u30eb\u306b\u5206\u5272</li> <li>\u30d5\u30a1\u30a4\u30eb\u540d: <code>fsmarttable_experiment_0001.csv</code>, <code>fsmarttable_experiment_0002.csv</code>...</li> <li>zip\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u5834\u5408\u306f<code>data/temp</code>\u306b\u5c55\u958b</li> <li><code>inputdataX</code>\u5217\u306e\u5024\u3068zip\u30d5\u30a1\u30a4\u30eb\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u95a2\u9023\u4ed8\u3051</li> <li>\u5404CSV\u30d5\u30a1\u30a4\u30eb\u304b\u3089invoice.json\u3092\u751f\u6210</li> <li>\u65e2\u5b58\u306einvoice.json\u304c\u3042\u308b\u5834\u5408\u306f\u5024\u3092\u7d99\u627f</li> </ol>"},{"location":"usage/config/mode/#_17","title":"\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c\u5f8c\u306e\u30d5\u30a1\u30a4\u30eb\u69cb\u6210","text":"<pre><code>data\n\u251c\u2500\u2500 divided\n\u2502   \u251c\u2500\u2500 0001\n\u2502   \u2502   \u251c\u2500\u2500 invoice\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 invoice.json  # smarttable\u306e1\u884c\u76ee\u304b\u3089\u751f\u6210\n\u2502   \u2502   \u251c\u2500\u2500 raw\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file1.txt\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 file2.txt\n\u2502   \u2502   \u2514\u2500\u2500 (\u305d\u306e\u4ed6\u306e\u6a19\u6e96\u30d5\u30a9\u30eb\u30c0)\n\u2502   \u2514\u2500\u2500 0002\n\u2502       \u251c\u2500\u2500 invoice\n\u2502       \u2502   \u2514\u2500\u2500 invoice.json  # smarttable\u306e2\u884c\u76ee\u304b\u3089\u751f\u6210\n\u2502       \u251c\u2500\u2500 raw\n\u2502       \u2502   \u251c\u2500\u2500 file3.txt\n\u2502       \u2502   \u2514\u2500\u2500 file4.txt\n\u2502       \u2514\u2500\u2500 (\u305d\u306e\u4ed6\u306e\u6a19\u6e96\u30d5\u30a9\u30eb\u30c0)\n\u251c\u2500\u2500 temp\n\u2502   \u251c\u2500\u2500 fsmarttable_experiment_0001.csv\n\u2502   \u251c\u2500\u2500 fsmarttable_experiment_0002.csv\n\u2502   \u251c\u2500\u2500 file1.txt\n\u2502   \u251c\u2500\u2500 file2.txt\n\u2502   \u251c\u2500\u2500 file3.txt\n\u2502   \u2514\u2500\u2500 file4.txt\n\u2514\u2500\u2500 (\u305d\u306e\u4ed6\u306e\u6a19\u6e96\u30d5\u30a9\u30eb\u30c0)\n</code></pre>"},{"location":"usage/config/mode/#invoicejson","title":"\u751f\u6210\u3055\u308c\u308binvoice.json\u306e\u4f8b","text":"<p><code>divided/0001/invoice/invoice.json</code>: <pre><code>{\n    \"datasetId\": \"999999884-1111-41b0-1dea-23bf4dh09g12\",\n    \"basic\": {\n        \"dataName\": \"\u5b9f\u9a131\",\n        \"dateSubmitted\": \"2024-01-15\",\n        \"dataOwnerId\": \"xxx\"\n    },\n    \"custom\": {\n        \"polymerName\": \"PVC\",\n        \"cycle\": \"1\",\n        \"thickness\": \"2mm\",\n        \"temperature\": \"25\"\n    },\n    \"sample\": {\n        \"names\": [\"sample001\"],\n        \"sampleId\": \"S001\",\n        \"generalAttributes\": [\n            {\n                \"termId\": \"3adf9874-7bcb-e5f8-99cb-3d6fd9d7b55e\",\n                \"value\": \"value1\"\n            }\n        ]\n    }\n}\n</code></pre></p>"},{"location":"usage/config/mode/#smarttable","title":"SmartTable\u8a2d\u5b9a\u30aa\u30d7\u30b7\u30e7\u30f3","text":"<p>SmartTableInvoice\u30e2\u30fc\u30c9\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u8a2d\u5b9a\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u5229\u7528\u3067\u304d\u307e\u3059\uff1a</p>"},{"location":"usage/config/mode/#save_table_file","title":"save_table_file","text":"<p>\u30aa\u30ea\u30b8\u30ca\u30eb\u306eSmartTable\u30d5\u30a1\u30a4\u30eb\uff08smarttable_*.xlsx/csv/tsv\uff09\u3092raw/nonshared_raw\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u5236\u5fa1\u3057\u307e\u3059\u3002</p> <p>\u8a2d\u5b9a\u9805\u76ee: <code>smarttable.save_table_file</code> \u30c7\u30d5\u30a9\u30eb\u30c8\u5024: <code>false</code> \u30c7\u30fc\u30bf\u578b: boolean</p> <pre><code># YAML\u8a2d\u5b9a\u4f8b\nsmarttable:\n  save_table_file: true  # SmartTable\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3059\u308b\u5834\u5408\n</code></pre> <pre><code># TOML\u8a2d\u5b9a\u4f8b\n[smarttable]\nsave_table_file = true\n</code></pre>"},{"location":"usage/config/mode/#_18","title":"\u52d5\u4f5c\u306e\u9055\u3044","text":"<p>save_table_file: false\uff08\u30c7\u30d5\u30a9\u30eb\u30c8\uff09\u306e\u5834\u5408: - SmartTable\u30d5\u30a1\u30a4\u30eb\u306fraw/nonshared_raw\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3055\u308c\u307e\u305b\u3093 - CSV\u751f\u6210\u30d5\u30a1\u30a4\u30eb\u306e\u307f\u304c\u51e6\u7406\u5bfe\u8c61\u3068\u306a\u308a\u307e\u3059 - \u30b9\u30c8\u30ec\u30fc\u30b8\u4f7f\u7528\u91cf\u3092\u7bc0\u7d04\u3067\u304d\u307e\u3059</p> <p>save_table_file: true\u306e\u5834\u5408: - \u30aa\u30ea\u30b8\u30ca\u30eb\u306eSmartTable\u30d5\u30a1\u30a4\u30eb\u304craw/nonshared_raw\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3055\u308c\u307e\u3059 - \u30c7\u30fc\u30bf\u306e\u5b8c\u5168\u306a\u76e3\u67fb\u8a3c\u8de1\u3092\u7dad\u6301\u3067\u304d\u307e\u3059 - SmartTable\u30d5\u30a1\u30a4\u30eb\u3068CSV\u751f\u6210\u30d5\u30a1\u30a4\u30eb\u306e\u4e21\u65b9\u304c\u51e6\u7406\u3055\u308c\u307e\u3059</p>"},{"location":"usage/config/mode/#_19","title":"\u8a2d\u5b9a\u4f8b\u3068\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u69cb\u6210","text":"<p>save_table_file: false\u306e\u5834\u5408\u306e\u51fa\u529b: <pre><code>data\n\u251c\u2500\u2500 divided\n\u2502   \u251c\u2500\u2500 0001\n\u2502   \u2502   \u251c\u2500\u2500 raw\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file1.txt  # zip\u304b\u3089\u5c55\u958b\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 file2.txt\n\u2502   \u2502   \u2514\u2500\u2500 (\u305d\u306e\u4ed6\u30d5\u30a9\u30eb\u30c0)\n\u2502   \u2514\u2500\u2500 0002\n\u2502       \u251c\u2500\u2500 raw\n\u2502       \u2502   \u251c\u2500\u2500 file3.txt\n\u2502       \u2502   \u2514\u2500\u2500 file4.txt\n\u2502       \u2514\u2500\u2500 (\u305d\u306e\u4ed6\u30d5\u30a9\u30eb\u30c0)\n\u2514\u2500\u2500 temp\n    \u251c\u2500\u2500 fsmarttable_experiment_0001.csv  # \u751f\u6210\u3055\u308c\u305fCSV\u30d5\u30a1\u30a4\u30eb\n    \u251c\u2500\u2500 fsmarttable_experiment_0002.csv\n    \u2514\u2500\u2500 (\u5c55\u958b\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb)\n</code></pre></p> <p>save_table_file: true\u306e\u5834\u5408\u306e\u51fa\u529b: <pre><code>data\n\u251c\u2500\u2500 divided\n\u2502   \u251c\u2500\u2500 0001\n\u2502   \u2502   \u251c\u2500\u2500 raw\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file1.txt\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 file2.txt\n\u2502   \u2502   \u2514\u2500\u2500 (\u305d\u306e\u4ed6\u30d5\u30a9\u30eb\u30c0)\n\u2502   \u2514\u2500\u2500 0002\n\u2502       \u251c\u2500\u2500 raw\n\u2502       \u2502   \u251c\u2500\u2500 file3.txt\n\u2502       \u2502   \u2514\u2500\u2500 file4.txt\n\u2502       \u2514\u2500\u2500 (\u305d\u306e\u4ed6\u30d5\u30a9\u30eb\u30c0)\n\u251c\u2500\u2500 raw  # \u307e\u305f\u306f nonshared_raw\n\u2502   \u2514\u2500\u2500 smarttable_experiment.xlsx  # \u30aa\u30ea\u30b8\u30ca\u30eb\u30d5\u30a1\u30a4\u30eb\u304c\u4fdd\u5b58\n\u2514\u2500\u2500 temp\n    \u251c\u2500\u2500 fsmarttable_experiment_0001.csv\n    \u251c\u2500\u2500 fsmarttable_experiment_0002.csv\n    \u2514\u2500\u2500 (\u5c55\u958b\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb)\n</code></pre></p>"},{"location":"usage/config/mode/#_20","title":"\u6ce8\u610f\u4e8b\u9805","text":"<ul> <li>\u30c6\u30fc\u30d6\u30eb\u30d5\u30a1\u30a4\u30eb\u306e\u547d\u540d\u898f\u5247\uff08<code>smarttable_*.{xlsx,csv,tsv}</code>\uff09\u306b\u5f93\u3046\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> <li>Excel\u30d5\u30a1\u30a4\u30eb\u306e\u5834\u5408\u3001\u6700\u521d\u306e\u30b7\u30fc\u30c8\uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b90\uff09\u304c\u8aad\u307f\u8fbc\u307e\u308c\u307e\u3059</li> <li>\u65e2\u5b58\u306einvoice.json\u304c\u3042\u308b\u5834\u5408\u3001SmartTable\u3067\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u5024\u306f\u4fdd\u6301\u3055\u308c\u307e\u3059</li> <li><code>meta/</code>\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\u306e\u30c7\u30fc\u30bf\u306finvoice.json\u306b\u51fa\u529b\u3055\u308c\u307e\u305b\u3093</li> <li>zip\u30d5\u30a1\u30a4\u30eb\u304c\u306a\u3044\u5834\u5408\u3067\u3082\u52d5\u4f5c\u3057\u307e\u3059\u304c\u3001<code>inputdataX</code>\u5217\u306f\u7121\u8996\u3055\u308c\u307e\u3059</li> <li><code>save_table_file</code>\u8a2d\u5b9a\u306f\u3001\u30b7\u30b9\u30c6\u30e0\u8a2d\u5b9a\u306e<code>save_raw</code>\u307e\u305f\u306f<code>save_nonshared_raw</code>\u304c\u6709\u52b9\u306a\u5834\u5408\u306b\u306e\u307f\u52d5\u4f5c\u3057\u307e\u3059</li> </ul>"},{"location":"usage/config/mode/#rdeformat","title":"RDEformat\u30e2\u30fc\u30c9","text":""},{"location":"usage/config/mode/#_21","title":"\u8aac\u660e","text":"<p>RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30e2\u30c3\u30af\u3092\u4f5c\u6210\u3059\u308b\u30e2\u30fc\u30c9\u3067\u3059\u3002\u30c7\u30fc\u30bf\u767b\u9332\u6642\u306b\u3001\u5177\u4f53\u7684\u306a\u69cb\u9020\u5316\u51e6\u7406\u306f\u884c\u308f\u305a\u3001\u6307\u5b9a\u3055\u308c\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5f62\u5f0f\u306b\u767b\u9332\u3092\u3057\u307e\u3059\u3002</p> <p>\u5165\u529b\u30c7\u30fc\u30bf\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6301\u3064zip\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u6295\u5165\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002zip\u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u306b\u3001<code>invoice</code>, <code>main_image</code>, <code>other_image</code>, <code>structured</code>\u3068\u3044\u3046\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u3042\u308a\u3001\u305d\u306e\u4e2d\u306b\u3001\u305d\u308c\u305e\u308c\u306e\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3057\u3066\u304f\u3060\u3055\u3044\u3002zip\u30d5\u30a1\u30a4\u30eb\u306f\u3001\u3059\u3067\u306b\u69cb\u9020\u5316\u51e6\u7406\u304c\u5b9f\u884c\u3055\u308c\u51fa\u529b\u3055\u308c\u305f\u60f3\u5b9a\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u683c\u7d0d\u3057\u3001\u305d\u306e\u30c7\u30fc\u30bf\u3092\u6307\u5b9a\u30d5\u30a9\u30eb\u30c0\u306b\u683c\u7d0d\u3059\u308b\u30a4\u30e1\u30fc\u30b8\u3067\u3059\u3002</p> <pre><code>\u2514\u2500\u2500 sample.zip\n    \u251c\u2500\u2500 invoice/\n    \u2502   \u2514\u2500\u2500 invoice.json\n    \u251c\u2500\u2500 main_image/\n    \u2502   \u2514\u2500\u2500 xxxx.png\n    \u251c\u2500\u2500 other_image/\n    \u2502   \u2514\u2500\u2500 xxxx.png\n    \u2514\u2500\u2500 structured/\n        \u2514\u2500\u2500 sample.csv\n</code></pre> <p></p>"},{"location":"usage/config/mode/#_22","title":"\u8d77\u52d5\u6761\u4ef6","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b<code>extended_mode: 'rdeformat'</code>\u3092\u8ffd\u52a0</p> <pre><code>extended_mode: 'rdeformat'\n</code></pre>"},{"location":"usage/config/mode/#rdeformat_1","title":"RDEformat\u30e2\u30fc\u30c9\u5b9f\u884c\u4f8b","text":""},{"location":"usage/config/mode/#_23","title":"\u6295\u5165\u30c7\u30fc\u30bf","text":"<ul> <li>\u767b\u9332\u30d5\u30a1\u30a4\u30eb</li> <li>structured.zip (RDEformat\u5f62\u5f0f\u3067\u5c55\u958b\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u4e00\u5f0f\u3092zip\u3067\u307e\u3068\u3081\u305f\u3082\u306e)</li> <li>tasksupport</li> <li>rdeconfig.yml</li> </ul> <p>Documents</p> <p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb<code>rdeconfig.yml</code>\u306f\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb - config\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p> <p>structured.zip\u306e\u5185\u5bb9\u306f\u4e0b\u8a18\u306e\u901a\u308a\u3067\u3059\u3002</p> <pre><code># unzip -t structured.zip\nArchive: structured.zip\n    testing: divided/\n    testing: divided/0002/\n    testing: divided/0002/meta/\n    testing: divided/0002/meta/metadata.json\n    testing: divided/0002/structured/\n    testing: divided/0002/structured/test23_2-output.html\n    testing: divided/0002/structured/test23_2-output.csv\n    testing: divided/0002/main_image/\n    testing: divided/0002/main_image/test23_2-output.png\n    testing: divided/0002/raw/\n    testing: divided/0002/raw/test23_2.csv\n    testing: divided/0001/\n    testing: divided/0001/meta/\n    testing: divided/0001/meta/metadata.json\n    testing: divided/0001/structured/\n    testing: divided/0001/structured/test23_1-output.html\n    testing: divided/0001/structured/test23_1-output.csv\n    testing: divided/0001/main_image/\n    testing: divided/0001/main_image/test23_1-output.png\n    testing: divided/0001/raw/\n    testing: divided/0001/raw/test23_1.csv\n    testing: meta/\n    testing: meta/metadata.json\n    testing: structured/\n    testing: structured/test23_0-output.html\n    testing: structured/test23_0-output.csv\n    testing: main_image/\n    testing: main_image/test23_0-output.png\n    testing: raw/\n    testing: raw/test23_0.csv\nNo errors detected in compressed data of data/inputdata/structured.zip.\n</code></pre>"},{"location":"usage/config/mode/#_24","title":"\u5b9f\u884c\u5f8c\u30d5\u30a1\u30a4\u30eb\u69cb\u6210","text":"<p>\u4e0a\u8a18\u306e\u8a2d\u5b9a\u3067\u3001\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <pre><code>data\n\u251c\u2500\u2500 divided\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 0001\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_1-output.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 other_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_1.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 structured\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test23_1-output.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_1-output.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 temp\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 thumbnail\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 test23_1-output.png\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 0002\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_2-output.png\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 other_image\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_2.csv\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 structured\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 test23_2-output.csv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_2-output.html\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 temp\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 thumbnail\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 test23_2-output.png\n\u251c\u2500\u2500 inputdata\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 structured.zip\n\u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 rdesys.log\n\u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_0-output.png\n\u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u251c\u2500\u2500 other_image\n\u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_0.csv\n\u251c\u2500\u2500 structured\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test23_0-output.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_0-output.html\n\u251c\u2500\u2500 tasksupport\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice.schema.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 temp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 divided\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 0001\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_1-output.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_1.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 structured\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 test23_1-output.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 test23_1-output.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 0002\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_2-output.png\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_2.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 structured\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u251c\u2500\u2500 test23_2-output.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 test23_2-output.html\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice_org.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main_image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_0-output.png\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test23_0.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 structured\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test23_0-output.csv\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test23_0-output.html\n\u2514\u2500\u2500 thumbnail\n    \u2514\u2500\u2500 test23_0-output.png\n</code></pre> <p>structured.zip\u304ctemp\u30d5\u30a9\u30eb\u30c0\u306b\u5c55\u958b\u3055\u308c\u305f\u306e\u3061\u306b\u898f\u7a0b\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u5c55\u958b\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"usage/config/mode/#invoicejsonzip","title":"invoice.json\u3092zip\u306b\u542b\u3081\u305f\u5834\u5408","text":"<p>RDEformat\u30e2\u30fc\u30c9\u3067\u306f\u3001\u6295\u5165\u3057\u305f<code>invoice.json</code>\u306f\u5229\u7528\u3055\u308c\u305a<code>invoice/invoice.json</code>\u304c\u30b3\u30d4\u30fc\u3055\u308c\u308b</p> <p>Warning</p> <ul> <li>RDEformat\u30e2\u30fc\u30c9\u3067\u306f\u3001<code>invoice/invoice.json</code>\u304c\u5229\u7528\u3055\u308c\u308b(divided\u306b\u3082\u30b3\u30d4\u30fc)</li> <li>temp\u30d5\u30a9\u30eb\u30c0\u306b\u5c55\u958b\u3055\u308c\u308b\u304c\u7d42\u4e86\u5f8c\u524a\u9664\u3055\u308c\u306a\u3044</li> </ul>"},{"location":"usage/config/mode/#multidatatile","title":"\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb(MultiDataTile)","text":""},{"location":"usage/config/mode/#_25","title":"\u8aac\u660e","text":"<p>\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u306f\u3001\u4e00\u5ea6\u306b\u8907\u6570\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8ffd\u52a0\u3059\u308b\u30e2\u30fc\u30c9\u3067\u3059\u3002\u3053\u306e\u30e2\u30fc\u30c9\u306f\u3001\u30d6\u30e9\u30a6\u30b6\u306eRDE\u30c7\u30fc\u30bf\u53d7\u3051\u5165\u308c\u753b\u9762\u3088\u308a\u767b\u9332\u3057\u307e\u3059\u3002\u4e0b\u8a18\u306e\u4f8b\u306e\u5834\u5408\u3001<code>rdeconfig.yml</code>\u3092\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306b\u683c\u7d0d\u3057\u3001<code>extended_mode: 'MultiDataTile'</code>\u3092\u8ffd\u52a0\u3059\u308b\u3068\u3001\u767b\u9332\u3057\u305f\u30c7\u30fc\u30bf\u6570\u3054\u3068\u306b\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30bf\u30a4\u30eb\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002<code>rdeconfig.yml</code>\u304c\u306a\u3044\u5834\u5408\u3001\u3082\u3057\u304f\u306f\u3001<code>extended_mode</code>\u306e\u6307\u5b9a\u304c\u306a\u3044\u5834\u5408\u3001\u4e00\u3064\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30bf\u30a4\u30eb\u306b\u767b\u9332\u3057\u305f\u30d5\u30a1\u30a4\u30eb\u304c\u3059\u3079\u3066\u767b\u9332\u3055\u308c\u307e\u3059\u3002</p> <p></p>"},{"location":"usage/config/mode/#_26","title":"\u8d77\u52d5\u6761\u4ef6","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b<code>extended_mode: 'MultiDataTile'</code>\u3092\u8ffd\u52a0</p> <pre><code>extended_mode: 'MultiDataTile'\n</code></pre> <p>Documents</p> <p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb<code>rdeconfig.yml</code>\u306f\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb - config\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"usage/config/mode/#multidatatile_1","title":"\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb(MultiDataTile) \u5b9f\u884c\u4f8b","text":""},{"location":"usage/config/mode/#_27","title":"\u6295\u5165\u30c7\u30fc\u30bf","text":"<ul> <li>\u767b\u9332\u30d5\u30a1\u30a4\u30eb</li> <li>tdata0000.dat</li> <li>data0001.dat</li> <li>tasksupport</li> <li>rdeconfig.yaml</li> </ul>"},{"location":"usage/config/mode/#_28","title":"\u5b9f\u884c\u524d\u30d5\u30a1\u30a4\u30eb\u69cb\u6210","text":"<pre><code>$ tree data\ndata\n\u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 data0000.dat\n\u2502   \u2514\u2500\u2500 data0001.dat\n\u251c\u2500\u2500 invoice\n\u2502   \u2514\u2500\u2500 invoice.json\n\u2514\u2500\u2500 tasksupport\n    \u251c\u2500\u2500 rdeconfig.yml\n    \u251c\u2500\u2500 invoice.schema.json\n    \u2514\u2500\u2500 metadata-def.json\n</code></pre>"},{"location":"usage/config/mode/#_29","title":"\u5b9f\u884c\u5f8c\u30d5\u30a1\u30a4\u30eb\u69cb\u6210","text":"<p>\u4e0a\u8a18\u306e\u8a2d\u5b9a\u3067\u3001\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u4ee5\u4e0b\u306e\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\u3002</p> <pre><code>data\n\u251c\u2500\u2500 divided\n\u2502   \u2514\u2500\u2500 0001\n\u2502       \u251c\u2500\u2500 invoice\n\u2502       \u2502   \u2514\u2500\u2500 invoice.json\n\u2502       \u251c\u2500\u2500 logs\n\u2502       \u251c\u2500\u2500 main_image\n\u2502       \u251c\u2500\u2500 meta\n\u2502       \u251c\u2500\u2500 other_image\n\u2502       \u251c\u2500\u2500 raw\n\u2502       \u2502   \u2514\u2500\u2500 data0000.dat\n\u2502       \u251c\u2500\u2500 structured\n\u2502       \u251c\u2500\u2500 temp\n\u2502       \u2514\u2500\u2500 thumbnail\n\u251c\u2500\u2500 inputdata\n\u2502   \u251c\u2500\u2500 data0000.dat\n\u2502   \u2514\u2500\u2500 data0001.dat\n\u251c\u2500\u2500 invoice\n\u2502   \u2514\u2500\u2500 invoice.json\n\u251c\u2500\u2500 logs\n\u2502   \u2514\u2500\u2500 rdesys.log\n\u251c\u2500\u2500 main_image\n\u251c\u2500\u2500 meta\n\u251c\u2500\u2500 other_image\n\u251c\u2500\u2500 raw\n\u2502   \u2514\u2500\u2500 data0001.dat\n\u251c\u2500\u2500 structured\n\u251c\u2500\u2500 tasksupport\n\u2502   \u251c\u2500\u2500 rdeconfig.yml\n\u2502   \u251c\u2500\u2500 metadata-def.json\n\u2502   \u2514\u2500\u2500 invoice.schema.json\n\u251c\u2500\u2500 temp\n\u2502   \u2514\u2500\u2500 invoice_org.json\n\u2514\u2500\u2500 thumbnail\n</code></pre>"},{"location":"usage/structured_process/directory/","title":"\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea","text":""},{"location":"usage/structured_process/directory/#_2","title":"\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020","text":"<p>RDE\u69cb\u9020\u5316\u51e6\u7406\u3067\u306f\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"usage/structured_process/directory/#_3","title":"\u5165\u529b","text":"<p>\u4ee5\u4e0b\u306e3\u3064\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c\u6642\u3001\u30b7\u30b9\u30c6\u30e0\u5074\u3067\u81ea\u52d5\u3067\u751f\u6210\u3055\u308c\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u306a\u308a\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u30ed\u30fc\u30ab\u30eb\u3067\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001\u4e8b\u524d\u306b\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4f5c\u6210\u3059\u308b\u5834\u6240\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5b9f\u884c\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u540c\u3058\u968e\u5c64\u306b\u3001<code>data</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e<code>data</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u306b\u3001\u4e0b\u8a18\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d \u7a2e\u5225 \u7528\u9014 inputdata \u5165\u529b\u30c7\u30fc\u30bf \u5165\u529b\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u683c\u7d0d invoice \u9001\u308a\u72b6\u30c7\u30fc\u30bf \u9001\u308a\u72b6(invoice.json)\u304c\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002 tasksupport \u753b\u50cf\u30d5\u30a1\u30a4\u30eb \u4e8b\u524d\u306b\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u4f5c\u6210\u6642\u306b\u767b\u9332\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u88dc\u52a9\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u683c\u7d0d"},{"location":"usage/structured_process/directory/#_4","title":"\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f8b","text":"<p>\u30ed\u30fc\u30ab\u30eb\u3067\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4e8b\u524d\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3059\u308b\u3002</p> <pre><code>.\n\u251c\u2500\u2500 modules\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 custom_modules.py\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 inputdata\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 sample_data.ras\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500 invoice.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tasksupport\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 invoice.schema.json\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 metadata-def.json\n\u251c\u2500\u2500 main.py # \u8d77\u52d5\u51e6\u7406\u3092\u5b9a\u7fa9(entry point)\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"usage/structured_process/directory/#_5","title":"\u51fa\u529b","text":"<p>\u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3057\u305f\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u7fa4\u3067\u3059\u3002rdetoolkit\u3067\u306f\u3001\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c\u3059\u308b\u3068\u3001\u4e0b\u8a18\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u81ea\u52d5\u3067\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002</p> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d \u7a2e\u5225 \u7528\u9014 meta \u4e3b\u8981\u30d1\u30e9\u30e1\u30fc\u30bf\u60c5\u5831\u30d5\u30a1\u30a4\u30eb \u4e3b\u8981\u30d1\u30e9\u30e1\u30fc\u30bf\u60c5\u5831\u30d5\u30a1\u30a4\u30eb(<code>metadata.json</code>)\u3092\u683c\u7d0d main_image \u753b\u50cf\u30d5\u30a1\u30a4\u30eb RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u8a73\u7d30\u753b\u50cf\u3068\u3057\u3066\u8868\u793a\u3055\u308c\u308b\u30b5\u30e0\u30cd\u30a4\u30eb\u30d5\u30a1\u30a4\u30eb other_image \u753b\u50cf\u30d5\u30a1\u30a4\u30eb RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u306b\u306e\u307f\u8868\u793a\u3055\u308c\u308b thumbnail \u753b\u50cf\u30d5\u30a1\u30a4\u30eb RDE\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4e00\u89a7\u306b\u8868\u793a\u3055\u308c\u308b\u753b\u50cf\u30d5\u30a1\u30a4\u30eb attachment - \u6dfb\u4ed8\u30d5\u30a1\u30a4\u30eb(\u203b) nonshared_raw - \u5171\u6709\u4e0d\u53ef\u80fd\u306a\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u914d\u7f6e raw raw\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb \u5171\u6709\u53ef\u80fd\u306araw\u30d5\u30a1\u30a4\u30eb\u7fa4\u3092\u914d\u7f6e\u3002\u5165\u529b\u30c7\u30fc\u30bf\u3092\u914d\u7f6e\u3059\u308b\u3002 structured \u69cb\u9020\u5316\u30d5\u30a1\u30a4\u30eb \u69cb\u9020\u5316\u51e6\u7406\u306b\u3088\u308a\u751f\u6210\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3002\u5165\u529b\u30c7\u30fc\u30bf\u3092\u914d\u7f6e\u3059\u308b\u3002 logs - \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u767b\u9332\u30fb\u53cd\u6620\u3055\u308c\u307e\u305b\u3093\u304c\u3001\u30ed\u30b0\u3092\u84c4\u7a4d\u3059\u308b\u305f\u3081\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002 temp - \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u767b\u9332\u30fb\u53cd\u6620\u3055\u308c\u307e\u305b\u3093\u304c\u3001\u4e00\u6642\u7684\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u3057\u3066\u3001temp\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u308b\u30b1\u30fc\u30b9\u304c\u3042\u308a\u307e\u3059\u3002 <p>Warning</p> <ul> <li>attachment\u306f\u3001rdetoolkit\u3067\u306f\u81ea\u52d5\u3067\u751f\u6210\u3055\u308c\u307e\u305b\u3093\u3002</li> </ul>"},{"location":"usage/structured_process/directory/#_6","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c\u5f8c\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f8b","text":"<pre><code>\u251c\u2500\u2500 modules\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 custom_modules.py\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 inputdata\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 excelinvoice.zip\n\u2502   \u251c\u2500\u2500 invoice\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice.json\n\u2502   \u251c\u2500\u2500 logs\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 rdesys.log\n\u2502   \u251c\u2500\u2500 main_image\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 iamge0.png\n\u2502   \u251c\u2500\u2500 meta\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata.json\n\u2502   \u251c\u2500\u2500 nonshared_raw\n\u2502   \u251c\u2500\u2500 other_image\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 sub_image1.png\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 sub_image2.png\n\u2502   \u251c\u2500\u2500 raw\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 DMF-pos-1.xyz\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 li-mole.inp\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 opt.xyz\n\u2502   \u251c\u2500\u2500 structured\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample.csv\n\u2502   \u251c\u2500\u2500 tasksupport\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 invoice.schema.json\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata-def.json\n\u2502   \u251c\u2500\u2500 temp\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 invoice_org.json\n\u2502   \u2514\u2500\u2500 thumbnail\n\u2502       \u2514\u2500\u2500 image.png\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"usage/structured_process/directory/#divided","title":"divided\u5c5e\u6027\u3092\u6301\u3064\u30c7\u30a3\u30ec\u30af\u30c8\u30ea","text":"<p>RDE\u69cb\u9020\u5316\u51e6\u7406\u3067\u306f\u3001Excelinvoice\u3084\u3001\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u30e2\u30fc\u30c9\u3068\u3044\u3046\u3001\u4e00\u5ea6\u306b\u8907\u6570\u306e\u30c7\u30fc\u30bf\u3092\u767b\u9332\u3059\u308b\u30e2\u30fc\u30c9\u304c\u5b58\u5728\u3059\u308b\u3002\u3053\u306e\u30e2\u30fc\u30c9\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u3001<code>divdied</code>\u3068\u3044\u3046\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002\u30ed\u30fc\u30ab\u30eb\u3067\u69cb\u9020\u5316\u51e6\u7406\u306e\u958b\u767a\u3092\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u30eb\u30fc\u30eb\u306b\u57fa\u3065\u3044\u3066\u3001<code>divided</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u305f\u3060\u3057\u3001Excelinvoice\u3084\u3001\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u30e2\u30fc\u30c9\u3092\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u8a18\u8ff0\u3057\u3001\u6307\u5b9a\u3057\u305f\u5165\u529b\u898f\u5247\u3067\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3057\u305f\u5834\u5408\u3001\u81ea\u52d5\u7684\u306bdivided\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>divided\u30c7\u30a3\u30ec\u30af\u30c8\u30ea<ul> <li>\u547d\u540d\u898f\u5247: <code>data/divided/00xx</code></li> <li><code>00xx</code>: 4\u6841\u3067\u30bc\u30ed\u57cb\u3081(\u4f8b\uff1a0001/, 0029/\u306a\u3069)</li> <li><code>data/divided/00xx</code>\u914d\u4e0b\u306b\u3001<code>structured</code>\u3084<code>meta</code>\u306a\u3069\u306eRDE\u69cb\u9020\u5316\u51e6\u7406\u304c\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u914d\u7f6e\u3002\u3053\u306e\u6642\u3001<code>inputdata</code>, <code>invoice</code>, <code>tasksupport</code>\u306f\u4f5c\u6210\u3057\u306a\u304f\u3066\u826f\u3044\u3002</li> </ul> </li> </ul> <pre><code>\u251c\u2500\u2500 divided/\n\u2502   \u251c\u2500\u2500 0001/  # \u4f8b: 0001/, 0002/ \u306a\u3069\n\u2502   \u2502   \u251c\u2500\u2500 structured/\n\u2502   \u2502   \u251c\u2500\u2500 meta/\n\u2502   \u2502   \u251c\u2500\u2500 thumbnail/\n\u2502   \u2502   \u251c\u2500\u2500 main_image/\n\u2502   \u2502   \u251c\u2500\u2500 other_image/\n\u2502   \u2502   \u251c\u2500\u2500 nonshared_raw/\n\u2502   \u2502   \u2514\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 0002/\n\u2502   \u2502   \u251c\u2500\u2500 structured/\n\u2502   \u2502   \u251c\u2500\u2500 meta/\n\u2502   \u2502   \u251c\u2500\u2500 thumbnail/\n\u2502   \u2502   \u251c\u2500\u2500 main_image/\n\u2502   \u2502   \u251c\u2500\u2500 other_image/\n\u2502   \u2502   \u251c\u2500\u2500 nonshared_raw/\n\u2502   \u2502   \u2514\u2500\u2500 raw/\n\u251c\u2500\u2500 inputdata/\n\u251c\u2500\u2500 invoice/\n\u251c\u2500\u2500 tasksupport/\n\u251c\u2500\u2500 structured/\n\u251c\u2500\u2500 meta/\n\u251c\u2500\u2500 thumbnail/\n\u251c\u2500\u2500 main_image/\n\u251c\u2500\u2500 other_image/\n\u251c\u2500\u2500 nonshared_raw/\n\u2514\u2500\u2500 raw/\n</code></pre>"},{"location":"usage/structured_process/directory/#rdetoolkitrde","title":"rdetoolkit\u3067RDE\u69cb\u9020\u5316\u51e6\u7406\u3067\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3059\u308b","text":"<p>\u4e0a\u8a18\u3067\u793a\u3057\u305f\u901a\u308a\u3001RDE\u69cb\u9020\u5316\u51e6\u7406\u3067\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u3001<code>divided/00xx</code>\u306e\u3088\u3046\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u3069\u3001\u547d\u540d\u898f\u5247\u306b\u3057\u305f\u304c\u3063\u3066\u751f\u6210\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u30b1\u30fc\u30b9\u304c\u5b58\u5728\u3057\u307e\u3059\u3002<code>rdetoolkit.core.DirectoryOps</code>\u4f7f\u7528\u3059\u308b\u3068\u3001\u7c21\u5358\u306bRDE\u69cb\u9020\u5316\u51e6\u7406\u3092\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"usage/structured_process/directory/#rde","title":"RDE\u304c\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\u3059\u308b","text":"<pre><code>from rdetoolkit.core improt DirectoryOps\n\n# 1. \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210: data\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\ndir_ops = DirectoryOps(\"data\")\n\n# 2. structured\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\np = dir_ops.structured.path\nprint(p)  # data/structured\n\n# 3. index\u4ed8\u304d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u6210\np = dir_ops.structured(2).path\nprint(p)  # data/divided/0002/structured\n\n# 4. data\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306b\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u4e00\u5ea6\u306b\u4f5c\u6210\u3057\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b\np = dir_ops.all()\nprint(p)  #['data/invoice', 'data/invoice_patch', 'data/attachment', 'data/tasksupport', 'data/structured', 'data/meta', 'data/thumbnail', 'data/main_image', 'data/other_image', 'data/nonshared_raw', 'data/raw']\n\n# 5. data\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306b\u3001index\u4ed8\u304d\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u4e00\u5ea6\u306b\u4f5c\u6210\u3057\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b\np = dir_ops.all(1)\nprint(p)  # ['data/invoice', 'data/invoice_patch', 'data/attachment', 'data/tasksupport', 'data/structured', 'data/meta', 'data/thumbnail', 'data/main_image', 'data/other_image', 'data/nonshared_raw', 'data/raw', 'data/divided/0001/structured', 'data/divided/0001/meta', 'data/divided/0001/thumbnail', 'data/divided/0001/main_image', 'data/divided/0001/other_image', 'data/divided/0001/nonshared_raw', 'data/divided/0001/raw']\n\n# 6. \u6307\u5b9a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\np = dir_ops.structured.list()\nprint(p)  # ['data/structured/strucuted_item_1.csv', 'data/structured/strucuted_item_2.csv', 'data/structured/strucuted_item_3.csv']\n\n# 7. \u6307\u5b9a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b(divided)\np = dir_ops.structured(2).list()\nprint(p)  # ['data/divided/0002/structured/strucuted_item_1.csv', 'data/divided/0002/structured/strucuted_item_2.csv', 'data/divided/0002/structured/strucuted_item_3.csv']\n</code></pre>"},{"location":"usage/structured_process/errorhandling/","title":"\u69cb\u9020\u5316\u51e6\u7406\u306e\u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0","text":""},{"location":"usage/structured_process/errorhandling/#rde","title":"RDE\u306e\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9\u3068\u30e1\u30c3\u30bb\u30fc\u30b8","text":"<p>RDE\u306e\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9\u3068\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u3001<code>job.failed</code>\u3068\u3044\u3046\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3078\u51fa\u529b\u3059\u308b\u3053\u3068\u3067\u7d42\u4e86\u30b3\u30fc\u30c90\u4ee5\u5916\u3092\u30ea\u30bf\u30fc\u30f3\u3059\u308b\u3053\u3068\u3067\u3001\u69cb\u9020\u5316\u51e6\u7406\u306e\u7570\u5e38\u7d42\u4e86\u3092RDE\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u304b\u3089\u78ba\u8a8d\u53ef\u80fd\u3067\u3059\u3002</p> <p>\u30d5\u30a9\u30fc\u30de\u30c3\u30c8</p> <pre><code>ErrorCode=&lt;\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9\u30fb\u756a\u53f7&gt;\nErrorMessage=&lt;\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8&gt;\n</code></pre> <p><code>jobs.faild</code>\u306e\u8a18\u8ff0\u4f8b</p> <pre><code>ErrorCode=1\nErrorMessage=ERROR: failed in data processing\n</code></pre> <p></p>"},{"location":"usage/structured_process/errorhandling/#rdetoolkit","title":"RDEToolKit\u3092\u4f7f\u3063\u3066\u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\u3092\u5b9f\u88c5\u3059\u308b","text":"<p>RDEToolKit\u3067\u306f\u3001<code>rdetoolkit.workflows.run()</code>\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u5185\u90e8\u3067\u767a\u751f\u3057\u305f\u4f8b\u5916<code>rdetoolkit.exceptions.StructuredError</code>\u3092\u30ad\u30e3\u30c3\u30c1\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u4e0b\u8a18\u306e\u4f8b\u3067\u306f\u3001\u5b58\u5728\u3057\u306a\u3044\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306e\u30a8\u30e9\u30fc\u3092\u3001job.failed\u306b\u8a18\u8ff0\u3059\u308b\u4f8b\u3067\u3059\u3002</p> <pre><code># main.py\nimport json\n\nimport rdetoolkit\nfrom rdetoolkit.exceptions import StructuredError\n\n\ndef read_experiment_data(config_path: str) -&gt; dict:\n    with open(config_path, \"r\") as f:\n        return json.load(f)\n\n\ndef dataset(srcpaths, resource_paths):\n    try:\n        config = read_experiment_data(\"not_found_file_path.txt\")\n    except FileNotFoundError as e:\n        # Add the error message and the error code\n        raise StructuredError(\"Config file not found\", ecode=3, eobj=e) from e\n\n    # Do something with the dataset\n    pass\n\n\nif __name__ == \"__main__\":\n    rdetoolkit.workflows.run(custom_dataset_function=dataset)\n</code></pre> <p><code>job.failed</code>\u306e\u51fa\u529b\u7d50\u679c\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> <pre><code>ErrorCode=3\nErrorMessage=Config file not found\n</code></pre>"},{"location":"usage/structured_process/errorhandling/#_2","title":"\u30b9\u30bf\u30c3\u30af\u30c8\u30ec\u30fc\u30b9\u6574\u5f62\u3059\u308b","text":"<p><code>rdetoolkit.errors.catch_exception_with_message</code>\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u69cb\u9020\u5316\u51e6\u7406\u306e\u30b9\u30bf\u30c3\u30af\u30c8\u30ec\u30fc\u30b9\u3092\u6574\u5f62\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001\u4e0a\u8a18\u306e<code>dataset</code>\u306b\u3001<code>catch_exception_with_message</code>\u3067\u30c7\u30b3\u30ec\u30fc\u30bf\u30fc\u3068\u3057\u3066\u4ed8\u4e0e\u3057\u307e\u3059\u3002</p> <p>\u30c7\u30b3\u30ec\u30fc\u30bf\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u30b9\u30bf\u30c3\u30af\u30c8\u30ec\u30fc\u30b9\u306e\u6574\u5f62\u3001\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3001\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9\u306e\u4e0a\u66f8\u304d\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b9\u30bf\u30c3\u30af\u30c8\u30ec\u30fc\u30b9\u306e\u8868\u793a\u30fb\u975e\u8868\u793a\u3092\u8a2d\u5b9a\u3067\u304d\u307e\u3059\u3002</p> <ul> <li><code>error_message</code>\uff1a \u4e0a\u66f8\u304d\u3059\u308b\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8</li> <li><code>error_code</code>: \u4e0a\u66f8\u304d\u3059\u308b\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9</li> <li><code>verbose</code>: \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b9\u30bf\u30c3\u30af\u30c8\u30ec\u30fc\u30b9\u306e\u8868\u793a\u30fb\u975e\u8868\u793a</li> </ul> <pre><code>@catch_exception_with_message(error_message=\"Overwrite message!\", error_code=100, verbose=False)\ndef dataset(srcpaths, resource_paths):\n    try:\n        config = read_experiment_data(\"not_found_file_path.txt\")\n    except FileNotFoundError as e:\n        # Add the error message and the error code\n        raise StructuredError(\"Config file not found\", ecode=3, eobj=e) from e\n\n    # Do something with the dataset\n    passs\n</code></pre> <p>\u3053\u306e\u6642\u306e\u30b9\u30bf\u30c3\u30af\u30c8\u30ec\u30fc\u30b9\u306f\u3001\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002</p> <pre><code>Traceback (simplified message):\nCall Path:\n   File: /Users/myproject/container/modules/custom_modules.py, Line: 109 in wrapper()\n    \u2514\u2500 File: /Users/myproject/container/main.py, Line: 27 in dataset()\n        \u2514\u2500&gt; L27: raise StructuredError(\"Config file not found\", ecode=3, eobj=e) from e \ud83d\udd25\n\nException Type: StructuredError\nError: Config file not found\n</code></pre> <p>\u307e\u305f\u3001\u30c7\u30b3\u30ec\u30fc\u30bf\u30fc\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306e<code>job.failed</code>\u306f\u3001\u30c7\u30b3\u30ec\u30fc\u30bf\u3092\u4ed8\u4e0e\u3059\u308b\u524d\u3068\u540c\u69d8\u306e\u51fa\u529b\u3068\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u5185\u90e8\u3067\u660e\u793a\u7684\u306b\u5b9a\u7fa9\u3057\u305f<code>raise &lt;\u4f8b\u5916\u30af\u30e9\u30b9&gt;</code>\u306e\u5185\u5bb9\u3092\u6355\u6349\u3057\u3001\u8a73\u7d30\u60c5\u5831\u3092<code>job.faild</code>\u306b\u66f8\u304d\u8fbc\u307f\u307e\u3059\u3002</p> <p>\u30c7\u30b3\u30ec\u30fc\u30bf\u30fc\u3092\u4ed8\u4e0e\u3057\u305f\u95a2\u6570\u5185\u90e8\u3067\u660e\u793a\u7684\u306b\u5b9a\u7fa9\u3057\u305f\u4f8b\u5916\u30af\u30e9\u30b9\u3067\u30a8\u30e9\u30fc\u3092\u6355\u6349\u3057\u305f\u5834\u5408\u3001\u305d\u306e\u4f8b\u5916\u60c5\u5831\u304c\u512a\u5148\u3055\u308c\u308b\u305f\u3081\u3001\u30c7\u30b3\u30ec\u30fc\u30bf\u5f15\u6570\u3067\u4e0e\u3048\u305f\u30e1\u30c3\u30bb\u30fc\u30b8\u3084\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9\u3067\u306f\u4e0a\u66f8\u304d\u3055\u308c\u307e\u305b\u3093\u3002</p> <p>\u3053\u306e\u30c7\u30b3\u30ec\u30fc\u30bf\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u4e8b\u524d\u306b\u4f8b\u5916\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u306a\u3044\u304b\u3064\u3001\u4e88\u671f\u3057\u306a\u3044\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u6355\u6349\u3057\u305f\u3068\u304d\u3001\u30c7\u30b3\u30ec\u30fc\u30bf\u306e\u5f15\u6570\u3067\u4e8b\u524d\u306b\u5b9a\u7fa9\u3057\u305f\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3068\u30a8\u30e9\u30fc\u30b3\u30fc\u30c9\u3092<code>job.faild</code>\u306b\u66f8\u304d\u8fbc\u3080\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>ErrorCode=3\nErrorMessage=Error: Config file not found\n</code></pre> <p>\u3082\u3057\u3001\u30c7\u30b3\u30ec\u30fc\u30bf\u3092\u4f7f\u308f\u306a\u305a\u3001\u4e88\u671f\u3057\u306a\u3044\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001<code>job.faild</code>\u306b\u306f\u4ee5\u4e0b\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u304c\u66f8\u304d\u8fbc\u307e\u308c\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001Web UI\u3067\u78ba\u8a8d\u3057\u305f\u3068\u304d\u3001\u30a8\u30e9\u30fc\u306e\u7279\u5b9a\u304c\u96e3\u3057\u3044\u3067\u3059\u3002</p> <pre><code>ErrorCode=999\nErrorMessage=Error: Please check the logs and code, then try again.\n</code></pre>"},{"location":"usage/structured_process/feature_description/","title":"\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u8aac\u660e\u6b04\u3078\u306e\u81ea\u52d5\u8a18\u8ff0\u306b\u3064\u3044\u3066","text":"<p>Reference</p> <ul> <li>API Documentation: update_description_with_features()</li> </ul> <p><code>data/tasksupport/metadata-def.json</code>\u306b\u3001<code>_feature</code>\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u3001\u81ea\u52d5\u7684\u306b\u8aac\u660e\u6b04\u3078\u8a18\u8ff0\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u3001<code>length</code>\u3068\u3001<code>weight</code>\u306b<code>_feature=true</code>\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u305f\u3081\u3001\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u306e\u8aac\u660e\u6b04\u3078\u81ea\u52d5\u7684\u306b\u8a18\u8ff0\u3055\u308c\u307e\u3059\u3002</p> <pre><code>{\n    \"length\": {\n        \"name\": {\n            \"ja\": \"\u9577\u3055\",\n            \"en\": \"length\"\n        },\n        \"schema\": {\n            \"type\": \"number\"\n        },\n        \"unit\": \"nm\",\n        \"_feature\": true\n    },\n    \"weight\": {\n        \"name\": {\n            \"ja\": \"\u91cd\u3055\",\n            \"en\": \"weight\"\n        },\n        \"schema\": {\n            \"type\": \"number\"\n        },\n        \"unit\": \"nm\",\n        \"_feature\": true\n    },\n    \"hight\": {\n        \"name\": {\n            \"ja\": \"\u9ad8\u3055\",\n            \"en\": \"hight\"\n        },\n        \"schema\": {\n            \"type\": \"number\"\n        },\n        \"unit\": \"nm\"\n    }\n}\n</code></pre> <p><code>_feature</code>\u30d5\u30a3\u30fc\u30eb\u30c9\u304c\u3042\u308b\u30e1\u30bf\u30c7\u30fc\u30bf\u306f\u3001\u4ee5\u4e0b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u5909\u63db\u3055\u308c\u3001<code>invoice/invoice.json</code>\u306e<code>basic.description</code>\u3078\u8ee2\u8a18\u3055\u308c\u307e\u3059\u3002</p> <p>Tip</p> <p>key\u540d\u306f\u65e5\u672c\u8a9e\u540d\u306e\u307f\u306e\u5bfe\u5fdc\u3067\u3059\u3002value\u306f<code>meta/metadata.json</code>\u3092\u53c2\u7167\u3057\u307e\u3059\u3002</p> \u5358\u4f4d(unit\u30d5\u30a3\u30fc\u30eb\u30c9)\u3042\u308a\u5358\u4f4d(unit\u30d5\u30a3\u30fc\u30eb\u30c9)\u306a\u3057 <p><code>invoice/invoice.json</code>\u8a18\u8f09\u4f8b</p> <pre><code>{\n    \"basic\": {\n        \"description\": \"\u9577\u3055(nm):100\\n\u91cd\u3055(nm):200\"\n    }\n}\n</code></pre> <p><code>invoice/invoice.json</code>\u8a18\u8f09\u4f8b</p> <pre><code>{\n    \"basic\": {\n        \"description\": \"\u9577\u3055:100\\n\u91cd\u3055:200\"\n    }\n}\n</code></pre>"},{"location":"usage/structured_process/rdepath/","title":"\u69cb\u9020\u5316\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u306e\u53d6\u5f97","text":""},{"location":"usage/structured_process/rdepath/#_2","title":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u306e\u5236\u7d04","text":"<p>\u30e6\u30fc\u30b6\u30fc\u81ea\u8eab\u304c\u3001\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9a\u7fa9\u3059\u308b\u969b\u304c\u3001<code>RdeInputDirPaths</code>, <code>RdeOutputResourcePath</code>\u3092\u5f15\u6570\u3067\u53d7\u3051\u53d6\u308c\u308a\u53ef\u80fd\u306a\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002</p> <pre><code>def dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    # \u3053\u306e\u95a2\u6570\u5185\u3067\u30e6\u30fc\u30b6\u81ea\u8eab\u304c\u5b9a\u7fa9\u3057\u305f\u30af\u30e9\u30b9\u3084\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\n    ...\n</code></pre> <p>Reference</p> <ul> <li>API Documentation: RdeInputDirPaths - rde2types</li> <li>API Documentation: RdeOutputResourcePath - rde2types</li> </ul> <p><code>RdeInputDirPaths</code>\u306f\u3001\u5165\u529b\u3067\u6271\u308f\u308c\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u3084\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u7fa4\u3092\u683c\u7d0d\u3057\u3066\u307e\u3059\u3002\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u306f\u3001<code>pathlib.Path</code>\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <pre><code>@dataclass\nclass RdeInputDirPaths:\n    inputdata: Path\n    invoice: Path\n    tasksupport: Path\n</code></pre> <p>\u95a2\u6570\u5185\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u5b9a\u7fa9\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u3092\u53d6\u5f97\u3067\u304d\u307e\u3059\u3002</p> <pre><code>def dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    inputdata_dir = srcpaths.inputdata\n    invoice_dir = srcpaths.invoice\n    tasksupport = srcpaths.tasksupport\n</code></pre> <p><code>RdeOutputResourcePath</code>\u3067\u306f\u3001\u51fa\u529b\u3067\u6271\u308f\u308c\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u7fa4\u3092\u683c\u7d0d\u3057\u3066\u3044\u307e\u3059\u3002</p> <pre><code>@dataclass\nclass RdeOutputResourcePath:\n    raw: Path\n    rawfiles: tuple[Path, ...]\n    struct: Path\n    main_image: Path\n    other_image: Path\n    meta: Path\n    thumbnail: Path\n    logs: Path\n    invoice: Path\n    invoice_schema_json: Path\n    invoice_org: Path\n    temp: Path | None = None\n    invoice_patch: Path | None = None\n    attachment: Path | None = None\n    nonshared_raw: Path | None = None\n</code></pre> <p>\u95a2\u6570\u5185\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u5b9a\u7fa9\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u3092\u53d6\u5f97\u3067\u304d\u307e\u3059\u3002</p> <pre><code>def dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    rawfiles = resource_paths.rawfiles\n    raw_dir = resource_paths.raw\n    struct_dir = resource_paths.struct\n    main_image_dir = resource_paths.main_image\n</code></pre>"},{"location":"usage/structured_process/structured/","title":"\u69cb\u9020\u5316\u51e6\u7406\u3092\u69cb\u7bc9\u3059\u308b","text":"<p>RDE\u69cb\u9020\u5316\u51e6\u7406\u306f\u3001\u5927\u304d\u304f\u5206\u3051\u3066\u3001\u4ee5\u4e0b\u306e3\u3064\u306e\u30d5\u30a7\u30fc\u30ba\u306b\u5206\u3051\u3089\u308c\u307e\u3059\u3002</p> <pre><code>graph LR\n    \u8d77\u52d5\u51e6\u7406 --&gt; \u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\n    \u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406 --&gt; \u7d42\u4e86\u51e6\u7406\n</code></pre> <p>\u8d77\u52d5\u51e6\u7406\u3001\u7d42\u4e86\u51e6\u7406\u306f\u3001rdetoolkit\u3092\u4f7f\u3046\u3053\u3068\u3067\u7c21\u5358\u306b\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u30e6\u30fc\u30b6\u30fc\u81ea\u8eab\u306f\u3001\u3054\u81ea\u8eab\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b<code>\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406</code>\u3092\u5b9a\u7fa9\u3059\u308b\u3060\u3051\u3067\u3059\u3002</p>"},{"location":"usage/structured_process/structured/#rdetoolkit","title":"RDEToolKit\u304c\u30b5\u30dd\u30fc\u30c8\u3059\u308b\u51e6\u7406","text":"<ul> <li>\u8d77\u52d5\u30e2\u30fc\u30c9\u306e\u81ea\u52d5\u5224\u5b9a</li> <li>\u5165\u529b\u30c7\u30fc\u30bf\u3092\u6307\u5b9a\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3059\u308b</li> <li>Main\u753b\u50cf\u3092\u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u306b\u4fdd\u5b58\u3059\u308b</li> <li>magic variable\u306e\u9069\u7528</li> <li>invoice.schema.json, invoice.json\u7b49\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3</li> <li>metadata-def.json\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30bf\u30a4\u30eb\u306e\u8aac\u660e\u6b04\u81ea\u52d5\u751f\u6210</li> </ul>"},{"location":"usage/structured_process/structured/#_2","title":"\u8d77\u52d5\u51e6\u7406","text":"<p>\u8d77\u52d5\u51e6\u7406\u3067\u306f\u3001\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b\u524d\u306e\u51e6\u7406\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</p> <p>Reference</p> <p>API Documents: rdetoolkit.workflows.run</p>"},{"location":"usage/structured_process/structured/#_3","title":"\u5b9f\u88c5\u4f8b","text":"<p>\u5b9f\u88c5\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u30dd\u30a4\u30f3\u30c8\u3068\u306a\u308b\u30d5\u30a1\u30a4\u30eb\u306b\u5b9f\u88c5\u3059\u308b\u3053\u3068\u3092\u63a8\u5968\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001<code>main.py</code>\u3084<code>run.py</code>\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9f\u88c5\u3057\u307e\u3059\u3002</p> <pre><code>from modules import process #\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\nimport rdetoolkit\n\n# run()\u304cRDE\u69cb\u9020\u5316\u306e\u8d77\u52d5\u51e6\u7406\u3068\u5f8c\u51e6\u7406\u3092\u5b9f\u884c\nrdetoolkit.workflows.run(custom_dataset_function=process.dataset)\n</code></pre>"},{"location":"usage/structured_process/structured/#_4","title":"\u5177\u4f53\u7684\u306a\u51e6\u7406\u306b\u3064\u3044\u3066","text":"<p>\u8d77\u52d5\u51e6\u7406\u306f\u3001\u6b21\u306e\u51e6\u7406\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</p> <ul> <li>RDE\u69cb\u9020\u5316\u51e6\u7406\u3067\u5fc5\u8981\u306a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u81ea\u52d5\u4f5c\u6210</li> <li>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u3092<code>raw</code>\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3078\u81ea\u52d5\u4fdd\u5b58</li> <li>\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u30fb\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u304b\u3089\u3001\u5404\u7a2e\u30e2\u30fc\u30c9\u306b\u5fdc\u3058\u305f\u8aad\u307f\u8fbc\u307f\u51e6\u7406</li> <li>\u8aad\u307f\u8fbc\u3093\u3060\u30d5\u30a1\u30a4\u30eb\u3092\u3001\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u306b\u6e21\u3057\u5b9f\u884c</li> </ul> <p>\u5404\u7a2e\u30e2\u30fc\u30c9\u3054\u3068\u306e\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u307f\u3067\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u306e\u5185\u5bb9\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3044\u307e\u305b\u3093\u3002\u305d\u306e\u305f\u3081\u3001\u30d5\u30a1\u30a4\u30eb\u306b\u5bfe\u3059\u308b\u5177\u4f53\u7684\u306a\u51e6\u7406\u306f\u3001\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u7b49\u3067\u5b9a\u7fa9\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>graph TD\n    init1[\u8d77\u52d5\u51e6\u7406] --&gt; init2[\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f5c\u6210]\n    init2--&gt;init3{\u30e2\u30fc\u30c9\u9078\u629e}\n    init3--&gt;|default|init6[invoice\u30e2\u30fc\u30c9]\n    init3--&gt;init7[Excelinvoice\u30e2\u30fc\u30c9]\n    init3--&gt;init8[\u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb]\n    init3--&gt;init9[RDE\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u30e2\u30fc\u30c9]\n    init6--&gt;init10[\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406]\n    init7--&gt;init10\n    init8--&gt;init10\n    init9--&gt;init10\n    init10 --&gt; init11[\u5165\u529b\u30c7\u30fc\u30bf\u306eraw\u30d5\u30a9\u30eb\u30c0\u306b\u8ffd\u52a0]\n    init11 --&gt; init12[[\u30e6\u30fc\u30b6\u30fc\u5b9a\u7fa9\u306e\u69cb\u9020\u5316\u51e6\u7406\u5b9f\u884c]]\n    init12 --&gt; init13[\u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u306e\u81ea\u52d5\u4fdd\u5b58]\n    init13 --&gt; init14[magic variable]\n    init14 --&gt; init15[\u7d42\u4e86]\n</code></pre>"},{"location":"usage/structured_process/structured/#_5","title":"\u30ab\u30b9\u30bf\u30e0\u7528\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\u306e\u4f5c\u6210","text":"<p>rdetoolkit\u3067\u306f\u3001\u72ec\u81ea\u306e\u51e6\u7406\u3092RDE\u306e\u69cb\u9020\u5316\u51e6\u7406\u306e\u30d5\u30ed\u30fc\u306b\u7d44\u307f\u8fbc\u307f\u8fbc\u3080\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u306f\u3001\u5165\u529b\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30c7\u30fc\u30bf\u52a0\u5de5\u30fb\u30b0\u30e9\u30d5\u5316\u30fb\u6a5f\u68b0\u5b66\u7fd2\u7528\u306ecsv\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u306a\u3069\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u56fa\u6709\u306e\u51e6\u7406\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u3001RDE\u3078\u67d4\u8edf\u306b\u30c7\u30fc\u30bf\u3092\u767b\u9332\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"usage/structured_process/structured/#_6","title":"\u5b9f\u88c5\u4f8b","text":"<p>\u3053\u3053\u3067\u306f\u3001rdetoolkit\u3078\u6e21\u3059\u72ec\u81ea\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u95a2\u6570\u3092\u3001<code>dataset()</code>\u3068\u3057\u307e\u3059\u3002<code>dataset()</code>\u306f\u3001\u4ee5\u4e0b\u306e2\u3064\u306e\u5f15\u6570\u3092\u6e21\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>Tip</p> <p>\u72ec\u81ea\u306e\u30af\u30e9\u30b9\u30fb\u95a2\u6570\u7fa4\u3092\u5b9a\u7fa9\u3059\u308b\u5834\u5408\u3001\u5fc5\u305a<code>RdeInputDirPaths</code>, <code>RdeOutputResourcePath</code>\u3092\u5f15\u6570\u3067\u53d7\u3051\u53d6\u308a\u53ef\u80fd\u306a\u95a2\u6570\u3067wrap\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code># wrap\u7528\u95a2\u6570\ndef dataset(srcpaths: RdeInputDirPaths, resource_paths: RdeOutputResourcePath):\n    # \u3053\u306e\u95a2\u6570\u5185\u3067\u30e6\u30fc\u30b6\u81ea\u8eab\u304c\u5b9a\u7fa9\u3057\u305f\u30af\u30e9\u30b9\u3084\u95a2\u6570\u3092\u547c\u3073\u51fa\u3059\n    ...\n</code></pre> <p>\u3053\u308c\u306e\u5f15\u6570\u306b\u306f\u3001\u69cb\u9020\u5316\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u5404\u7a2e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u60c5\u5831\u3084\u30d5\u30a1\u30a4\u30eb\u60c5\u5831\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u7279\u306b\u3001<code>RdeOutputResourcePath</code>\u306b\u306f\u3001\u30d5\u30a1\u30a4\u30eb\u4fdd\u5b58\u5148\u60c5\u5831\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u69cb\u9020\u5316\u51e6\u7406\u5185\u3067\u4e0a\u8a18\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30fb\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b\u65b9\u6cd5\u306f\u3001\u69cb\u9020\u5316\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\u306e\u53d6\u5f97\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>srcpaths (RdeInputDirPaths): \u5165\u529b\u3055\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\u60c5\u5831</li> <li>resource_paths (RdeOutputResourcePath): \u51e6\u7406\u7d50\u679c\u3092\u4fdd\u5b58\u3059\u308b\u305f\u3081\u306e\u51fa\u529b\u30ea\u30bd\u30fc\u30b9\u30d1\u30b9\u60c5\u5831</li> </ul> <p>Reference</p> <ul> <li>API Documentation: RdeInputDirPaths - rde2types</li> <li>API Documentation: RdeOutputResourcePath - rde2types</li> </ul> <p>\u4eca\u56de\u306e\u4f8b\u3067\u306f\u3001<code>modules</code>\u4ee5\u4e0b\u306b\u3001<code>display_messsage()</code>, <code>custom_graph()</code>, <code>custom_extract_metadata()</code>\u3068\u3044\u3046\u30c0\u30df\u30fc\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u3001\u72ec\u81ea\u306e\u69cb\u9020\u5316\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u95a2\u6570\u306f\u3001<code>modules/process.py</code>\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3057\u5b9a\u7fa9\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306e2\u3064\u306e\u5f15\u6570\u3092\u6e21\u3059\u95a2\u6570\u3067\u306a\u3051\u308c\u3070\u3001rdetoolkit\u306f\u6b63\u3057\u304f\u51e6\u7406\u304c\u5b9f\u884c\u3067\u304d\u307e\u305b\u3093\u3002</p> <pre><code># modules/process.py\ndef display_messsage(path):\n    print(f\"Test Message!: {path}\")\n\ndef custom_graph():\n    print(\"graph\")\n\ndef custom_extract_metadata():\n    print(\"extract metadata\")\n\ndef dataset(srcpaths, resource_paths):\n    display_messsage(srcpaths)\n    display_messsage(resource_paths)\n    custom_graph()\n    custom_extract_metadata()\n</code></pre>"},{"location":"usage/structured_process/structured/#_7","title":"\u8d77\u52d5\u51e6\u7406\u3078\u7d44\u307f\u8fbc\u3080","text":"<p>\u3053\u306e<code>dataset()</code>\u3092\u8d77\u52d5\u3059\u308b\u305f\u3081\u306b\u306f\u3001\u5148\u307b\u3069\u306e\u8d77\u52d5\u51e6\u7406\u3067\u4f5c\u6210\u3057\u305f\u30a8\u30f3\u30c8\u30ea\u30fc\u30dd\u30a4\u30f3\u30c8\u3068\u306a\u308b\u30d5\u30a1\u30a4\u30eb(<code>main.py</code>\u306a\u3069)\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3057\u307e\u3059\u3002</p> <pre><code>from modules import process #\u72ec\u81ea\u3067\u5b9a\u7fa9\u3057\u305f\u69cb\u9020\u5316\u51e6\u7406\u95a2\u6570\nimport rdetoolkit\n\n# run()\u306b\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406\u3092\u3092\u6e21\u3059\nresult = rdetoolkit.workflows.run(custom_dataset_function=process.dataset)\n</code></pre> <p><code>result</code>\u306b\u306f\u3001\u69cb\u9020\u5316\u51e6\u7406\u306e\u5b9f\u884c\u30b9\u30c6\u30fc\u30bf\u30b9\u304c\u683c\u7d0d\u3055\u308c\u307e\u3059\u3002</p> <pre><code>{\n  \"statuses\": [\n    {\n      \"run_id\": \"0000\",\n      \"title\": \"test-dataset\",\n      \"status\": \"success\",\n      \"mode\": \"MultiDataTile\",\n      \"error_code\": null,\n      \"error_message\": null,\n      \"target\": \"data/inputdata\",\n      \"stacktrace\": null\n    },\n    {\n      \"run_id\": \"0001\",\n      \"title\": \"test-dataset\",\n      \"status\": \"success\",\n      \"mode\": \"MultiDataTile\",\n      \"error_code\": null,\n      \"error_message\": null,\n      \"target\": \"data/inputdata\",\n      \"stacktrace\": null\n    }\n  ]\n}\n</code></pre> <p>\u5931\u6557\u3057\u305f\u6642\u306e<code>result</code>\u306e\u51fa\u529b</p> <pre><code>{\n  \"statuses\": [\n    {\n      \"run_id\": \"0000\",\n      \"title\": \"Structured Process Faild: MultiDataTile\",\n      \"status\": \"failed\",\n      \"mode\": \"MultiDataTile\",\n      \"error_code\": 999,\n      \"error_message\": \"Error: Error in modules\",\n      \"target\": \"data/inputdata/sample1.txt\",\n      \"stacktrace\": \"Traceback (most recent call last):\\n  File \\\"/Users/myproject/.venv/lib/python3.10/site-packages/rdetoolkit/exceptions.py\\\", line 158, in skip_exception_context\\n    yield error_info\\n  File \\\"/Users/myproject/.venv/lib/python3.10/site-packages/rdetoolkit/workflows.py\\\", line 242, in run\\n    status = multifile_mode_process(str(idx), srcpaths, rdeoutput_resource, custom_dataset_function)\\n  File \\\"/Users/myproject/.venv/lib/python3.10/site-packages/rdetoolkit/modeproc.py\\\", line 157, in multifile_mode_process\\n    datasets_process_function(srcpaths, resource_paths)\\n  File \\\"/Users/myproject/modules/modules.py\\\", line 5, in error_modules\\n    raise Exception(\\\"Error in modules\\\")\\nException: Error in modules\\n\"\n    },\n    {\n      \"run_id\": \"0001\",\n      \"title\": \"Structured Process Faild: MultiDataTile\",\n      \"status\": \"failed\",\n      \"mode\": \"MultiDataTile\",\n      \"error_code\": 999,\n      \"error_message\": \"Error: Error in modules\",\n      \"target\": \"data/inputdata/sample2.txt\",\n      \"stacktrace\": \"Traceback (most recent call last):\\n  File \\\"/Users/myproject/.venv/lib/python3.10/site-packages/rdetoolkit/exceptions.py\\\", line 158, in skip_exception_context\\n    yield error_info\\n  File \\\"/Users/myproject/.venv/lib/python3.10/site-packages/rdetoolkit/workflows.py\\\", line 242, in run\\n    status = multifile_mode_process(str(idx), srcpaths, rdeoutput_resource, custom_dataset_function)\\n  File \\\"/Users/myproject/.venv/lib/python3.10/site-packages/rdetoolkit/modeproc.py\\\", line 157, in multifile_mode_process\\n    datasets_process_function(srcpaths, resource_paths)\\n  File \\\"/Users/myproject/modules/modules.py\\\", line 5, in error_modules\\n    raise Exception(\\\"Error in modules\\\")\\nException: Error in modules\\n\"\n    }\n  ]\n}\n</code></pre>"},{"location":"usage/structured_process/structured/#_8","title":"\u7d42\u4e86\u51e6\u7406\u306b\u3064\u3044\u3066","text":"<p>\u7d9a\u3044\u3066\u3001<code>rdetoolkit.workflow.run()</code>\u304c\u5b9f\u884c\u3059\u308b\u7d42\u4e86\u51e6\u7406\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002</p> <ul> <li>\u751f\u6210\u30d5\u30a1\u30a4\u30eb\u3001\u5165\u529b\u30d5\u30a1\u30a4\u30eb\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3</li> <li>Main\u753b\u50cf\u304b\u3089\u4ee3\u8868\u753b\u50cf\u306e\u81ea\u52d5\u4fdd\u5b58</li> <li>\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u8aac\u660e\u6b04\u3078\u6307\u5b9a\u30e1\u30bf\u30c7\u30fc\u30bf\u306e\u81ea\u52d5\u8a18\u8ff0</li> </ul> <pre><code>graph TD\n    end1[\u30ab\u30b9\u30bf\u30e0\u69cb\u9020\u5316\u51e6\u7406] --&gt; end2[\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3]\n    end2 --&gt; end3{\u8a2d\u5b9a:save_thumbnail_image}\n    end3 --&gt;|False|end6[\u8aac\u660e\u6b04\u306e\u81ea\u52d5\u8a18\u8ff0]\n    end3--&gt;|True|end5[Main\u753b\u50cf\u304b\u3089\u30b5\u30e0\u30cd\u30a4\u30eb\u753b\u50cf\u4fdd\u5b58]\n    end5 --&gt; end6\n    end6 --&gt; end7[\u7d42\u4e86]\n</code></pre>"},{"location":"usage/structured_process/structured/#_9","title":"\u5404\u7a2e\u30d5\u30a1\u30a4\u30eb\u306e\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3","text":"<p>\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306f\u3001\u6b21\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u5bfe\u8c61\u3068\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30d5\u30a1\u30a4\u30eb\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u958b\u8a2d\u6642\u3001\u30c7\u30fc\u30bf\u767b\u9332\u6642\u306b\u91cd\u8981\u306a\u30d5\u30a1\u30a4\u30eb\u3068\u306a\u308a\u307e\u3059\u3002</p> <ul> <li><code>tasksupport/metadata-def.json</code></li> <li><code>tasksupport/invoice.shcema.json</code></li> <li><code>data/invoice/invoice.json</code></li> </ul> <p>Documents</p> <p>\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u3064\u3044\u3066</p>"},{"location":"usage/structured_process/structured/#_10","title":"\u8aac\u660e\u6b04\u3078\u306e\u81ea\u52d5\u8ee2\u8a18","text":"<p>\u4ee5\u4e0b\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>\u30c7\u30fc\u30bf\u30bf\u30a4\u30eb\u8aac\u660e\u6b04\u3078\u306e\u81ea\u52d5\u8ee2\u8a18</li> </ul>"}]}